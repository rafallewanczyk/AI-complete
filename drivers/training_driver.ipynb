{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import src.utils as utils\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import src.generate_vocabulary as gv\n",
    "\n",
    "with open('..\\\\data\\\\my_train.txt') as f:\n",
    "    train_dirs = f.readlines()\n",
    "train_dirs = [re.sub(r'/', r'\\\\', ('..\\\\data\\\\'+d)[:-1]) for d in train_dirs ]\n",
    "\n",
    "with open('..\\\\data\\\\my_eval.txt') as f:\n",
    "    eval_dirs = f.readlines()\n",
    "eval_dirs = [re.sub(r'/', r'\\\\', ('..\\\\data\\\\'+d)[:-1]) for d in eval_dirs]\n",
    "\n",
    "with open('..\\\\data\\\\all.txt') as f:\n",
    "    all_dirs = f.readlines()\n",
    "all_dirs = [re.sub(r'/', r'\\\\', ('..\\\\data\\\\'+d)[:-1]) for d in all_dirs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating vocabulary\n",
      "found 13607 file matches. \n",
      "\n",
      "threshold: 18\n",
      "last 3 words under threshold ['accept_inplace', 'vis_chains', 'IVGMM', 'nifm', 'book_names', 'allvars', 'o_start', 'parent_cards', 'check_cspace_convert', \"'references'\"]\n",
      "generated 20000 words\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=13607.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fba349742df94bf78ea3f4383b952b01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v = gv.generate_vocabulary(train_dirs + eval_dirs, ['py'], 20000, name='.\\\\vocabulary.voc')\n",
    "print(f'generated {len(v)} words')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_files = utils.tokenize_data(train_dirs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum([len(x[1]) for x in tokenized_files]))\n",
    "# with open('.\\\\tokenized_files.tok', 'wb') as f:\n",
    "#     pickle.dump(tokenized_files, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('.\\\\tokenized_files.tok', 'rb') as f:\n",
    "    tokenized_files = pickle.load(f)\n",
    "\n",
    "print(len(tokenized_files))\n",
    "sum([len(x[1]) for x in tokenized_files])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from src.model import Model\n",
    "\n",
    "vocab_size = 20001\n",
    "embedding_dim = 32 #CONST\n",
    "rnn_units = 512\n",
    "batch_size = 128 #CONST\n",
    "win_size = 1\n",
    "model = Model(vocab_size, embedding_dim, rnn_units, batch_size, win_size, checkpoint_name='.\\\\checkpoints\\\\model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train('.\\\\vocabulary.voc', tokenized_files, 25)\n",
    "#start 17:54"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.prepare_predictions('.\\\\vocabulary', '.\\\\checkpoints\\\\model.h5')\n",
    "# model.get_prediction('import numpy', 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}