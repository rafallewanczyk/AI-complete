{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import src.utils as utils\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# import src.generate_vocabulary as gv\n",
    "#\n",
    "# with open('.\\\\data\\\\my_train.txt') as f:\n",
    "#     train_dirs = f.readlines()\n",
    "# train_dirs = [re.sub(r'/', r'\\\\', ('.\\\\data\\\\'+d)[:-1]) for d in train_dirs ]\n",
    "#\n",
    "# with open('.\\\\data\\\\my_eval.txt') as f:\n",
    "#     eval_dirs = f.readlines()\n",
    "# eval_dirs = [re.sub(r'/', r'\\\\', ('.\\\\data\\\\'+d)[:-1]) for d in eval_dirs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# v = gv.generate_vocabulary(train_dirs + eval_dirs, ['py'], 20000, name='.\\\\vocabulary.voc')\n",
    "# print(f'generated {len(v)} words')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# tokenized_files = utils.tokenize_data(train_dirs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# with open('.\\\\tokenized_files.tok', 'wb') as f:\n",
    "#     pickle.dump(tokenized_files, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9103\n"
     ]
    },
    {
     "data": {
      "text/plain": "9118453"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\\\tokenized_files.tok', 'rb') as f:\n",
    "    tokenized_files = pickle.load(f)\n",
    "\n",
    "print(len(tokenized_files))\n",
    "sum([len(x[1]) for x in tokenized_files])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from src.model import Model\n",
    "\n",
    "vocab_size = 20001\n",
    "embedding_dim = 32\n",
    "rnn_units = 512\n",
    "batch_size = 128\n",
    "win_size = 10\n",
    "model = Model(vocab_size, embedding_dim, rnn_units, batch_size, win_size, checkpoint_name='.\\\\checkpoints\\\\model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (128, None, 32)           640032    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (128, None, 32)           1056      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (128, None, 512)          838656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (128, None, 20001)        10260513  \n",
      "=================================================================\n",
      "Total params: 11,740,257\n",
      "Trainable params: 11,740,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "number of batches : 70553\n",
      "Epoch 0 Batch 0 Loss 9.903407096862793 in 1.254232\n",
      "saving model with loss 9.903407096862793\n",
      "Epoch 0 Batch 500 Loss 4.384105682373047 in 15.254120\n",
      "Epoch 0 Batch 1000 Loss 4.230160236358643 in 15.820534\n",
      "saving model with loss 4.230160236358643\n",
      "Epoch 0 Batch 1500 Loss 4.100945472717285 in 15.726305\n",
      "Epoch 0 Batch 2000 Loss 4.039811611175537 in 15.613519\n",
      "saving model with loss 4.039811611175537\n",
      "Epoch 0 Batch 2500 Loss 3.964020252227783 in 15.381206\n",
      "Epoch 0 Batch 3000 Loss 3.8595314025878906 in 15.155217\n",
      "saving model with loss 3.8595314025878906\n",
      "Epoch 0 Batch 3500 Loss 3.8185763359069824 in 15.213303\n",
      "Epoch 0 Batch 4000 Loss 2.8260414600372314 in 15.155255\n",
      "saving model with loss 2.8260414600372314\n",
      "Epoch 0 Batch 4500 Loss 2.8998093605041504 in 15.296387\n",
      "Epoch 0 Batch 5000 Loss 2.6149353981018066 in 15.218393\n",
      "saving model with loss 2.6149353981018066\n",
      "Epoch 0 Batch 5500 Loss 2.5776495933532715 in 15.268656\n",
      "Epoch 0 Batch 6000 Loss 2.451035261154175 in 15.211568\n",
      "saving model with loss 2.451035261154175\n",
      "Epoch 0 Batch 6500 Loss 2.2623794078826904 in 15.243353\n",
      "Epoch 0 Batch 7000 Loss 2.332775592803955 in 15.185123\n",
      "saving model with loss 2.332775592803955\n",
      "Epoch 0 Batch 7500 Loss 2.3821229934692383 in 15.244976\n",
      "Epoch 0 Batch 8000 Loss 2.351471185684204 in 15.178178\n",
      "saving model with loss 2.351471185684204\n",
      "Epoch 0 Batch 8500 Loss 2.3541297912597656 in 15.236554\n",
      "Epoch 0 Batch 9000 Loss 2.299436092376709 in 15.149272\n",
      "saving model with loss 2.299436092376709\n",
      "Epoch 0 Batch 9500 Loss 2.1802189350128174 in 15.242844\n",
      "Epoch 0 Batch 10000 Loss 2.1741435527801514 in 15.150970\n",
      "saving model with loss 2.1741435527801514\n",
      "Epoch 0 Batch 10500 Loss 2.3187479972839355 in 15.232091\n",
      "Epoch 0 Batch 11000 Loss 2.280986785888672 in 15.151457\n",
      "saving model with loss 2.280986785888672\n",
      "Epoch 0 Batch 11500 Loss 2.098275899887085 in 15.242716\n",
      "Epoch 0 Batch 12000 Loss 2.125235080718994 in 15.176846\n",
      "saving model with loss 2.125235080718994\n",
      "Epoch 0 Batch 12500 Loss 2.2676048278808594 in 15.610015\n",
      "Epoch 0 Batch 13000 Loss 2.2144761085510254 in 15.555615\n",
      "saving model with loss 2.2144761085510254\n",
      "Epoch 0 Batch 13500 Loss 2.0544912815093994 in 15.917067\n",
      "Epoch 0 Batch 14000 Loss 2.0360755920410156 in 16.226402\n",
      "saving model with loss 2.0360755920410156\n",
      "Epoch 0 Batch 14500 Loss 2.1595637798309326 in 16.234886\n",
      "Epoch 0 Batch 15000 Loss 2.1631054878234863 in 16.031472\n",
      "saving model with loss 2.1631054878234863\n",
      "Epoch 0 Batch 15500 Loss 2.282731533050537 in 15.886447\n",
      "Epoch 0 Batch 16000 Loss 1.9969857931137085 in 16.180902\n",
      "saving model with loss 1.9969857931137085\n",
      "Epoch 0 Batch 16500 Loss 2.1289591789245605 in 16.356056\n",
      "Epoch 0 Batch 17000 Loss 2.1348748207092285 in 15.874010\n",
      "saving model with loss 2.1348748207092285\n",
      "Epoch 0 Batch 17500 Loss 2.024005651473999 in 16.167355\n",
      "Epoch 0 Batch 18000 Loss 2.081702709197998 in 15.971582\n",
      "saving model with loss 2.081702709197998\n",
      "Epoch 0 Batch 18500 Loss 2.038656711578369 in 16.023776\n",
      "Epoch 0 Batch 19000 Loss 2.0604562759399414 in 16.392431\n",
      "saving model with loss 2.0604562759399414\n",
      "Epoch 0 Batch 19500 Loss 2.06770658493042 in 16.000639\n",
      "Epoch 0 Batch 20000 Loss 2.052294969558716 in 16.057524\n",
      "saving model with loss 2.052294969558716\n",
      "Epoch 0 Batch 20500 Loss 2.1753273010253906 in 16.037616\n",
      "Epoch 0 Batch 21000 Loss 1.853696584701538 in 16.013169\n",
      "saving model with loss 1.853696584701538\n",
      "Epoch 0 Batch 21500 Loss 2.1074230670928955 in 16.026668\n",
      "Epoch 0 Batch 22000 Loss 2.1711032390594482 in 16.180846\n",
      "saving model with loss 2.1711032390594482\n",
      "Epoch 0 Batch 22500 Loss 1.9385318756103516 in 16.558192\n",
      "Epoch 0 Batch 23000 Loss 1.9504413604736328 in 16.092269\n",
      "saving model with loss 1.9504413604736328\n",
      "Epoch 0 Batch 23500 Loss 2.0502991676330566 in 16.392159\n",
      "Epoch 0 Batch 24000 Loss 1.9634208679199219 in 16.295678\n",
      "saving model with loss 1.9634208679199219\n",
      "Epoch 0 Batch 24500 Loss 2.185737133026123 in 16.561666\n",
      "Epoch 0 Batch 25000 Loss 2.0641682147979736 in 16.720348\n",
      "saving model with loss 2.0641682147979736\n",
      "Epoch 0 Batch 25500 Loss 2.0240681171417236 in 16.693369\n",
      "Epoch 0 Batch 26000 Loss 2.070263624191284 in 16.061543\n",
      "saving model with loss 2.070263624191284\n",
      "Epoch 0 Batch 26500 Loss 2.0900378227233887 in 16.263568\n",
      "Epoch 0 Batch 27000 Loss 2.07381534576416 in 15.998792\n",
      "saving model with loss 2.07381534576416\n",
      "Epoch 0 Batch 27500 Loss 2.09865140914917 in 15.947779\n",
      "Epoch 0 Batch 28000 Loss 1.9448810815811157 in 15.758400\n",
      "saving model with loss 1.9448810815811157\n",
      "Epoch 0 Batch 28500 Loss 2.2094240188598633 in 15.782780\n",
      "Epoch 0 Batch 29000 Loss 1.9985564947128296 in 15.873700\n",
      "saving model with loss 1.9985564947128296\n",
      "Epoch 0 Batch 29500 Loss 1.9602720737457275 in 15.906690\n",
      "Epoch 0 Batch 30000 Loss 1.8665975332260132 in 16.170917\n",
      "saving model with loss 1.8665975332260132\n",
      "Epoch 0 Batch 30500 Loss 1.927310585975647 in 16.250932\n",
      "Epoch 0 Batch 31000 Loss 1.923771619796753 in 15.842225\n",
      "saving model with loss 1.923771619796753\n",
      "Epoch 0 Batch 31500 Loss 2.1995224952697754 in 15.866702\n",
      "Epoch 0 Batch 32000 Loss 2.049891948699951 in 16.027054\n",
      "saving model with loss 2.049891948699951\n",
      "Epoch 0 Batch 32500 Loss 2.135246753692627 in 16.476004\n",
      "Epoch 0 Batch 33000 Loss 2.0665996074676514 in 15.702714\n",
      "saving model with loss 2.0665996074676514\n",
      "Epoch 0 Batch 33500 Loss 2.0820300579071045 in 16.852874\n",
      "Epoch 0 Batch 34000 Loss 1.9904028177261353 in 16.747873\n",
      "saving model with loss 1.9904028177261353\n",
      "Epoch 0 Batch 34500 Loss 2.2038989067077637 in 16.783982\n",
      "Epoch 0 Batch 35000 Loss 1.7372289896011353 in 16.218307\n",
      "saving model with loss 1.7372289896011353\n",
      "Epoch 0 Batch 35500 Loss 2.025615930557251 in 15.740959\n",
      "Epoch 0 Batch 36000 Loss 2.097716808319092 in 15.864671\n",
      "saving model with loss 2.097716808319092\n",
      "Epoch 0 Batch 36500 Loss 1.9120264053344727 in 16.321755\n",
      "Epoch 0 Batch 37000 Loss 1.9950811862945557 in 15.999670\n",
      "saving model with loss 1.9950811862945557\n",
      "Epoch 0 Batch 37500 Loss 1.998268723487854 in 15.862839\n",
      "Epoch 0 Batch 38000 Loss 2.0602471828460693 in 15.406937\n",
      "saving model with loss 2.0602471828460693\n",
      "Epoch 0 Batch 38500 Loss 1.8801437616348267 in 15.348831\n",
      "Epoch 0 Batch 39000 Loss 2.019660472869873 in 15.322255\n",
      "saving model with loss 2.019660472869873\n",
      "Epoch 0 Batch 39500 Loss 1.9960565567016602 in 15.335930\n",
      "Epoch 0 Batch 40000 Loss 1.8953269720077515 in 15.287761\n",
      "saving model with loss 1.8953269720077515\n",
      "Epoch 0 Batch 40500 Loss 2.000657558441162 in 15.744449\n",
      "Epoch 0 Batch 41000 Loss 1.8697803020477295 in 16.537071\n",
      "saving model with loss 1.8697803020477295\n",
      "Epoch 0 Batch 41500 Loss 2.0089669227600098 in 17.255130\n",
      "Epoch 0 Batch 42000 Loss 2.018514633178711 in 16.831301\n",
      "saving model with loss 2.018514633178711\n",
      "Epoch 0 Batch 42500 Loss 2.1694998741149902 in 16.448241\n",
      "Epoch 0 Batch 43000 Loss 2.060494899749756 in 15.899824\n",
      "saving model with loss 2.060494899749756\n",
      "Epoch 0 Batch 43500 Loss 1.9153831005096436 in 15.786365\n",
      "Epoch 0 Batch 44000 Loss 2.0665829181671143 in 15.541621\n",
      "saving model with loss 2.0665829181671143\n",
      "Epoch 0 Batch 44500 Loss 1.9588384628295898 in 15.882386\n",
      "Epoch 0 Batch 45000 Loss 2.01729679107666 in 16.737398\n",
      "saving model with loss 2.01729679107666\n",
      "Epoch 0 Batch 45500 Loss 1.8501745462417603 in 16.766530\n",
      "Epoch 0 Batch 46000 Loss 2.0642619132995605 in 16.723361\n",
      "saving model with loss 2.0642619132995605\n",
      "Epoch 0 Batch 46500 Loss 1.8557932376861572 in 16.333228\n",
      "Epoch 0 Batch 47000 Loss 2.07224702835083 in 15.401307\n",
      "saving model with loss 2.07224702835083\n",
      "Epoch 0 Batch 47500 Loss 1.9544731378555298 in 16.099915\n",
      "Epoch 0 Batch 48000 Loss 1.9502496719360352 in 15.873452\n",
      "saving model with loss 1.9502496719360352\n",
      "Epoch 0 Batch 48500 Loss 1.8463884592056274 in 16.404737\n",
      "Epoch 0 Batch 49000 Loss 1.9074394702911377 in 15.875642\n",
      "saving model with loss 1.9074394702911377\n",
      "Epoch 0 Batch 49500 Loss 1.8655054569244385 in 16.752047\n",
      "Epoch 0 Batch 50000 Loss 1.8972946405410767 in 16.432284\n",
      "saving model with loss 1.8972946405410767\n",
      "Epoch 0 Batch 50500 Loss 1.8822009563446045 in 16.773675\n",
      "Epoch 0 Batch 51000 Loss 1.7166130542755127 in 16.443894\n",
      "saving model with loss 1.7166130542755127\n",
      "Epoch 0 Batch 51500 Loss 1.831395149230957 in 15.249913\n",
      "Epoch 0 Batch 52000 Loss 1.9006264209747314 in 15.905431\n",
      "saving model with loss 1.9006264209747314\n",
      "Epoch 0 Batch 52500 Loss 1.9304840564727783 in 15.967792\n",
      "Epoch 0 Batch 53000 Loss 1.986614465713501 in 15.821112\n",
      "saving model with loss 1.986614465713501\n",
      "Epoch 0 Batch 53500 Loss 2.0228991508483887 in 15.952409\n",
      "Epoch 0 Batch 54000 Loss 1.9604723453521729 in 16.518653\n",
      "saving model with loss 1.9604723453521729\n",
      "Epoch 0 Batch 54500 Loss 1.918771505355835 in 16.785797\n",
      "Epoch 0 Batch 55000 Loss 1.9312727451324463 in 16.735620\n",
      "saving model with loss 1.9312727451324463\n",
      "Epoch 0 Batch 55500 Loss 2.074795722961426 in 17.005158\n",
      "Epoch 0 Batch 56000 Loss 2.10931396484375 in 15.729247\n",
      "saving model with loss 2.10931396484375\n",
      "Epoch 0 Batch 56500 Loss 1.883530855178833 in 15.264682\n",
      "Epoch 0 Batch 57000 Loss 1.8310248851776123 in 15.253112\n",
      "saving model with loss 1.8310248851776123\n",
      "Epoch 0 Batch 57500 Loss 1.8714298009872437 in 15.260176\n",
      "Epoch 0 Batch 58000 Loss 1.8424804210662842 in 15.204296\n",
      "saving model with loss 1.8424804210662842\n",
      "Epoch 0 Batch 58500 Loss 1.9762389659881592 in 15.254344\n",
      "Epoch 0 Batch 59000 Loss 1.8946924209594727 in 15.204875\n",
      "saving model with loss 1.8946924209594727\n",
      "Epoch 0 Batch 59500 Loss 1.8840854167938232 in 15.267988\n",
      "Epoch 0 Batch 60000 Loss 1.9597829580307007 in 15.195952\n",
      "saving model with loss 1.9597829580307007\n",
      "Epoch 0 Batch 60500 Loss 1.941520094871521 in 15.275222\n",
      "Epoch 0 Batch 61000 Loss 1.944504737854004 in 15.191676\n",
      "saving model with loss 1.944504737854004\n",
      "Epoch 0 Batch 61500 Loss 1.8475227355957031 in 15.275008\n",
      "Epoch 0 Batch 62000 Loss 1.8892326354980469 in 15.216727\n",
      "saving model with loss 1.8892326354980469\n",
      "Epoch 0 Batch 62500 Loss 1.9036452770233154 in 15.268016\n",
      "Epoch 0 Batch 63000 Loss 1.9729887247085571 in 15.194755\n",
      "saving model with loss 1.9729887247085571\n",
      "Epoch 0 Batch 63500 Loss 1.9855772256851196 in 15.266536\n",
      "Epoch 0 Batch 64000 Loss 1.9124183654785156 in 15.190174\n",
      "saving model with loss 1.9124183654785156\n",
      "Epoch 0 Batch 64500 Loss 1.8246126174926758 in 15.257325\n",
      "Epoch 0 Batch 65000 Loss 1.8737363815307617 in 15.184429\n",
      "saving model with loss 1.8737363815307617\n",
      "Epoch 0 Batch 65500 Loss 1.8775265216827393 in 15.264886\n",
      "Epoch 0 Batch 66000 Loss 2.030961513519287 in 15.177747\n",
      "saving model with loss 2.030961513519287\n",
      "Epoch 0 Batch 66500 Loss 1.942299246788025 in 15.256173\n",
      "Epoch 0 Batch 67000 Loss 1.851035714149475 in 15.197439\n",
      "saving model with loss 1.851035714149475\n",
      "Epoch 0 Batch 67500 Loss 1.9519526958465576 in 15.258376\n",
      "Epoch 0 Batch 68000 Loss 1.8694696426391602 in 15.176425\n",
      "saving model with loss 1.8694696426391602\n",
      "Epoch 0 Batch 68500 Loss 1.9444191455841064 in 15.267119\n",
      "Epoch 0 Batch 69000 Loss 1.7875874042510986 in 15.190991\n",
      "saving model with loss 1.7875874042510986\n",
      "Epoch 0 Batch 69500 Loss 2.0183749198913574 in 15.271565\n",
      "Epoch 0 Batch 70000 Loss 1.9737634658813477 in 15.203194\n",
      "saving model with loss 1.9737634658813477\n",
      "Epoch 0 Batch 70500 Loss 2.0002646446228027 in 15.251563\n",
      "Epoch 1 Loss 1.7060657739639282\n",
      "Time taken for 1 epoch 2230.68208193779\n",
      "number of batches : 70553\n",
      "Epoch 1 Batch 0 Loss 1.9133869409561157 in 1.172338\n",
      "saving model with loss 1.9133869409561157\n",
      "Epoch 1 Batch 500 Loss 1.9167531728744507 in 15.245537\n",
      "Epoch 1 Batch 1000 Loss 1.9165246486663818 in 15.166559\n",
      "saving model with loss 1.9165246486663818\n",
      "Epoch 1 Batch 1500 Loss 1.973387360572815 in 15.244019\n",
      "Epoch 1 Batch 2000 Loss 1.8951400518417358 in 15.186163\n",
      "saving model with loss 1.8951400518417358\n",
      "Epoch 1 Batch 2500 Loss 2.057321071624756 in 15.250095\n",
      "Epoch 1 Batch 3000 Loss 2.0089797973632812 in 15.228117\n",
      "saving model with loss 2.0089797973632812\n",
      "Epoch 1 Batch 3500 Loss 1.9211986064910889 in 15.209164\n",
      "Epoch 1 Batch 4000 Loss 1.828657865524292 in 15.162260\n",
      "saving model with loss 1.828657865524292\n",
      "Epoch 1 Batch 4500 Loss 1.8898597955703735 in 15.247927\n",
      "Epoch 1 Batch 5000 Loss 1.8571195602416992 in 15.193393\n",
      "saving model with loss 1.8571195602416992\n",
      "Epoch 1 Batch 5500 Loss 1.8758947849273682 in 15.239049\n",
      "Epoch 1 Batch 6000 Loss 1.9414504766464233 in 15.188628\n",
      "saving model with loss 1.9414504766464233\n",
      "Epoch 1 Batch 6500 Loss 2.021880626678467 in 15.243683\n",
      "Epoch 1 Batch 7000 Loss 1.9024690389633179 in 15.177818\n",
      "saving model with loss 1.9024690389633179\n",
      "Epoch 1 Batch 7500 Loss 1.8659846782684326 in 15.273663\n",
      "Epoch 1 Batch 8000 Loss 1.9068034887313843 in 15.181049\n",
      "saving model with loss 1.9068034887313843\n",
      "Epoch 1 Batch 8500 Loss 2.0184426307678223 in 15.238486\n",
      "Epoch 1 Batch 9000 Loss 1.902958631515503 in 15.156517\n",
      "saving model with loss 1.902958631515503\n",
      "Epoch 1 Batch 9500 Loss 2.062822103500366 in 15.253493\n",
      "Epoch 1 Batch 10000 Loss 1.9445879459381104 in 15.188218\n",
      "saving model with loss 1.9445879459381104\n",
      "Epoch 1 Batch 10500 Loss 1.9142528772354126 in 15.271242\n",
      "Epoch 1 Batch 11000 Loss 1.7977514266967773 in 15.179190\n",
      "saving model with loss 1.7977514266967773\n",
      "Epoch 1 Batch 11500 Loss 1.7954057455062866 in 15.244147\n",
      "Epoch 1 Batch 12000 Loss 1.8825600147247314 in 15.159513\n",
      "saving model with loss 1.8825600147247314\n",
      "Epoch 1 Batch 12500 Loss 2.0170750617980957 in 15.231380\n",
      "Epoch 1 Batch 13000 Loss 1.7728593349456787 in 15.152644\n",
      "saving model with loss 1.7728593349456787\n",
      "Epoch 1 Batch 13500 Loss 1.7902443408966064 in 15.255467\n",
      "Epoch 1 Batch 14000 Loss 1.8876253366470337 in 15.195414\n",
      "saving model with loss 1.8876253366470337\n",
      "Epoch 1 Batch 14500 Loss 1.832471489906311 in 15.243468\n",
      "Epoch 1 Batch 15000 Loss 1.9766842126846313 in 15.184766\n",
      "saving model with loss 1.9766842126846313\n",
      "Epoch 1 Batch 15500 Loss 1.97963547706604 in 15.246861\n",
      "Epoch 1 Batch 16000 Loss 1.9488420486450195 in 15.205560\n",
      "saving model with loss 1.9488420486450195\n",
      "Epoch 1 Batch 16500 Loss 1.8337666988372803 in 15.249826\n",
      "Epoch 1 Batch 17000 Loss 1.7905387878417969 in 15.179218\n",
      "saving model with loss 1.7905387878417969\n",
      "Epoch 1 Batch 17500 Loss 1.7991613149642944 in 15.248466\n",
      "Epoch 1 Batch 18000 Loss 1.8090139627456665 in 15.172836\n",
      "saving model with loss 1.8090139627456665\n",
      "Epoch 1 Batch 18500 Loss 1.8800805807113647 in 15.241442\n",
      "Epoch 1 Batch 19000 Loss 1.9848130941390991 in 15.181807\n",
      "saving model with loss 1.9848130941390991\n",
      "Epoch 1 Batch 19500 Loss 1.9762808084487915 in 15.259903\n",
      "Epoch 1 Batch 20000 Loss 1.9943240880966187 in 15.167983\n",
      "saving model with loss 1.9943240880966187\n",
      "Epoch 1 Batch 20500 Loss 1.9507091045379639 in 15.269757\n",
      "Epoch 1 Batch 21000 Loss 1.9274024963378906 in 15.150700\n",
      "saving model with loss 1.9274024963378906\n",
      "Epoch 1 Batch 21500 Loss 1.8845428228378296 in 15.247464\n",
      "Epoch 1 Batch 22000 Loss 1.982708215713501 in 15.181205\n",
      "saving model with loss 1.982708215713501\n",
      "Epoch 1 Batch 22500 Loss 1.8858544826507568 in 15.253726\n",
      "Epoch 1 Batch 23000 Loss 2.0379695892333984 in 15.179344\n",
      "saving model with loss 2.0379695892333984\n",
      "Epoch 1 Batch 23500 Loss 1.9065128564834595 in 15.258215\n",
      "Epoch 1 Batch 24000 Loss 1.9750595092773438 in 15.154844\n",
      "saving model with loss 1.9750595092773438\n",
      "Epoch 1 Batch 24500 Loss 2.003300189971924 in 15.254410\n",
      "Epoch 1 Batch 25000 Loss 1.9465583562850952 in 15.169259\n",
      "saving model with loss 1.9465583562850952\n",
      "Epoch 1 Batch 25500 Loss 1.838381052017212 in 15.244612\n",
      "Epoch 1 Batch 26000 Loss 1.9428918361663818 in 15.184237\n",
      "saving model with loss 1.9428918361663818\n",
      "Epoch 1 Batch 26500 Loss 1.8488613367080688 in 15.249148\n",
      "Epoch 1 Batch 27000 Loss 1.885213851928711 in 15.214993\n",
      "saving model with loss 1.885213851928711\n",
      "Epoch 1 Batch 27500 Loss 1.9623615741729736 in 15.247944\n",
      "Epoch 1 Batch 28000 Loss 1.750482201576233 in 15.174430\n",
      "saving model with loss 1.750482201576233\n",
      "Epoch 1 Batch 28500 Loss 1.9384233951568604 in 15.262769\n",
      "Epoch 1 Batch 29000 Loss 1.7731348276138306 in 15.249277\n",
      "saving model with loss 1.7731348276138306\n",
      "Epoch 1 Batch 29500 Loss 1.914515733718872 in 15.451878\n",
      "Epoch 1 Batch 30000 Loss 1.8969318866729736 in 15.202523\n",
      "saving model with loss 1.8969318866729736\n",
      "Epoch 1 Batch 30500 Loss 1.769118309020996 in 15.241916\n",
      "Epoch 1 Batch 31000 Loss 1.8913471698760986 in 15.173843\n",
      "saving model with loss 1.8913471698760986\n",
      "Epoch 1 Batch 31500 Loss 1.828000783920288 in 15.259046\n",
      "Epoch 1 Batch 32000 Loss 1.8481371402740479 in 15.196337\n",
      "saving model with loss 1.8481371402740479\n",
      "Epoch 1 Batch 32500 Loss 1.8338743448257446 in 15.225203\n",
      "Epoch 1 Batch 33000 Loss 1.6740407943725586 in 15.174660\n",
      "saving model with loss 1.6740407943725586\n",
      "Epoch 1 Batch 33500 Loss 1.987670660018921 in 15.240715\n",
      "Epoch 1 Batch 34000 Loss 1.875205397605896 in 15.184225\n",
      "saving model with loss 1.875205397605896\n",
      "Epoch 1 Batch 34500 Loss 1.8416748046875 in 15.268250\n",
      "Epoch 1 Batch 35000 Loss 1.7718368768692017 in 15.158836\n",
      "saving model with loss 1.7718368768692017\n",
      "Epoch 1 Batch 35500 Loss 1.7998170852661133 in 15.251245\n",
      "Epoch 1 Batch 36000 Loss 1.9780622720718384 in 15.171448\n",
      "saving model with loss 1.9780622720718384\n",
      "Epoch 1 Batch 36500 Loss 1.8116912841796875 in 15.242478\n",
      "Epoch 1 Batch 37000 Loss 1.836761236190796 in 15.184255\n",
      "saving model with loss 1.836761236190796\n",
      "Epoch 1 Batch 37500 Loss 1.8005355596542358 in 15.244837\n",
      "Epoch 1 Batch 38000 Loss 1.897802710533142 in 15.204126\n",
      "saving model with loss 1.897802710533142\n",
      "Epoch 1 Batch 38500 Loss 1.8422486782073975 in 15.250815\n",
      "Epoch 1 Batch 39000 Loss 1.6990903615951538 in 15.175489\n",
      "saving model with loss 1.6990903615951538\n",
      "Epoch 1 Batch 39500 Loss 1.8488385677337646 in 15.231457\n",
      "Epoch 1 Batch 40000 Loss 1.842307686805725 in 15.158114\n",
      "saving model with loss 1.842307686805725\n",
      "Epoch 1 Batch 40500 Loss 1.8935902118682861 in 15.246164\n",
      "Epoch 1 Batch 41000 Loss 1.993444800376892 in 15.185008\n",
      "saving model with loss 1.993444800376892\n",
      "Epoch 1 Batch 41500 Loss 1.9676666259765625 in 15.254363\n",
      "Epoch 1 Batch 42000 Loss 1.9851906299591064 in 15.181251\n",
      "saving model with loss 1.9851906299591064\n",
      "Epoch 1 Batch 42500 Loss 1.791467308998108 in 15.226982\n",
      "Epoch 1 Batch 43000 Loss 2.0635082721710205 in 15.154130\n",
      "saving model with loss 2.0635082721710205\n",
      "Epoch 1 Batch 43500 Loss 1.925676703453064 in 15.270319\n",
      "Epoch 1 Batch 44000 Loss 1.8896825313568115 in 15.181834\n",
      "saving model with loss 1.8896825313568115\n",
      "Epoch 1 Batch 44500 Loss 1.9580081701278687 in 15.270581\n",
      "Epoch 1 Batch 45000 Loss 1.849230408668518 in 15.181620\n",
      "saving model with loss 1.849230408668518\n",
      "Epoch 1 Batch 45500 Loss 1.8115812540054321 in 15.252095\n",
      "Epoch 1 Batch 46000 Loss 1.7839000225067139 in 15.174389\n",
      "saving model with loss 1.7839000225067139\n",
      "Epoch 1 Batch 46500 Loss 1.8249633312225342 in 15.246513\n",
      "Epoch 1 Batch 47000 Loss 1.9203916788101196 in 15.188749\n",
      "saving model with loss 1.9203916788101196\n",
      "Epoch 1 Batch 47500 Loss 1.8717607259750366 in 15.247677\n",
      "Epoch 1 Batch 48000 Loss 1.9159084558486938 in 15.162429\n",
      "saving model with loss 1.9159084558486938\n",
      "Epoch 1 Batch 48500 Loss 1.8536790609359741 in 15.245934\n",
      "Epoch 1 Batch 49000 Loss 1.81852126121521 in 15.180214\n",
      "saving model with loss 1.81852126121521\n",
      "Epoch 1 Batch 49500 Loss 1.907408356666565 in 15.247131\n",
      "Epoch 1 Batch 50000 Loss 1.8240324258804321 in 15.177129\n",
      "saving model with loss 1.8240324258804321\n",
      "Epoch 1 Batch 50500 Loss 1.7038383483886719 in 15.245099\n",
      "Epoch 1 Batch 51000 Loss 1.8316303491592407 in 15.182172\n",
      "saving model with loss 1.8316303491592407\n",
      "Epoch 1 Batch 51500 Loss 1.7380342483520508 in 15.267604\n",
      "Epoch 1 Batch 52000 Loss 1.7908716201782227 in 15.181668\n",
      "saving model with loss 1.7908716201782227\n",
      "Epoch 1 Batch 52500 Loss 1.9520561695098877 in 15.232122\n",
      "Epoch 1 Batch 53000 Loss 1.9735193252563477 in 15.193212\n",
      "saving model with loss 1.9735193252563477\n",
      "Epoch 1 Batch 53500 Loss 1.843705415725708 in 15.276325\n",
      "Epoch 1 Batch 54000 Loss 1.8270267248153687 in 15.179086\n",
      "saving model with loss 1.8270267248153687\n",
      "Epoch 1 Batch 54500 Loss 1.7928546667099 in 15.252362\n",
      "Epoch 1 Batch 55000 Loss 1.8193776607513428 in 15.202193\n",
      "saving model with loss 1.8193776607513428\n",
      "Epoch 1 Batch 55500 Loss 1.9007256031036377 in 15.246226\n",
      "Epoch 1 Batch 56000 Loss 1.8271077871322632 in 15.190934\n",
      "saving model with loss 1.8271077871322632\n",
      "Epoch 1 Batch 56500 Loss 1.9091002941131592 in 15.261354\n",
      "Epoch 1 Batch 57000 Loss 1.892748236656189 in 15.179701\n",
      "saving model with loss 1.892748236656189\n",
      "Epoch 1 Batch 57500 Loss 1.9211620092391968 in 15.284117\n",
      "Epoch 1 Batch 58000 Loss 1.8095585107803345 in 15.168799\n",
      "saving model with loss 1.8095585107803345\n",
      "Epoch 1 Batch 58500 Loss 1.8801358938217163 in 15.245554\n",
      "Epoch 1 Batch 59000 Loss 1.9824272394180298 in 15.197740\n",
      "saving model with loss 1.9824272394180298\n",
      "Epoch 1 Batch 59500 Loss 1.7493575811386108 in 15.256155\n",
      "Epoch 1 Batch 60000 Loss 1.9163148403167725 in 15.198466\n",
      "saving model with loss 1.9163148403167725\n",
      "Epoch 1 Batch 60500 Loss 1.9109519720077515 in 15.225680\n",
      "Epoch 1 Batch 61000 Loss 1.908678412437439 in 15.165240\n",
      "saving model with loss 1.908678412437439\n",
      "Epoch 1 Batch 61500 Loss 1.7465842962265015 in 15.246339\n",
      "Epoch 1 Batch 62000 Loss 1.87551748752594 in 15.157912\n",
      "saving model with loss 1.87551748752594\n",
      "Epoch 1 Batch 62500 Loss 1.8096929788589478 in 15.236993\n",
      "Epoch 1 Batch 63000 Loss 1.7598354816436768 in 15.175623\n",
      "saving model with loss 1.7598354816436768\n",
      "Epoch 1 Batch 63500 Loss 1.7000010013580322 in 15.247636\n",
      "Epoch 1 Batch 64000 Loss 1.8031413555145264 in 15.201880\n",
      "saving model with loss 1.8031413555145264\n",
      "Epoch 1 Batch 64500 Loss 1.9476029872894287 in 15.231255\n",
      "Epoch 1 Batch 65000 Loss 1.9472768306732178 in 15.198744\n",
      "saving model with loss 1.9472768306732178\n",
      "Epoch 1 Batch 65500 Loss 1.7857319116592407 in 15.278112\n",
      "Epoch 1 Batch 66000 Loss 1.9799001216888428 in 15.179493\n",
      "saving model with loss 1.9799001216888428\n",
      "Epoch 1 Batch 66500 Loss 1.8774404525756836 in 15.210638\n",
      "Epoch 1 Batch 67000 Loss 1.7238938808441162 in 15.153095\n",
      "saving model with loss 1.7238938808441162\n",
      "Epoch 1 Batch 67500 Loss 2.054053783416748 in 15.244407\n",
      "Epoch 1 Batch 68000 Loss 1.8614673614501953 in 15.147584\n",
      "saving model with loss 1.8614673614501953\n",
      "Epoch 1 Batch 68500 Loss 1.6588128805160522 in 15.229790\n",
      "Epoch 1 Batch 69000 Loss 1.7324622869491577 in 15.163351\n",
      "saving model with loss 1.7324622869491577\n",
      "Epoch 1 Batch 69500 Loss 1.8824011087417603 in 15.245866\n",
      "Epoch 1 Batch 70000 Loss 1.8394148349761963 in 15.150101\n",
      "saving model with loss 1.8394148349761963\n",
      "Epoch 1 Batch 70500 Loss 1.8607107400894165 in 15.216113\n",
      "Epoch 2 Loss 1.977705717086792\n",
      "Time taken for 1 epoch 2148.2036514282227\n",
      "number of batches : 70553\n",
      "Epoch 2 Batch 0 Loss 1.8187763690948486 in 1.156100\n",
      "saving model with loss 1.8187763690948486\n",
      "Epoch 2 Batch 500 Loss 1.9701019525527954 in 15.223336\n",
      "Epoch 2 Batch 1000 Loss 1.7802398204803467 in 15.150270\n",
      "saving model with loss 1.7802398204803467\n",
      "Epoch 2 Batch 1500 Loss 1.8580684661865234 in 15.238552\n",
      "Epoch 2 Batch 2000 Loss 1.784894585609436 in 15.154010\n",
      "saving model with loss 1.784894585609436\n",
      "Epoch 2 Batch 2500 Loss 1.8354610204696655 in 15.223506\n",
      "Epoch 2 Batch 3000 Loss 1.928147554397583 in 15.168293\n",
      "saving model with loss 1.928147554397583\n",
      "Epoch 2 Batch 3500 Loss 1.7860759496688843 in 15.230215\n",
      "Epoch 2 Batch 4000 Loss 1.7540308237075806 in 15.162525\n",
      "saving model with loss 1.7540308237075806\n",
      "Epoch 2 Batch 4500 Loss 1.9269211292266846 in 15.212850\n",
      "Epoch 2 Batch 5000 Loss 1.86348557472229 in 15.152336\n",
      "saving model with loss 1.86348557472229\n",
      "Epoch 2 Batch 5500 Loss 1.8524110317230225 in 15.272038\n",
      "Epoch 2 Batch 6000 Loss 1.8987524509429932 in 15.151516\n",
      "saving model with loss 1.8987524509429932\n",
      "Epoch 2 Batch 6500 Loss 1.9338089227676392 in 15.215069\n",
      "Epoch 2 Batch 7000 Loss 1.8599364757537842 in 15.179600\n",
      "saving model with loss 1.8599364757537842\n",
      "Epoch 2 Batch 7500 Loss 1.779170036315918 in 15.241677\n",
      "Epoch 2 Batch 8000 Loss 1.932863473892212 in 15.150743\n",
      "saving model with loss 1.932863473892212\n",
      "Epoch 2 Batch 8500 Loss 1.8033411502838135 in 15.211843\n",
      "Epoch 2 Batch 9000 Loss 1.8773109912872314 in 15.154687\n",
      "saving model with loss 1.8773109912872314\n",
      "Epoch 2 Batch 9500 Loss 1.9065507650375366 in 15.241384\n",
      "Epoch 2 Batch 10000 Loss 1.7746353149414062 in 15.151916\n",
      "saving model with loss 1.7746353149414062\n",
      "Epoch 2 Batch 10500 Loss 1.8836181163787842 in 15.211411\n",
      "Epoch 2 Batch 11000 Loss 1.8305202722549438 in 15.160572\n",
      "saving model with loss 1.8305202722549438\n",
      "Epoch 2 Batch 11500 Loss 1.94242262840271 in 15.234298\n",
      "Epoch 2 Batch 12000 Loss 1.8871736526489258 in 15.147169\n",
      "saving model with loss 1.8871736526489258\n",
      "Epoch 2 Batch 12500 Loss 1.851398229598999 in 15.214078\n",
      "Epoch 2 Batch 13000 Loss 1.7722327709197998 in 15.150732\n",
      "saving model with loss 1.7722327709197998\n",
      "Epoch 2 Batch 13500 Loss 1.8275988101959229 in 15.215937\n",
      "Epoch 2 Batch 14000 Loss 1.9796886444091797 in 15.149110\n",
      "saving model with loss 1.9796886444091797\n",
      "Epoch 2 Batch 14500 Loss 1.9459186792373657 in 15.241765\n",
      "Epoch 2 Batch 15000 Loss 1.9194376468658447 in 15.184067\n",
      "saving model with loss 1.9194376468658447\n",
      "Epoch 2 Batch 15500 Loss 1.8981075286865234 in 15.238121\n",
      "Epoch 2 Batch 16000 Loss 1.7803577184677124 in 15.152717\n",
      "saving model with loss 1.7803577184677124\n",
      "Epoch 2 Batch 16500 Loss 1.7151168584823608 in 15.243964\n",
      "Epoch 2 Batch 17000 Loss 1.7406120300292969 in 15.212423\n",
      "saving model with loss 1.7406120300292969\n",
      "Epoch 2 Batch 17500 Loss 1.9482618570327759 in 15.207799\n",
      "Epoch 2 Batch 18000 Loss 1.8827604055404663 in 15.153743\n",
      "saving model with loss 1.8827604055404663\n",
      "Epoch 2 Batch 18500 Loss 1.836016058921814 in 15.211543\n",
      "Epoch 2 Batch 19000 Loss 1.9622409343719482 in 15.151801\n",
      "saving model with loss 1.9622409343719482\n",
      "Epoch 2 Batch 19500 Loss 1.8287769556045532 in 15.209312\n",
      "Epoch 2 Batch 20000 Loss 1.7782866954803467 in 15.146111\n",
      "saving model with loss 1.7782866954803467\n",
      "Epoch 2 Batch 20500 Loss 1.988203763961792 in 15.215434\n",
      "Epoch 2 Batch 21000 Loss 1.964337944984436 in 15.155393\n",
      "saving model with loss 1.964337944984436\n",
      "Epoch 2 Batch 21500 Loss 1.8916181325912476 in 15.209662\n",
      "Epoch 2 Batch 22000 Loss 1.8138784170150757 in 15.128002\n",
      "saving model with loss 1.8138784170150757\n",
      "Epoch 2 Batch 22500 Loss 1.8615353107452393 in 15.203067\n",
      "Epoch 2 Batch 23000 Loss 1.85373055934906 in 15.155040\n",
      "saving model with loss 1.85373055934906\n",
      "Epoch 2 Batch 23500 Loss 1.7606704235076904 in 15.195406\n",
      "Epoch 2 Batch 24000 Loss 1.8551286458969116 in 15.152625\n",
      "saving model with loss 1.8551286458969116\n",
      "Epoch 2 Batch 24500 Loss 1.8792459964752197 in 15.213243\n",
      "Epoch 2 Batch 25000 Loss 1.727318525314331 in 15.151267\n",
      "saving model with loss 1.727318525314331\n",
      "Epoch 2 Batch 25500 Loss 1.7630589008331299 in 15.209808\n",
      "Epoch 2 Batch 26000 Loss 1.8363285064697266 in 15.152527\n",
      "saving model with loss 1.8363285064697266\n",
      "Epoch 2 Batch 26500 Loss 1.8218570947647095 in 15.212441\n",
      "Epoch 2 Batch 27000 Loss 1.853706955909729 in 15.144878\n",
      "saving model with loss 1.853706955909729\n",
      "Epoch 2 Batch 27500 Loss 1.7235071659088135 in 15.222304\n",
      "Epoch 2 Batch 28000 Loss 1.822298288345337 in 15.147249\n",
      "saving model with loss 1.822298288345337\n",
      "Epoch 2 Batch 28500 Loss 1.8143415451049805 in 15.211740\n",
      "Epoch 2 Batch 29000 Loss 1.856114387512207 in 15.145148\n",
      "saving model with loss 1.856114387512207\n",
      "Epoch 2 Batch 29500 Loss 1.7564932107925415 in 15.217403\n",
      "Epoch 2 Batch 30000 Loss 1.7616615295410156 in 15.154179\n",
      "saving model with loss 1.7616615295410156\n",
      "Epoch 2 Batch 30500 Loss 1.8888801336288452 in 15.208470\n",
      "Epoch 2 Batch 31000 Loss 1.8312218189239502 in 15.152641\n",
      "saving model with loss 1.8312218189239502\n",
      "Epoch 2 Batch 31500 Loss 1.7467550039291382 in 15.228901\n",
      "Epoch 2 Batch 32000 Loss 1.8215980529785156 in 15.149810\n",
      "saving model with loss 1.8215980529785156\n",
      "Epoch 2 Batch 32500 Loss 1.8474289178848267 in 15.611163\n",
      "Epoch 2 Batch 33000 Loss 1.9032726287841797 in 15.167616\n",
      "saving model with loss 1.9032726287841797\n",
      "Epoch 2 Batch 33500 Loss 1.8158546686172485 in 15.255623\n",
      "Epoch 2 Batch 34000 Loss 1.8171603679656982 in 15.150694\n",
      "saving model with loss 1.8171603679656982\n",
      "Epoch 2 Batch 34500 Loss 1.9214916229248047 in 15.215614\n",
      "Epoch 2 Batch 35000 Loss 1.8554236888885498 in 15.163723\n",
      "saving model with loss 1.8554236888885498\n",
      "Epoch 2 Batch 35500 Loss 1.7675955295562744 in 15.229448\n",
      "Epoch 2 Batch 36000 Loss 1.7175390720367432 in 15.150321\n",
      "saving model with loss 1.7175390720367432\n",
      "Epoch 2 Batch 36500 Loss 1.8471348285675049 in 15.214497\n",
      "Epoch 2 Batch 37000 Loss 1.8679511547088623 in 15.152465\n",
      "saving model with loss 1.8679511547088623\n",
      "Epoch 2 Batch 37500 Loss 1.7944676876068115 in 15.208327\n",
      "Epoch 2 Batch 38000 Loss 1.8836416006088257 in 15.152174\n",
      "saving model with loss 1.8836416006088257\n",
      "Epoch 2 Batch 38500 Loss 1.8724056482315063 in 15.212972\n",
      "Epoch 2 Batch 39000 Loss 1.8738752603530884 in 15.161056\n",
      "saving model with loss 1.8738752603530884\n",
      "Epoch 2 Batch 39500 Loss 1.9269218444824219 in 15.383351\n",
      "Epoch 2 Batch 40000 Loss 1.8903290033340454 in 15.151894\n",
      "saving model with loss 1.8903290033340454\n",
      "Epoch 2 Batch 40500 Loss 1.833367109298706 in 15.250394\n",
      "Epoch 2 Batch 41000 Loss 1.8243404626846313 in 15.175447\n",
      "saving model with loss 1.8243404626846313\n",
      "Epoch 2 Batch 41500 Loss 1.911138892173767 in 15.226469\n",
      "Epoch 2 Batch 42000 Loss 1.9527623653411865 in 15.165874\n",
      "saving model with loss 1.9527623653411865\n",
      "Epoch 2 Batch 42500 Loss 1.7699363231658936 in 15.225955\n",
      "Epoch 2 Batch 43000 Loss 1.838660478591919 in 15.156364\n",
      "saving model with loss 1.838660478591919\n",
      "Epoch 2 Batch 43500 Loss 1.9110530614852905 in 15.240075\n",
      "Epoch 2 Batch 44000 Loss 1.7785730361938477 in 15.182792\n",
      "saving model with loss 1.7785730361938477\n",
      "Epoch 2 Batch 44500 Loss 1.9052231311798096 in 15.243742\n",
      "Epoch 2 Batch 45000 Loss 1.9103057384490967 in 15.160762\n",
      "saving model with loss 1.9103057384490967\n",
      "Epoch 2 Batch 45500 Loss 1.8005874156951904 in 15.240072\n",
      "Epoch 2 Batch 46000 Loss 1.918058156967163 in 15.169960\n",
      "saving model with loss 1.918058156967163\n",
      "Epoch 2 Batch 46500 Loss 1.7671868801116943 in 15.262097\n",
      "Epoch 2 Batch 47000 Loss 1.795103669166565 in 15.163413\n",
      "saving model with loss 1.795103669166565\n",
      "Epoch 2 Batch 47500 Loss 1.8481152057647705 in 15.303676\n",
      "Epoch 2 Batch 48000 Loss 1.7333917617797852 in 15.147257\n",
      "saving model with loss 1.7333917617797852\n",
      "Epoch 2 Batch 48500 Loss 1.9292194843292236 in 15.245424\n",
      "Epoch 2 Batch 49000 Loss 1.9824374914169312 in 15.153653\n",
      "saving model with loss 1.9824374914169312\n",
      "Epoch 2 Batch 49500 Loss 1.7965452671051025 in 15.252768\n",
      "Epoch 2 Batch 50000 Loss 1.9283643960952759 in 15.181383\n",
      "saving model with loss 1.9283643960952759\n",
      "Epoch 2 Batch 50500 Loss 1.8318145275115967 in 15.228567\n",
      "Epoch 2 Batch 51000 Loss 1.8853633403778076 in 15.151841\n",
      "saving model with loss 1.8853633403778076\n",
      "Epoch 2 Batch 51500 Loss 1.8549779653549194 in 15.243511\n",
      "Epoch 2 Batch 52000 Loss 1.7878005504608154 in 15.152129\n",
      "saving model with loss 1.7878005504608154\n",
      "Epoch 2 Batch 52500 Loss 1.8688457012176514 in 15.210134\n",
      "Epoch 2 Batch 53000 Loss 1.8043744564056396 in 15.155099\n",
      "saving model with loss 1.8043744564056396\n",
      "Epoch 2 Batch 53500 Loss 1.7345250844955444 in 15.243944\n",
      "Epoch 2 Batch 54000 Loss 1.8139817714691162 in 15.179384\n",
      "saving model with loss 1.8139817714691162\n",
      "Epoch 2 Batch 54500 Loss 1.7946809530258179 in 15.213496\n",
      "Epoch 2 Batch 55000 Loss 1.8011423349380493 in 15.147512\n",
      "saving model with loss 1.8011423349380493\n",
      "Epoch 2 Batch 55500 Loss 1.8202831745147705 in 15.243664\n",
      "Epoch 2 Batch 56000 Loss 1.8066093921661377 in 15.168703\n",
      "saving model with loss 1.8066093921661377\n",
      "Epoch 2 Batch 56500 Loss 1.886254906654358 in 15.238707\n",
      "Epoch 2 Batch 57000 Loss 1.7473806142807007 in 15.166453\n",
      "saving model with loss 1.7473806142807007\n",
      "Epoch 2 Batch 57500 Loss 1.7136287689208984 in 15.244560\n",
      "Epoch 2 Batch 58000 Loss 1.9383280277252197 in 15.151244\n",
      "saving model with loss 1.9383280277252197\n",
      "Epoch 2 Batch 58500 Loss 1.804149866104126 in 15.211145\n",
      "Epoch 2 Batch 59000 Loss 1.8525171279907227 in 15.160773\n",
      "saving model with loss 1.8525171279907227\n",
      "Epoch 2 Batch 59500 Loss 1.9453957080841064 in 15.237683\n",
      "Epoch 2 Batch 60000 Loss 1.9153449535369873 in 15.176319\n",
      "saving model with loss 1.9153449535369873\n",
      "Epoch 2 Batch 60500 Loss 1.9467054605484009 in 15.245849\n",
      "Epoch 2 Batch 61000 Loss 1.7894527912139893 in 15.180334\n",
      "saving model with loss 1.7894527912139893\n",
      "Epoch 2 Batch 61500 Loss 1.892740249633789 in 15.246590\n",
      "Epoch 2 Batch 62000 Loss 1.9061428308486938 in 15.149944\n",
      "saving model with loss 1.9061428308486938\n",
      "Epoch 2 Batch 62500 Loss 1.797937035560608 in 15.237387\n",
      "Epoch 2 Batch 63000 Loss 1.9045054912567139 in 15.150220\n",
      "saving model with loss 1.9045054912567139\n",
      "Epoch 2 Batch 63500 Loss 1.881331205368042 in 15.242077\n",
      "Epoch 2 Batch 64000 Loss 1.722127914428711 in 15.151758\n",
      "saving model with loss 1.722127914428711\n",
      "Epoch 2 Batch 64500 Loss 1.9720628261566162 in 15.223622\n",
      "Epoch 2 Batch 65000 Loss 1.8338873386383057 in 15.231366\n",
      "saving model with loss 1.8338873386383057\n",
      "Epoch 2 Batch 65500 Loss 1.7630695104599 in 15.213185\n",
      "Epoch 2 Batch 66000 Loss 1.8839479684829712 in 15.146266\n",
      "saving model with loss 1.8839479684829712\n",
      "Epoch 2 Batch 66500 Loss 1.6768569946289062 in 15.212293\n",
      "Epoch 2 Batch 67000 Loss 1.8048651218414307 in 15.154678\n",
      "saving model with loss 1.8048651218414307\n",
      "Epoch 2 Batch 67500 Loss 1.769562005996704 in 15.210610\n",
      "Epoch 2 Batch 68000 Loss 1.938568115234375 in 15.149874\n",
      "saving model with loss 1.938568115234375\n",
      "Epoch 2 Batch 68500 Loss 1.8100299835205078 in 15.199717\n",
      "Epoch 2 Batch 69000 Loss 1.8789470195770264 in 15.151202\n",
      "saving model with loss 1.8789470195770264\n",
      "Epoch 2 Batch 69500 Loss 1.918310523033142 in 15.211288\n",
      "Epoch 2 Batch 70000 Loss 1.8656173944473267 in 15.152373\n",
      "saving model with loss 1.8656173944473267\n",
      "Epoch 2 Batch 70500 Loss 1.7978070974349976 in 15.210388\n",
      "Epoch 3 Loss 1.7145341634750366\n",
      "Time taken for 1 epoch 2145.573744535446\n",
      "number of batches : 70553\n",
      "Epoch 3 Batch 0 Loss 1.7909739017486572 in 1.179906\n",
      "saving model with loss 1.7909739017486572\n",
      "Epoch 3 Batch 500 Loss 1.9097301959991455 in 15.199578\n",
      "Epoch 3 Batch 1000 Loss 1.8313671350479126 in 15.147414\n",
      "saving model with loss 1.8313671350479126\n",
      "Epoch 3 Batch 1500 Loss 1.8537086248397827 in 15.212911\n",
      "Epoch 3 Batch 2000 Loss 1.773506760597229 in 15.152391\n",
      "saving model with loss 1.773506760597229\n",
      "Epoch 3 Batch 2500 Loss 1.7884082794189453 in 15.214256\n",
      "Epoch 3 Batch 3000 Loss 1.7124595642089844 in 15.163805\n",
      "saving model with loss 1.7124595642089844\n",
      "Epoch 3 Batch 3500 Loss 1.7747180461883545 in 15.211874\n",
      "Epoch 3 Batch 4000 Loss 1.8646793365478516 in 15.152181\n",
      "saving model with loss 1.8646793365478516\n",
      "Epoch 3 Batch 4500 Loss 1.8643438816070557 in 15.214355\n",
      "Epoch 3 Batch 5000 Loss 1.8790029287338257 in 15.149375\n",
      "saving model with loss 1.8790029287338257\n",
      "Epoch 3 Batch 5500 Loss 1.9010846614837646 in 15.213516\n",
      "Epoch 3 Batch 6000 Loss 1.8464571237564087 in 15.150492\n",
      "saving model with loss 1.8464571237564087\n",
      "Epoch 3 Batch 6500 Loss 1.8507843017578125 in 15.357697\n",
      "Epoch 3 Batch 7000 Loss 1.8386867046356201 in 15.248154\n",
      "saving model with loss 1.8386867046356201\n",
      "Epoch 3 Batch 7500 Loss 2.0171008110046387 in 15.214426\n",
      "Epoch 3 Batch 8000 Loss 1.9610989093780518 in 15.143352\n",
      "saving model with loss 1.9610989093780518\n",
      "Epoch 3 Batch 8500 Loss 1.8073151111602783 in 15.219014\n",
      "Epoch 3 Batch 9000 Loss 1.8496153354644775 in 15.150868\n",
      "saving model with loss 1.8496153354644775\n",
      "Epoch 3 Batch 9500 Loss 1.7928104400634766 in 15.210883\n",
      "Epoch 3 Batch 10000 Loss 2.0313217639923096 in 15.155013\n",
      "saving model with loss 2.0313217639923096\n",
      "Epoch 3 Batch 10500 Loss 1.740837812423706 in 15.209515\n",
      "Epoch 3 Batch 11000 Loss 1.8244041204452515 in 15.148575\n",
      "saving model with loss 1.8244041204452515\n",
      "Epoch 3 Batch 11500 Loss 1.8703346252441406 in 15.212872\n",
      "Epoch 3 Batch 12000 Loss 1.8682136535644531 in 15.151501\n",
      "saving model with loss 1.8682136535644531\n",
      "Epoch 3 Batch 12500 Loss 1.8644250631332397 in 15.220865\n",
      "Epoch 3 Batch 13000 Loss 1.779797911643982 in 15.145874\n",
      "saving model with loss 1.779797911643982\n",
      "Epoch 3 Batch 13500 Loss 1.9282395839691162 in 15.207876\n",
      "Epoch 3 Batch 14000 Loss 1.8336842060089111 in 15.153759\n",
      "saving model with loss 1.8336842060089111\n",
      "Epoch 3 Batch 14500 Loss 1.6796026229858398 in 15.212607\n",
      "Epoch 3 Batch 15000 Loss 1.9653615951538086 in 15.147944\n",
      "saving model with loss 1.9653615951538086\n",
      "Epoch 3 Batch 15500 Loss 1.7853893041610718 in 15.215967\n",
      "Epoch 3 Batch 16000 Loss 1.8299274444580078 in 15.150748\n",
      "saving model with loss 1.8299274444580078\n",
      "Epoch 3 Batch 16500 Loss 1.8393350839614868 in 15.210510\n",
      "Epoch 3 Batch 17000 Loss 1.8297113180160522 in 15.151254\n",
      "saving model with loss 1.8297113180160522\n",
      "Epoch 3 Batch 17500 Loss 1.812183141708374 in 15.213987\n",
      "Epoch 3 Batch 18000 Loss 1.9111019372940063 in 15.152368\n",
      "saving model with loss 1.9111019372940063\n",
      "Epoch 3 Batch 18500 Loss 1.805833101272583 in 15.211961\n",
      "Epoch 3 Batch 19000 Loss 1.6474082469940186 in 15.147463\n",
      "saving model with loss 1.6474082469940186\n",
      "Epoch 3 Batch 19500 Loss 1.8549087047576904 in 15.212775\n",
      "Epoch 3 Batch 20000 Loss 1.7863645553588867 in 15.152288\n",
      "saving model with loss 1.7863645553588867\n",
      "Epoch 3 Batch 20500 Loss 1.823665976524353 in 15.210415\n",
      "Epoch 3 Batch 21000 Loss 1.8269494771957397 in 15.155801\n",
      "saving model with loss 1.8269494771957397\n",
      "Epoch 3 Batch 21500 Loss 1.8415794372558594 in 15.225169\n",
      "Epoch 3 Batch 22000 Loss 1.8835729360580444 in 15.150370\n",
      "saving model with loss 1.8835729360580444\n",
      "Epoch 3 Batch 22500 Loss 1.7658360004425049 in 15.213471\n",
      "Epoch 3 Batch 23000 Loss 1.7564109563827515 in 15.150347\n",
      "saving model with loss 1.7564109563827515\n",
      "Epoch 3 Batch 23500 Loss 1.7573881149291992 in 15.217879\n",
      "Epoch 3 Batch 24000 Loss 1.8567250967025757 in 15.150608\n",
      "saving model with loss 1.8567250967025757\n",
      "Epoch 3 Batch 24500 Loss 1.864810585975647 in 15.206920\n",
      "Epoch 3 Batch 25000 Loss 1.77923583984375 in 15.153582\n",
      "saving model with loss 1.77923583984375\n",
      "Epoch 3 Batch 25500 Loss 1.7752692699432373 in 15.210439\n",
      "Epoch 3 Batch 26000 Loss 1.6640888452529907 in 15.153740\n",
      "saving model with loss 1.6640888452529907\n",
      "Epoch 3 Batch 26500 Loss 1.7827701568603516 in 15.214146\n",
      "Epoch 3 Batch 27000 Loss 1.8936455249786377 in 15.149265\n",
      "saving model with loss 1.8936455249786377\n",
      "Epoch 3 Batch 27500 Loss 1.7578548192977905 in 15.211736\n",
      "Epoch 3 Batch 28000 Loss 1.8321864604949951 in 15.195313\n",
      "saving model with loss 1.8321864604949951\n",
      "Epoch 3 Batch 28500 Loss 1.7849565744400024 in 15.231686\n",
      "Epoch 3 Batch 29000 Loss 1.9180996417999268 in 15.145633\n",
      "saving model with loss 1.9180996417999268\n",
      "Epoch 3 Batch 29500 Loss 1.8232135772705078 in 15.216082\n",
      "Epoch 3 Batch 30000 Loss 1.9037840366363525 in 15.149189\n",
      "saving model with loss 1.9037840366363525\n",
      "Epoch 3 Batch 30500 Loss 1.8207175731658936 in 15.210330\n",
      "Epoch 3 Batch 31000 Loss 1.7615480422973633 in 15.152141\n",
      "saving model with loss 1.7615480422973633\n",
      "Epoch 3 Batch 31500 Loss 1.8490089178085327 in 15.212323\n",
      "Epoch 3 Batch 32000 Loss 1.6944106817245483 in 15.153477\n",
      "saving model with loss 1.6944106817245483\n",
      "Epoch 3 Batch 32500 Loss 1.839740514755249 in 15.209407\n",
      "Epoch 3 Batch 33000 Loss 1.890068769454956 in 15.149137\n",
      "saving model with loss 1.890068769454956\n",
      "Epoch 3 Batch 33500 Loss 1.8889400959014893 in 15.217512\n",
      "Epoch 3 Batch 34000 Loss 1.9133355617523193 in 15.148123\n",
      "saving model with loss 1.9133355617523193\n",
      "Epoch 3 Batch 34500 Loss 1.8508083820343018 in 15.215902\n",
      "Epoch 3 Batch 35000 Loss 1.7759733200073242 in 15.151036\n",
      "saving model with loss 1.7759733200073242\n",
      "Epoch 3 Batch 35500 Loss 1.9567981958389282 in 15.212031\n",
      "Epoch 3 Batch 36000 Loss 1.7750728130340576 in 15.180203\n",
      "saving model with loss 1.7750728130340576\n",
      "Epoch 3 Batch 36500 Loss 1.7898054122924805 in 15.211763\n",
      "Epoch 3 Batch 37000 Loss 1.8805046081542969 in 15.149377\n",
      "saving model with loss 1.8805046081542969\n",
      "Epoch 3 Batch 37500 Loss 1.7038971185684204 in 15.214010\n",
      "Epoch 3 Batch 38000 Loss 1.7520439624786377 in 15.148258\n",
      "saving model with loss 1.7520439624786377\n",
      "Epoch 3 Batch 38500 Loss 1.844926118850708 in 15.213956\n",
      "Epoch 3 Batch 39000 Loss 1.8178373575210571 in 15.150016\n",
      "saving model with loss 1.8178373575210571\n",
      "Epoch 3 Batch 39500 Loss 1.8022801876068115 in 15.216755\n",
      "Epoch 3 Batch 40000 Loss 1.9715887308120728 in 15.149728\n",
      "saving model with loss 1.9715887308120728\n",
      "Epoch 3 Batch 40500 Loss 1.8196089267730713 in 15.209614\n",
      "Epoch 3 Batch 41000 Loss 1.901418924331665 in 15.153018\n",
      "saving model with loss 1.901418924331665\n",
      "Epoch 3 Batch 41500 Loss 1.8212525844573975 in 15.612635\n",
      "Epoch 3 Batch 42000 Loss 1.8862720727920532 in 15.179198\n",
      "saving model with loss 1.8862720727920532\n",
      "Epoch 3 Batch 42500 Loss 1.8532873392105103 in 15.239395\n",
      "Epoch 3 Batch 43000 Loss 1.835619330406189 in 15.173498\n",
      "saving model with loss 1.835619330406189\n",
      "Epoch 3 Batch 43500 Loss 1.7867090702056885 in 15.223469\n",
      "Epoch 3 Batch 44000 Loss 1.8383140563964844 in 15.154428\n",
      "saving model with loss 1.8383140563964844\n",
      "Epoch 3 Batch 44500 Loss 1.840993881225586 in 15.261478\n",
      "Epoch 3 Batch 45000 Loss 1.9100830554962158 in 15.155945\n",
      "saving model with loss 1.9100830554962158\n",
      "Epoch 3 Batch 45500 Loss 1.9035861492156982 in 15.263248\n",
      "Epoch 3 Batch 46000 Loss 1.7720590829849243 in 15.195767\n",
      "saving model with loss 1.7720590829849243\n",
      "Epoch 3 Batch 46500 Loss 1.9223636388778687 in 15.245470\n",
      "Epoch 3 Batch 47000 Loss 1.8811756372451782 in 15.171470\n",
      "saving model with loss 1.8811756372451782\n",
      "Epoch 3 Batch 47500 Loss 1.8165867328643799 in 15.246353\n",
      "Epoch 3 Batch 48000 Loss 1.7424488067626953 in 15.187232\n",
      "saving model with loss 1.7424488067626953\n",
      "Epoch 3 Batch 48500 Loss 1.8884214162826538 in 15.252381\n",
      "Epoch 3 Batch 49000 Loss 1.8913828134536743 in 15.191919\n",
      "saving model with loss 1.8913828134536743\n",
      "Epoch 3 Batch 49500 Loss 1.9888088703155518 in 15.238720\n",
      "Epoch 3 Batch 50000 Loss 1.841638207435608 in 15.193353\n",
      "saving model with loss 1.841638207435608\n",
      "Epoch 3 Batch 50500 Loss 1.8643152713775635 in 15.238876\n",
      "Epoch 3 Batch 51000 Loss 1.781609296798706 in 15.190276\n",
      "saving model with loss 1.781609296798706\n",
      "Epoch 3 Batch 51500 Loss 1.7466552257537842 in 15.236428\n",
      "Epoch 3 Batch 52000 Loss 1.7505581378936768 in 15.189239\n",
      "saving model with loss 1.7505581378936768\n",
      "Epoch 3 Batch 52500 Loss 1.9114558696746826 in 15.245445\n",
      "Epoch 3 Batch 53000 Loss 1.9135745763778687 in 15.181610\n",
      "saving model with loss 1.9135745763778687\n",
      "Epoch 3 Batch 53500 Loss 1.8244497776031494 in 15.234118\n",
      "Epoch 3 Batch 54000 Loss 1.7900480031967163 in 15.179735\n",
      "saving model with loss 1.7900480031967163\n",
      "Epoch 3 Batch 54500 Loss 1.8329662084579468 in 15.241641\n",
      "Epoch 3 Batch 55000 Loss 1.8676681518554688 in 15.150036\n",
      "saving model with loss 1.8676681518554688\n",
      "Epoch 3 Batch 55500 Loss 1.812805414199829 in 15.242351\n",
      "Epoch 3 Batch 56000 Loss 1.8730878829956055 in 15.181226\n",
      "saving model with loss 1.8730878829956055\n",
      "Epoch 3 Batch 56500 Loss 1.8361399173736572 in 15.255930\n",
      "Epoch 3 Batch 57000 Loss 1.8467772006988525 in 15.151654\n",
      "saving model with loss 1.8467772006988525\n",
      "Epoch 3 Batch 57500 Loss 1.7814109325408936 in 15.247603\n",
      "Epoch 3 Batch 58000 Loss 1.714605689048767 in 15.200987\n",
      "saving model with loss 1.714605689048767\n",
      "Epoch 3 Batch 58500 Loss 1.7251088619232178 in 15.242774\n",
      "Epoch 3 Batch 59000 Loss 1.8634744882583618 in 15.188496\n",
      "saving model with loss 1.8634744882583618\n",
      "Epoch 3 Batch 59500 Loss 1.7762295007705688 in 15.246540\n",
      "Epoch 3 Batch 60000 Loss 1.8192106485366821 in 15.168699\n",
      "saving model with loss 1.8192106485366821\n",
      "Epoch 3 Batch 60500 Loss 1.7780640125274658 in 15.245619\n",
      "Epoch 3 Batch 61000 Loss 1.9681406021118164 in 15.156917\n",
      "saving model with loss 1.9681406021118164\n",
      "Epoch 3 Batch 61500 Loss 1.8715633153915405 in 15.244000\n",
      "Epoch 3 Batch 62000 Loss 1.8908040523529053 in 15.153928\n",
      "saving model with loss 1.8908040523529053\n",
      "Epoch 3 Batch 62500 Loss 1.8647310733795166 in 15.286757\n",
      "Epoch 3 Batch 63000 Loss 1.7920490503311157 in 15.169081\n",
      "saving model with loss 1.7920490503311157\n",
      "Epoch 3 Batch 63500 Loss 1.7762256860733032 in 15.232341\n",
      "Epoch 3 Batch 64000 Loss 1.7830150127410889 in 15.174711\n",
      "saving model with loss 1.7830150127410889\n",
      "Epoch 3 Batch 64500 Loss 1.8427493572235107 in 15.247449\n",
      "Epoch 3 Batch 65000 Loss 1.7683109045028687 in 15.213308\n",
      "saving model with loss 1.7683109045028687\n",
      "Epoch 3 Batch 65500 Loss 1.8336025476455688 in 15.251935\n",
      "Epoch 3 Batch 66000 Loss 1.7994283437728882 in 15.176647\n",
      "saving model with loss 1.7994283437728882\n",
      "Epoch 3 Batch 66500 Loss 1.9455156326293945 in 15.231828\n",
      "Epoch 3 Batch 67000 Loss 1.8395426273345947 in 15.161300\n",
      "saving model with loss 1.8395426273345947\n",
      "Epoch 3 Batch 67500 Loss 1.786355972290039 in 15.217398\n",
      "Epoch 3 Batch 68000 Loss 1.8518249988555908 in 15.178686\n",
      "saving model with loss 1.8518249988555908\n",
      "Epoch 3 Batch 68500 Loss 1.8063037395477295 in 15.253848\n",
      "Epoch 3 Batch 69000 Loss 1.8636449575424194 in 15.179530\n",
      "saving model with loss 1.8636449575424194\n",
      "Epoch 3 Batch 69500 Loss 1.8717483282089233 in 15.233590\n",
      "Epoch 3 Batch 70000 Loss 1.9547138214111328 in 15.197739\n",
      "saving model with loss 1.9547138214111328\n",
      "Epoch 3 Batch 70500 Loss 1.7323341369628906 in 15.246726\n",
      "Epoch 4 Loss 1.8668909072875977\n",
      "Time taken for 1 epoch 2145.9230263233185\n",
      "number of batches : 70553\n",
      "Epoch 4 Batch 0 Loss 1.6336761713027954 in 1.178014\n",
      "saving model with loss 1.6336761713027954\n",
      "Epoch 4 Batch 500 Loss 1.9578616619110107 in 15.251978\n",
      "Epoch 4 Batch 1000 Loss 1.8890225887298584 in 15.148516\n",
      "saving model with loss 1.8890225887298584\n",
      "Epoch 4 Batch 1500 Loss 1.9502642154693604 in 15.245011\n",
      "Epoch 4 Batch 2000 Loss 1.9237687587738037 in 15.149952\n",
      "saving model with loss 1.9237687587738037\n",
      "Epoch 4 Batch 2500 Loss 1.723615288734436 in 15.245940\n",
      "Epoch 4 Batch 3000 Loss 1.905154824256897 in 15.177063\n",
      "saving model with loss 1.905154824256897\n",
      "Epoch 4 Batch 3500 Loss 1.6776540279388428 in 15.223252\n",
      "Epoch 4 Batch 4000 Loss 1.8866058588027954 in 15.179139\n",
      "saving model with loss 1.8866058588027954\n",
      "Epoch 4 Batch 4500 Loss 1.9565633535385132 in 15.268466\n",
      "Epoch 4 Batch 5000 Loss 1.8203321695327759 in 15.180916\n",
      "saving model with loss 1.8203321695327759\n",
      "Epoch 4 Batch 5500 Loss 1.9278780221939087 in 15.247087\n",
      "Epoch 4 Batch 6000 Loss 1.8599140644073486 in 15.178363\n",
      "saving model with loss 1.8599140644073486\n",
      "Epoch 4 Batch 6500 Loss 1.8204551935195923 in 15.257775\n",
      "Epoch 4 Batch 7000 Loss 1.8776203393936157 in 15.194919\n",
      "saving model with loss 1.8776203393936157\n",
      "Epoch 4 Batch 7500 Loss 1.9648609161376953 in 15.240938\n",
      "Epoch 4 Batch 8000 Loss 1.8118393421173096 in 15.148455\n",
      "saving model with loss 1.8118393421173096\n",
      "Epoch 4 Batch 8500 Loss 1.8209936618804932 in 15.243065\n",
      "Epoch 4 Batch 9000 Loss 1.8470033407211304 in 15.153980\n",
      "saving model with loss 1.8470033407211304\n",
      "Epoch 4 Batch 9500 Loss 1.8127645254135132 in 15.247988\n",
      "Epoch 4 Batch 10000 Loss 1.6935577392578125 in 15.172468\n",
      "saving model with loss 1.6935577392578125\n",
      "Epoch 4 Batch 10500 Loss 1.8059215545654297 in 15.248180\n",
      "Epoch 4 Batch 11000 Loss 1.8085914850234985 in 15.207407\n",
      "saving model with loss 1.8085914850234985\n",
      "Epoch 4 Batch 11500 Loss 1.7807796001434326 in 15.258035\n",
      "Epoch 4 Batch 12000 Loss 1.973390817642212 in 15.181243\n",
      "saving model with loss 1.973390817642212\n",
      "Epoch 4 Batch 12500 Loss 1.9792709350585938 in 15.264763\n",
      "Epoch 4 Batch 13000 Loss 1.8145948648452759 in 15.175147\n",
      "saving model with loss 1.8145948648452759\n",
      "Epoch 4 Batch 13500 Loss 1.8620872497558594 in 15.247372\n",
      "Epoch 4 Batch 14000 Loss 1.806946039199829 in 15.176314\n",
      "saving model with loss 1.806946039199829\n",
      "Epoch 4 Batch 14500 Loss 1.929419755935669 in 15.268373\n",
      "Epoch 4 Batch 15000 Loss 1.6151841878890991 in 15.180429\n",
      "saving model with loss 1.6151841878890991\n",
      "Epoch 4 Batch 15500 Loss 1.8655874729156494 in 15.227229\n",
      "Epoch 4 Batch 16000 Loss 1.7117643356323242 in 15.177510\n",
      "saving model with loss 1.7117643356323242\n",
      "Epoch 4 Batch 16500 Loss 1.691237211227417 in 15.260508\n",
      "Epoch 4 Batch 17000 Loss 1.7981445789337158 in 15.186529\n",
      "saving model with loss 1.7981445789337158\n",
      "Epoch 4 Batch 17500 Loss 1.9040796756744385 in 15.247901\n",
      "Epoch 4 Batch 18000 Loss 1.6672576665878296 in 15.180950\n",
      "saving model with loss 1.6672576665878296\n",
      "Epoch 4 Batch 18500 Loss 1.952781081199646 in 15.249372\n",
      "Epoch 4 Batch 19000 Loss 1.8969736099243164 in 15.179071\n",
      "saving model with loss 1.8969736099243164\n",
      "Epoch 4 Batch 19500 Loss 1.904235601425171 in 15.265693\n",
      "Epoch 4 Batch 20000 Loss 1.88531494140625 in 15.151775\n",
      "saving model with loss 1.88531494140625\n",
      "Epoch 4 Batch 20500 Loss 1.764791488647461 in 15.254036\n",
      "Epoch 4 Batch 21000 Loss 1.7574840784072876 in 15.177813\n",
      "saving model with loss 1.7574840784072876\n",
      "Epoch 4 Batch 21500 Loss 1.8825985193252563 in 15.236229\n",
      "Epoch 4 Batch 22000 Loss 1.7135741710662842 in 15.181005\n",
      "saving model with loss 1.7135741710662842\n",
      "Epoch 4 Batch 22500 Loss 1.9577544927597046 in 15.253872\n",
      "Epoch 4 Batch 23000 Loss 1.8307266235351562 in 15.189080\n",
      "saving model with loss 1.8307266235351562\n",
      "Epoch 4 Batch 23500 Loss 1.7180449962615967 in 15.227007\n",
      "Epoch 4 Batch 24000 Loss 1.8660471439361572 in 15.155437\n",
      "saving model with loss 1.8660471439361572\n",
      "Epoch 4 Batch 24500 Loss 1.7191112041473389 in 15.290078\n",
      "Epoch 4 Batch 25000 Loss 1.718550682067871 in 15.174771\n",
      "saving model with loss 1.718550682067871\n",
      "Epoch 4 Batch 25500 Loss 1.7967560291290283 in 15.256038\n",
      "Epoch 4 Batch 26000 Loss 1.8925507068634033 in 15.194629\n",
      "saving model with loss 1.8925507068634033\n",
      "Epoch 4 Batch 26500 Loss 1.8345668315887451 in 15.235933\n",
      "Epoch 4 Batch 27000 Loss 1.7281687259674072 in 15.197631\n",
      "saving model with loss 1.7281687259674072\n",
      "Epoch 4 Batch 27500 Loss 1.7546093463897705 in 15.251388\n",
      "Epoch 4 Batch 28000 Loss 1.7708408832550049 in 15.155679\n",
      "saving model with loss 1.7708408832550049\n",
      "Epoch 4 Batch 28500 Loss 1.7788591384887695 in 15.250659\n",
      "Epoch 4 Batch 29000 Loss 1.9356310367584229 in 15.164239\n",
      "saving model with loss 1.9356310367584229\n",
      "Epoch 4 Batch 29500 Loss 1.7442474365234375 in 15.248666\n",
      "Epoch 4 Batch 30000 Loss 1.8069689273834229 in 15.176645\n",
      "saving model with loss 1.8069689273834229\n",
      "Epoch 4 Batch 30500 Loss 1.8597322702407837 in 15.256920\n",
      "Epoch 4 Batch 31000 Loss 1.797275185585022 in 15.181271\n",
      "saving model with loss 1.797275185585022\n",
      "Epoch 4 Batch 31500 Loss 1.916738510131836 in 15.254247\n",
      "Epoch 4 Batch 32000 Loss 2.0479941368103027 in 15.156914\n",
      "saving model with loss 2.0479941368103027\n",
      "Epoch 4 Batch 32500 Loss 1.9451595544815063 in 15.243133\n",
      "Epoch 4 Batch 33000 Loss 1.8059899806976318 in 15.157927\n",
      "saving model with loss 1.8059899806976318\n",
      "Epoch 4 Batch 33500 Loss 1.8792922496795654 in 15.237349\n",
      "Epoch 4 Batch 34000 Loss 1.7447665929794312 in 15.152577\n",
      "saving model with loss 1.7447665929794312\n",
      "Epoch 4 Batch 34500 Loss 1.7640615701675415 in 15.242793\n",
      "Epoch 4 Batch 35000 Loss 1.9621143341064453 in 15.158629\n",
      "saving model with loss 1.9621143341064453\n",
      "Epoch 4 Batch 35500 Loss 1.8138525485992432 in 15.293341\n",
      "Epoch 4 Batch 36000 Loss 1.9149320125579834 in 15.179038\n",
      "saving model with loss 1.9149320125579834\n",
      "Epoch 4 Batch 36500 Loss 1.656691551208496 in 15.267092\n",
      "Epoch 4 Batch 37000 Loss 1.8049068450927734 in 15.162564\n",
      "saving model with loss 1.8049068450927734\n",
      "Epoch 4 Batch 37500 Loss 1.774239182472229 in 15.246026\n",
      "Epoch 4 Batch 38000 Loss 1.845887541770935 in 15.173069\n",
      "saving model with loss 1.845887541770935\n",
      "Epoch 4 Batch 38500 Loss 1.7958323955535889 in 15.245185\n",
      "Epoch 4 Batch 39000 Loss 1.851654291152954 in 15.180281\n",
      "saving model with loss 1.851654291152954\n",
      "Epoch 4 Batch 39500 Loss 1.8673298358917236 in 15.243072\n",
      "Epoch 4 Batch 40000 Loss 1.8407208919525146 in 15.192281\n",
      "saving model with loss 1.8407208919525146\n",
      "Epoch 4 Batch 40500 Loss 1.8234643936157227 in 15.243708\n",
      "Epoch 4 Batch 41000 Loss 1.8151832818984985 in 15.175309\n",
      "saving model with loss 1.8151832818984985\n",
      "Epoch 4 Batch 41500 Loss 1.8915523290634155 in 15.264830\n",
      "Epoch 4 Batch 42000 Loss 1.816666603088379 in 15.156744\n",
      "saving model with loss 1.816666603088379\n",
      "Epoch 4 Batch 42500 Loss 1.8176578283309937 in 15.240621\n",
      "Epoch 4 Batch 43000 Loss 1.7431011199951172 in 15.159056\n",
      "saving model with loss 1.7431011199951172\n",
      "Epoch 4 Batch 43500 Loss 1.7749664783477783 in 15.256496\n",
      "Epoch 4 Batch 44000 Loss 1.7916854619979858 in 15.156910\n",
      "saving model with loss 1.7916854619979858\n",
      "Epoch 4 Batch 44500 Loss 1.7952442169189453 in 15.275163\n",
      "Epoch 4 Batch 45000 Loss 1.8513052463531494 in 15.156597\n",
      "saving model with loss 1.8513052463531494\n",
      "Epoch 4 Batch 45500 Loss 1.7916085720062256 in 15.262314\n",
      "Epoch 4 Batch 46000 Loss 1.8748767375946045 in 15.159447\n",
      "saving model with loss 1.8748767375946045\n",
      "Epoch 4 Batch 46500 Loss 1.8495376110076904 in 15.275125\n",
      "Epoch 4 Batch 47000 Loss 1.993726372718811 in 15.206083\n",
      "saving model with loss 1.993726372718811\n",
      "Epoch 4 Batch 47500 Loss 1.948664665222168 in 15.271918\n",
      "Epoch 4 Batch 48000 Loss 1.8170315027236938 in 15.210804\n",
      "saving model with loss 1.8170315027236938\n",
      "Epoch 4 Batch 48500 Loss 1.9394680261611938 in 15.276136\n",
      "Epoch 4 Batch 49000 Loss 1.772351622581482 in 15.215313\n",
      "saving model with loss 1.772351622581482\n",
      "Epoch 4 Batch 49500 Loss 1.7180674076080322 in 15.314425\n",
      "Epoch 4 Batch 50000 Loss 1.811751127243042 in 15.263572\n",
      "saving model with loss 1.811751127243042\n",
      "Epoch 4 Batch 50500 Loss 1.8691591024398804 in 15.517246\n",
      "Epoch 4 Batch 51000 Loss 1.842230200767517 in 15.271583\n",
      "saving model with loss 1.842230200767517\n",
      "Epoch 4 Batch 51500 Loss 1.886986494064331 in 15.363405\n",
      "Epoch 4 Batch 52000 Loss 1.8290046453475952 in 15.264571\n",
      "saving model with loss 1.8290046453475952\n",
      "Epoch 4 Batch 52500 Loss 1.7668771743774414 in 15.300186\n",
      "Epoch 4 Batch 53000 Loss 1.8052889108657837 in 15.217411\n",
      "saving model with loss 1.8052889108657837\n",
      "Epoch 4 Batch 53500 Loss 1.833112359046936 in 15.298125\n",
      "Epoch 4 Batch 54000 Loss 1.8713668584823608 in 15.207168\n",
      "saving model with loss 1.8713668584823608\n",
      "Epoch 4 Batch 54500 Loss 1.7415869235992432 in 15.278916\n",
      "Epoch 4 Batch 55000 Loss 1.7893823385238647 in 15.204213\n",
      "saving model with loss 1.7893823385238647\n",
      "Epoch 4 Batch 55500 Loss 1.8667023181915283 in 15.219903\n",
      "Epoch 4 Batch 56000 Loss 1.7595630884170532 in 15.180901\n",
      "saving model with loss 1.7595630884170532\n",
      "Epoch 4 Batch 56500 Loss 1.880758285522461 in 15.235262\n",
      "Epoch 4 Batch 57000 Loss 1.829397201538086 in 15.158255\n",
      "saving model with loss 1.829397201538086\n",
      "Epoch 4 Batch 57500 Loss 1.8297027349472046 in 15.213082\n",
      "Epoch 4 Batch 58000 Loss 1.914807677268982 in 15.146932\n",
      "saving model with loss 1.914807677268982\n",
      "Epoch 4 Batch 58500 Loss 1.9356244802474976 in 15.242559\n",
      "Epoch 4 Batch 59000 Loss 1.8024190664291382 in 15.150417\n",
      "saving model with loss 1.8024190664291382\n",
      "Epoch 4 Batch 59500 Loss 1.8458627462387085 in 15.226263\n",
      "Epoch 4 Batch 60000 Loss 1.8795467615127563 in 15.162547\n",
      "saving model with loss 1.8795467615127563\n",
      "Epoch 4 Batch 60500 Loss 1.7896562814712524 in 15.231837\n",
      "Epoch 4 Batch 61000 Loss 1.6964629888534546 in 15.171827\n",
      "saving model with loss 1.6964629888534546\n",
      "Epoch 4 Batch 61500 Loss 1.8822643756866455 in 15.239536\n",
      "Epoch 4 Batch 62000 Loss 1.8078291416168213 in 15.156768\n",
      "saving model with loss 1.8078291416168213\n",
      "Epoch 4 Batch 62500 Loss 1.8724054098129272 in 15.238369\n",
      "Epoch 4 Batch 63000 Loss 1.8263723850250244 in 15.153124\n",
      "saving model with loss 1.8263723850250244\n",
      "Epoch 4 Batch 63500 Loss 1.740691900253296 in 15.266109\n",
      "Epoch 4 Batch 64000 Loss 1.7894887924194336 in 15.151592\n",
      "saving model with loss 1.7894887924194336\n",
      "Epoch 4 Batch 64500 Loss 1.949904441833496 in 15.592760\n",
      "Epoch 4 Batch 65000 Loss 1.6513181924819946 in 15.177435\n",
      "saving model with loss 1.6513181924819946\n",
      "Epoch 4 Batch 65500 Loss 1.7796790599822998 in 15.231277\n",
      "Epoch 4 Batch 66000 Loss 1.7032077312469482 in 15.158307\n",
      "saving model with loss 1.7032077312469482\n",
      "Epoch 4 Batch 66500 Loss 1.741876244544983 in 15.248818\n",
      "Epoch 4 Batch 67000 Loss 1.7622921466827393 in 15.171075\n",
      "saving model with loss 1.7622921466827393\n",
      "Epoch 4 Batch 67500 Loss 1.7588145732879639 in 15.243196\n",
      "Epoch 4 Batch 68000 Loss 1.703555703163147 in 15.156538\n",
      "saving model with loss 1.703555703163147\n",
      "Epoch 4 Batch 68500 Loss 1.8548851013183594 in 15.241916\n",
      "Epoch 4 Batch 69000 Loss 1.8926289081573486 in 15.175500\n",
      "saving model with loss 1.8926289081573486\n",
      "Epoch 4 Batch 69500 Loss 1.7784525156021118 in 15.213311\n",
      "Epoch 4 Batch 70000 Loss 1.7499282360076904 in 15.149621\n",
      "saving model with loss 1.7499282360076904\n",
      "Epoch 4 Batch 70500 Loss 1.8581371307373047 in 15.244969\n",
      "Epoch 5 Loss 1.8067079782485962\n",
      "Time taken for 1 epoch 2148.8085005283356\n",
      "number of batches : 70553\n",
      "Epoch 5 Batch 0 Loss 1.6227500438690186 in 1.162918\n",
      "saving model with loss 1.6227500438690186\n",
      "Epoch 5 Batch 500 Loss 1.8446853160858154 in 15.220194\n",
      "Epoch 5 Batch 1000 Loss 1.9738695621490479 in 15.149490\n",
      "saving model with loss 1.9738695621490479\n",
      "Epoch 5 Batch 1500 Loss 1.7786601781845093 in 15.240273\n",
      "Epoch 5 Batch 2000 Loss 1.820765495300293 in 15.155548\n",
      "saving model with loss 1.820765495300293\n",
      "Epoch 5 Batch 2500 Loss 1.7304985523223877 in 15.237243\n",
      "Epoch 5 Batch 3000 Loss 1.8309195041656494 in 15.158014\n",
      "saving model with loss 1.8309195041656494\n",
      "Epoch 5 Batch 3500 Loss 1.8242781162261963 in 15.245258\n",
      "Epoch 5 Batch 4000 Loss 1.7943159341812134 in 15.148057\n",
      "saving model with loss 1.7943159341812134\n",
      "Epoch 5 Batch 4500 Loss 1.8277475833892822 in 15.225493\n",
      "Epoch 5 Batch 5000 Loss 1.8090133666992188 in 15.150721\n",
      "saving model with loss 1.8090133666992188\n",
      "Epoch 5 Batch 5500 Loss 1.8596627712249756 in 15.216107\n",
      "Epoch 5 Batch 6000 Loss 1.864330530166626 in 15.150458\n",
      "saving model with loss 1.864330530166626\n",
      "Epoch 5 Batch 6500 Loss 1.7458693981170654 in 15.240561\n",
      "Epoch 5 Batch 7000 Loss 1.8209154605865479 in 15.168685\n",
      "saving model with loss 1.8209154605865479\n",
      "Epoch 5 Batch 7500 Loss 1.720902442932129 in 15.223716\n",
      "Epoch 5 Batch 8000 Loss 1.914025902748108 in 15.148894\n",
      "saving model with loss 1.914025902748108\n",
      "Epoch 5 Batch 8500 Loss 1.6862379312515259 in 15.216719\n",
      "Epoch 5 Batch 9000 Loss 1.6811058521270752 in 15.147393\n",
      "saving model with loss 1.6811058521270752\n",
      "Epoch 5 Batch 9500 Loss 1.7511355876922607 in 15.211924\n",
      "Epoch 5 Batch 10000 Loss 1.946504831314087 in 15.153295\n",
      "saving model with loss 1.946504831314087\n",
      "Epoch 5 Batch 10500 Loss 1.7535219192504883 in 15.213107\n",
      "Epoch 5 Batch 11000 Loss 1.8343842029571533 in 15.149457\n",
      "saving model with loss 1.8343842029571533\n",
      "Epoch 5 Batch 11500 Loss 1.9191519021987915 in 15.227266\n",
      "Epoch 5 Batch 12000 Loss 1.8345839977264404 in 15.167526\n",
      "saving model with loss 1.8345839977264404\n",
      "Epoch 5 Batch 12500 Loss 1.8943252563476562 in 15.243942\n",
      "Epoch 5 Batch 13000 Loss 1.8717825412750244 in 15.174649\n",
      "saving model with loss 1.8717825412750244\n",
      "Epoch 5 Batch 13500 Loss 1.8864974975585938 in 15.219133\n",
      "Epoch 5 Batch 14000 Loss 1.9048824310302734 in 15.150777\n",
      "saving model with loss 1.9048824310302734\n",
      "Epoch 5 Batch 14500 Loss 1.7416448593139648 in 15.212504\n",
      "Epoch 5 Batch 15000 Loss 1.7923848628997803 in 15.151011\n",
      "saving model with loss 1.7923848628997803\n",
      "Epoch 5 Batch 15500 Loss 1.8930962085723877 in 15.212725\n",
      "Epoch 5 Batch 16000 Loss 1.8409054279327393 in 15.148088\n",
      "saving model with loss 1.8409054279327393\n",
      "Epoch 5 Batch 16500 Loss 1.649844765663147 in 15.215991\n",
      "Epoch 5 Batch 17000 Loss 1.9106372594833374 in 15.147963\n",
      "saving model with loss 1.9106372594833374\n",
      "Epoch 5 Batch 17500 Loss 1.884988784790039 in 15.243902\n",
      "Epoch 5 Batch 18000 Loss 1.759342908859253 in 15.150791\n",
      "saving model with loss 1.759342908859253\n",
      "Epoch 5 Batch 18500 Loss 1.8576877117156982 in 15.243873\n",
      "Epoch 5 Batch 19000 Loss 1.8843886852264404 in 15.153031\n",
      "saving model with loss 1.8843886852264404\n",
      "Epoch 5 Batch 19500 Loss 1.7604297399520874 in 15.237300\n",
      "Epoch 5 Batch 20000 Loss 1.8739912509918213 in 15.151068\n",
      "saving model with loss 1.8739912509918213\n",
      "Epoch 5 Batch 20500 Loss 1.8665282726287842 in 15.216253\n",
      "Epoch 5 Batch 21000 Loss 1.7580108642578125 in 15.146999\n",
      "saving model with loss 1.7580108642578125\n",
      "Epoch 5 Batch 21500 Loss 1.7411324977874756 in 15.217160\n",
      "Epoch 5 Batch 22000 Loss 1.802046537399292 in 15.150504\n",
      "saving model with loss 1.802046537399292\n",
      "Epoch 5 Batch 22500 Loss 1.784307837486267 in 15.223120\n",
      "Epoch 5 Batch 23000 Loss 1.7547805309295654 in 15.139778\n",
      "saving model with loss 1.7547805309295654\n",
      "Epoch 5 Batch 23500 Loss 1.851662039756775 in 15.215712\n",
      "Epoch 5 Batch 24000 Loss 1.7164405584335327 in 15.199939\n",
      "saving model with loss 1.7164405584335327\n",
      "Epoch 5 Batch 24500 Loss 1.6978294849395752 in 15.236073\n",
      "Epoch 5 Batch 25000 Loss 1.8249315023422241 in 15.139382\n",
      "saving model with loss 1.8249315023422241\n",
      "Epoch 5 Batch 25500 Loss 1.8158128261566162 in 15.221696\n",
      "Epoch 5 Batch 26000 Loss 1.8678827285766602 in 15.150136\n",
      "saving model with loss 1.8678827285766602\n",
      "Epoch 5 Batch 26500 Loss 1.91982901096344 in 15.214350\n",
      "Epoch 5 Batch 27000 Loss 1.8067023754119873 in 15.119872\n",
      "saving model with loss 1.8067023754119873\n",
      "Epoch 5 Batch 27500 Loss 1.8101593255996704 in 15.213754\n",
      "Epoch 5 Batch 28000 Loss 1.9491221904754639 in 15.150104\n",
      "saving model with loss 1.9491221904754639\n",
      "Epoch 5 Batch 28500 Loss 1.8846886157989502 in 15.206565\n",
      "Epoch 5 Batch 29000 Loss 1.7889776229858398 in 15.157370\n",
      "saving model with loss 1.7889776229858398\n",
      "Epoch 5 Batch 29500 Loss 1.7317692041397095 in 15.213100\n",
      "Epoch 5 Batch 30000 Loss 1.7783708572387695 in 15.148622\n",
      "saving model with loss 1.7783708572387695\n",
      "Epoch 5 Batch 30500 Loss 1.89887273311615 in 15.215366\n",
      "Epoch 5 Batch 31000 Loss 1.7765443325042725 in 15.142890\n",
      "saving model with loss 1.7765443325042725\n",
      "Epoch 5 Batch 31500 Loss 1.7597862482070923 in 15.216369\n",
      "Epoch 5 Batch 32000 Loss 1.770016074180603 in 15.155514\n",
      "saving model with loss 1.770016074180603\n",
      "Epoch 5 Batch 32500 Loss 1.8992702960968018 in 15.207384\n",
      "Epoch 5 Batch 33000 Loss 1.7809383869171143 in 15.153646\n",
      "saving model with loss 1.7809383869171143\n",
      "Epoch 5 Batch 33500 Loss 1.8264029026031494 in 15.213597\n",
      "Epoch 5 Batch 34000 Loss 1.7390806674957275 in 15.149408\n",
      "saving model with loss 1.7390806674957275\n",
      "Epoch 5 Batch 34500 Loss 2.0151450634002686 in 15.212259\n",
      "Epoch 5 Batch 35000 Loss 1.8404213190078735 in 15.152942\n",
      "saving model with loss 1.8404213190078735\n",
      "Epoch 5 Batch 35500 Loss 1.8254308700561523 in 15.216688\n",
      "Epoch 5 Batch 36000 Loss 1.8560636043548584 in 15.147979\n",
      "saving model with loss 1.8560636043548584\n",
      "Epoch 5 Batch 36500 Loss 1.8721164464950562 in 15.211715\n",
      "Epoch 5 Batch 37000 Loss 1.7863190174102783 in 15.144660\n",
      "saving model with loss 1.7863190174102783\n",
      "Epoch 5 Batch 37500 Loss 1.9356956481933594 in 15.219184\n",
      "Epoch 5 Batch 38000 Loss 1.8659569025039673 in 15.147380\n",
      "saving model with loss 1.8659569025039673\n",
      "Epoch 5 Batch 38500 Loss 1.9485324621200562 in 15.218743\n",
      "Epoch 5 Batch 39000 Loss 1.7684980630874634 in 15.141451\n",
      "saving model with loss 1.7684980630874634\n",
      "Epoch 5 Batch 39500 Loss 1.7615325450897217 in 15.216396\n",
      "Epoch 5 Batch 40000 Loss 1.8063970804214478 in 15.150847\n",
      "saving model with loss 1.8063970804214478\n",
      "Epoch 5 Batch 40500 Loss 1.7158523797988892 in 15.216206\n",
      "Epoch 5 Batch 41000 Loss 1.8228023052215576 in 15.152771\n",
      "saving model with loss 1.8228023052215576\n",
      "Epoch 5 Batch 41500 Loss 1.7734472751617432 in 15.210331\n",
      "Epoch 5 Batch 42000 Loss 1.755942702293396 in 15.151455\n",
      "saving model with loss 1.755942702293396\n",
      "Epoch 5 Batch 42500 Loss 1.8561465740203857 in 15.209154\n",
      "Epoch 5 Batch 43000 Loss 1.7974300384521484 in 15.151611\n",
      "saving model with loss 1.7974300384521484\n",
      "Epoch 5 Batch 43500 Loss 1.9557749032974243 in 15.213567\n",
      "Epoch 5 Batch 44000 Loss 1.8475849628448486 in 15.152960\n",
      "saving model with loss 1.8475849628448486\n",
      "Epoch 5 Batch 44500 Loss 1.8344835042953491 in 15.209702\n",
      "Epoch 5 Batch 45000 Loss 1.7856483459472656 in 15.152473\n",
      "saving model with loss 1.7856483459472656\n",
      "Epoch 5 Batch 45500 Loss 1.8304328918457031 in 15.214309\n",
      "Epoch 5 Batch 46000 Loss 1.8000385761260986 in 15.150364\n",
      "saving model with loss 1.8000385761260986\n",
      "Epoch 5 Batch 46500 Loss 1.9092628955841064 in 15.208283\n",
      "Epoch 5 Batch 47000 Loss 1.8047288656234741 in 15.152451\n",
      "saving model with loss 1.8047288656234741\n",
      "Epoch 5 Batch 47500 Loss 1.7111027240753174 in 15.612017\n",
      "Epoch 5 Batch 48000 Loss 1.8366124629974365 in 15.149055\n",
      "saving model with loss 1.8366124629974365\n",
      "Epoch 5 Batch 48500 Loss 1.7103513479232788 in 15.240916\n",
      "Epoch 5 Batch 49000 Loss 1.9406235218048096 in 15.152747\n",
      "saving model with loss 1.9406235218048096\n",
      "Epoch 5 Batch 49500 Loss 1.8552567958831787 in 15.241556\n",
      "Epoch 5 Batch 50000 Loss 1.7829577922821045 in 15.150942\n",
      "saving model with loss 1.7829577922821045\n",
      "Epoch 5 Batch 50500 Loss 1.8095753192901611 in 15.213948\n",
      "Epoch 5 Batch 51000 Loss 1.913916826248169 in 15.149565\n",
      "saving model with loss 1.913916826248169\n",
      "Epoch 5 Batch 51500 Loss 1.8914827108383179 in 15.212654\n",
      "Epoch 5 Batch 52000 Loss 1.7984641790390015 in 15.151037\n",
      "saving model with loss 1.7984641790390015\n",
      "Epoch 5 Batch 52500 Loss 1.7993195056915283 in 15.240686\n",
      "Epoch 5 Batch 53000 Loss 1.8157899379730225 in 15.152167\n",
      "saving model with loss 1.8157899379730225\n",
      "Epoch 5 Batch 53500 Loss 1.6964991092681885 in 15.218159\n",
      "Epoch 5 Batch 54000 Loss 1.7033262252807617 in 15.148039\n",
      "saving model with loss 1.7033262252807617\n",
      "Epoch 5 Batch 54500 Loss 1.9143226146697998 in 15.240602\n",
      "Epoch 5 Batch 55000 Loss 1.6730741262435913 in 15.166723\n",
      "saving model with loss 1.6730741262435913\n",
      "Epoch 5 Batch 55500 Loss 1.9509941339492798 in 15.257133\n",
      "Epoch 5 Batch 56000 Loss 1.8106588125228882 in 15.153025\n",
      "saving model with loss 1.8106588125228882\n",
      "Epoch 5 Batch 56500 Loss 1.7620290517807007 in 15.236032\n",
      "Epoch 5 Batch 57000 Loss 1.9379770755767822 in 15.161016\n",
      "saving model with loss 1.9379770755767822\n",
      "Epoch 5 Batch 57500 Loss 1.865652084350586 in 15.246905\n",
      "Epoch 5 Batch 58000 Loss 1.9052833318710327 in 15.143866\n",
      "saving model with loss 1.9052833318710327\n",
      "Epoch 5 Batch 58500 Loss 1.797811508178711 in 15.216627\n",
      "Epoch 5 Batch 59000 Loss 1.8458614349365234 in 15.147960\n",
      "saving model with loss 1.8458614349365234\n",
      "Epoch 5 Batch 59500 Loss 1.8658363819122314 in 15.230742\n",
      "Epoch 5 Batch 60000 Loss 1.8555704355239868 in 15.180159\n",
      "saving model with loss 1.8555704355239868\n",
      "Epoch 5 Batch 60500 Loss 1.9203535318374634 in 15.228053\n",
      "Epoch 5 Batch 61000 Loss 1.714707612991333 in 15.154316\n",
      "saving model with loss 1.714707612991333\n",
      "Epoch 5 Batch 61500 Loss 1.894639253616333 in 15.239831\n",
      "Epoch 5 Batch 62000 Loss 1.8525283336639404 in 15.177983\n",
      "saving model with loss 1.8525283336639404\n",
      "Epoch 5 Batch 62500 Loss 1.7712013721466064 in 15.243844\n",
      "Epoch 5 Batch 63000 Loss 1.931417465209961 in 15.151070\n",
      "saving model with loss 1.931417465209961\n",
      "Epoch 5 Batch 63500 Loss 1.861488699913025 in 15.223781\n",
      "Epoch 5 Batch 64000 Loss 1.9984753131866455 in 15.172953\n",
      "saving model with loss 1.9984753131866455\n",
      "Epoch 5 Batch 64500 Loss 1.9413589239120483 in 15.212201\n",
      "Epoch 5 Batch 65000 Loss 1.8005247116088867 in 15.144841\n",
      "saving model with loss 1.8005247116088867\n",
      "Epoch 5 Batch 65500 Loss 1.868685007095337 in 15.214344\n",
      "Epoch 5 Batch 66000 Loss 1.7051231861114502 in 15.153565\n",
      "saving model with loss 1.7051231861114502\n",
      "Epoch 5 Batch 66500 Loss 1.878767967224121 in 15.208527\n",
      "Epoch 5 Batch 67000 Loss 1.7011661529541016 in 15.180192\n",
      "saving model with loss 1.7011661529541016\n",
      "Epoch 5 Batch 67500 Loss 1.8551381826400757 in 15.214147\n",
      "Epoch 5 Batch 68000 Loss 1.753448486328125 in 15.150295\n",
      "saving model with loss 1.753448486328125\n",
      "Epoch 5 Batch 68500 Loss 1.9511274099349976 in 15.213063\n",
      "Epoch 5 Batch 69000 Loss 2.0247485637664795 in 15.152998\n",
      "saving model with loss 2.0247485637664795\n",
      "Epoch 5 Batch 69500 Loss 1.8994184732437134 in 15.237347\n",
      "Epoch 5 Batch 70000 Loss 1.9051411151885986 in 15.152895\n",
      "saving model with loss 1.9051411151885986\n",
      "Epoch 5 Batch 70500 Loss 1.8368686437606812 in 15.209637\n",
      "Epoch 6 Loss 1.9288442134857178\n",
      "Time taken for 1 epoch 2144.7278850078583\n",
      "number of batches : 70553\n",
      "Epoch 6 Batch 0 Loss 1.7687164545059204 in 1.174444\n",
      "saving model with loss 1.7687164545059204\n",
      "Epoch 6 Batch 500 Loss 1.7066795825958252 in 15.205184\n",
      "Epoch 6 Batch 1000 Loss 1.7062734365463257 in 15.151264\n",
      "saving model with loss 1.7062734365463257\n",
      "Epoch 6 Batch 1500 Loss 1.8646366596221924 in 15.210980\n",
      "Epoch 6 Batch 2000 Loss 1.875365972518921 in 15.152830\n",
      "saving model with loss 1.875365972518921\n",
      "Epoch 6 Batch 2500 Loss 1.8682079315185547 in 15.210240\n",
      "Epoch 6 Batch 3000 Loss 1.5688247680664062 in 15.153478\n",
      "saving model with loss 1.5688247680664062\n",
      "Epoch 6 Batch 3500 Loss 1.8532402515411377 in 15.210007\n",
      "Epoch 6 Batch 4000 Loss 1.7720413208007812 in 15.152827\n",
      "saving model with loss 1.7720413208007812\n",
      "Epoch 6 Batch 4500 Loss 1.7572135925292969 in 15.214264\n",
      "Epoch 6 Batch 5000 Loss 1.9014774560928345 in 15.147823\n",
      "saving model with loss 1.9014774560928345\n",
      "Epoch 6 Batch 5500 Loss 1.8549476861953735 in 15.211292\n",
      "Epoch 6 Batch 6000 Loss 1.7755630016326904 in 15.141049\n",
      "saving model with loss 1.7755630016326904\n",
      "Epoch 6 Batch 6500 Loss 1.8471025228500366 in 15.226719\n",
      "Epoch 6 Batch 7000 Loss 1.8427212238311768 in 15.151479\n",
      "saving model with loss 1.8427212238311768\n",
      "Epoch 6 Batch 7500 Loss 1.8318513631820679 in 15.213214\n",
      "Epoch 6 Batch 8000 Loss 1.7016092538833618 in 15.146352\n",
      "saving model with loss 1.7016092538833618\n",
      "Epoch 6 Batch 8500 Loss 1.6403648853302002 in 15.227098\n",
      "Epoch 6 Batch 9000 Loss 1.9350173473358154 in 15.151929\n",
      "saving model with loss 1.9350173473358154\n",
      "Epoch 6 Batch 9500 Loss 1.7740598917007446 in 15.212182\n",
      "Epoch 6 Batch 10000 Loss 1.9752423763275146 in 15.157433\n",
      "saving model with loss 1.9752423763275146\n",
      "Epoch 6 Batch 10500 Loss 1.8318521976470947 in 15.209419\n",
      "Epoch 6 Batch 11000 Loss 1.7880502939224243 in 15.149817\n",
      "saving model with loss 1.7880502939224243\n",
      "Epoch 6 Batch 11500 Loss 1.873094916343689 in 15.211677\n",
      "Epoch 6 Batch 12000 Loss 1.8426487445831299 in 15.546018\n",
      "saving model with loss 1.8426487445831299\n",
      "Epoch 6 Batch 12500 Loss 1.6460494995117188 in 15.251370\n",
      "Epoch 6 Batch 13000 Loss 1.9199857711791992 in 15.180406\n",
      "saving model with loss 1.9199857711791992\n",
      "Epoch 6 Batch 13500 Loss 1.7055801153182983 in 15.242614\n",
      "Epoch 6 Batch 14000 Loss 1.747889757156372 in 15.179473\n",
      "saving model with loss 1.747889757156372\n",
      "Epoch 6 Batch 14500 Loss 1.6418449878692627 in 15.256728\n",
      "Epoch 6 Batch 15000 Loss 1.8323643207550049 in 15.177941\n",
      "saving model with loss 1.8323643207550049\n",
      "Epoch 6 Batch 15500 Loss 1.7497373819351196 in 15.232844\n",
      "Epoch 6 Batch 16000 Loss 1.8615539073944092 in 15.163291\n",
      "saving model with loss 1.8615539073944092\n",
      "Epoch 6 Batch 16500 Loss 1.8590914011001587 in 15.261063\n",
      "Epoch 6 Batch 17000 Loss 1.8096832036972046 in 15.157807\n",
      "saving model with loss 1.8096832036972046\n",
      "Epoch 6 Batch 17500 Loss 1.8336271047592163 in 15.231941\n",
      "Epoch 6 Batch 18000 Loss 1.824901819229126 in 15.153429\n",
      "saving model with loss 1.824901819229126\n",
      "Epoch 6 Batch 18500 Loss 1.7308365106582642 in 15.252517\n",
      "Epoch 6 Batch 19000 Loss 1.7309048175811768 in 15.184443\n",
      "saving model with loss 1.7309048175811768\n",
      "Epoch 6 Batch 19500 Loss 1.7482293844223022 in 15.232140\n",
      "Epoch 6 Batch 20000 Loss 1.8886101245880127 in 15.146541\n",
      "saving model with loss 1.8886101245880127\n",
      "Epoch 6 Batch 20500 Loss 1.823878288269043 in 15.248758\n",
      "Epoch 6 Batch 21000 Loss 1.9057559967041016 in 15.176491\n",
      "saving model with loss 1.9057559967041016\n",
      "Epoch 6 Batch 21500 Loss 1.7780802249908447 in 15.253208\n",
      "Epoch 6 Batch 22000 Loss 1.8281099796295166 in 15.184544\n",
      "saving model with loss 1.8281099796295166\n",
      "Epoch 6 Batch 22500 Loss 1.8103110790252686 in 15.249923\n",
      "Epoch 6 Batch 23000 Loss 1.8223215341567993 in 15.183883\n",
      "saving model with loss 1.8223215341567993\n",
      "Epoch 6 Batch 23500 Loss 1.8656835556030273 in 15.245152\n",
      "Epoch 6 Batch 24000 Loss 1.7463502883911133 in 15.170049\n",
      "saving model with loss 1.7463502883911133\n",
      "Epoch 6 Batch 24500 Loss 1.8035567998886108 in 15.245383\n",
      "Epoch 6 Batch 25000 Loss 1.7806484699249268 in 15.176581\n",
      "saving model with loss 1.7806484699249268\n",
      "Epoch 6 Batch 25500 Loss 1.9459154605865479 in 15.250386\n",
      "Epoch 6 Batch 26000 Loss 1.866571068763733 in 15.175790\n",
      "saving model with loss 1.866571068763733\n",
      "Epoch 6 Batch 26500 Loss 1.7696735858917236 in 15.241966\n",
      "Epoch 6 Batch 27000 Loss 1.8740774393081665 in 15.187592\n",
      "saving model with loss 1.8740774393081665\n",
      "Epoch 6 Batch 27500 Loss 1.8460973501205444 in 15.237448\n",
      "Epoch 6 Batch 28000 Loss 1.817030906677246 in 15.149606\n",
      "saving model with loss 1.817030906677246\n",
      "Epoch 6 Batch 28500 Loss 1.95058274269104 in 15.251465\n",
      "Epoch 6 Batch 29000 Loss 1.8555608987808228 in 15.172389\n",
      "saving model with loss 1.8555608987808228\n",
      "Epoch 6 Batch 29500 Loss 1.9088237285614014 in 15.251258\n",
      "Epoch 6 Batch 30000 Loss 1.8378612995147705 in 15.176819\n",
      "saving model with loss 1.8378612995147705\n",
      "Epoch 6 Batch 30500 Loss 2.1134488582611084 in 15.241896\n",
      "Epoch 6 Batch 31000 Loss 1.8096386194229126 in 15.182483\n",
      "saving model with loss 1.8096386194229126\n",
      "Epoch 6 Batch 31500 Loss 1.8061158657073975 in 15.395956\n",
      "Epoch 6 Batch 32000 Loss 1.959946632385254 in 15.266357\n",
      "saving model with loss 1.959946632385254\n",
      "Epoch 6 Batch 32500 Loss 1.8384307622909546 in 15.215280\n",
      "Epoch 6 Batch 33000 Loss 1.8453105688095093 in 15.152776\n",
      "saving model with loss 1.8453105688095093\n",
      "Epoch 6 Batch 33500 Loss 1.8258002996444702 in 15.262152\n",
      "Epoch 6 Batch 34000 Loss 1.8396625518798828 in 15.157853\n",
      "saving model with loss 1.8396625518798828\n",
      "Epoch 6 Batch 34500 Loss 1.918977975845337 in 15.245725\n",
      "Epoch 6 Batch 35000 Loss 1.888837456703186 in 15.149082\n",
      "saving model with loss 1.888837456703186\n",
      "Epoch 6 Batch 35500 Loss 1.8426223993301392 in 15.234711\n",
      "Epoch 6 Batch 36000 Loss 1.9037309885025024 in 15.175609\n",
      "saving model with loss 1.9037309885025024\n",
      "Epoch 6 Batch 36500 Loss 1.9983924627304077 in 15.233665\n",
      "Epoch 6 Batch 37000 Loss 1.9263406991958618 in 15.172936\n",
      "saving model with loss 1.9263406991958618\n",
      "Epoch 6 Batch 37500 Loss 1.8169931173324585 in 15.247143\n",
      "Epoch 6 Batch 38000 Loss 1.890824556350708 in 15.176646\n",
      "saving model with loss 1.890824556350708\n",
      "Epoch 6 Batch 38500 Loss 1.7459170818328857 in 15.227542\n",
      "Epoch 6 Batch 39000 Loss 1.8191848993301392 in 15.167606\n",
      "saving model with loss 1.8191848993301392\n",
      "Epoch 6 Batch 39500 Loss 1.8246815204620361 in 15.243185\n",
      "Epoch 6 Batch 40000 Loss 1.8862364292144775 in 15.181450\n",
      "saving model with loss 1.8862364292144775\n",
      "Epoch 6 Batch 40500 Loss 1.751584768295288 in 15.239996\n",
      "Epoch 6 Batch 41000 Loss 1.8684651851654053 in 15.181867\n",
      "saving model with loss 1.8684651851654053\n",
      "Epoch 6 Batch 41500 Loss 1.595205545425415 in 15.245586\n",
      "Epoch 6 Batch 42000 Loss 1.8498882055282593 in 15.180597\n",
      "saving model with loss 1.8498882055282593\n",
      "Epoch 6 Batch 42500 Loss 1.7313302755355835 in 15.263703\n",
      "Epoch 6 Batch 43000 Loss 1.6823161840438843 in 15.161590\n",
      "saving model with loss 1.6823161840438843\n",
      "Epoch 6 Batch 43500 Loss 1.7776310443878174 in 15.243463\n",
      "Epoch 6 Batch 44000 Loss 1.9801788330078125 in 15.181296\n",
      "saving model with loss 1.9801788330078125\n",
      "Epoch 6 Batch 44500 Loss 1.964458703994751 in 15.239807\n",
      "Epoch 6 Batch 45000 Loss 1.8185373544692993 in 15.152820\n",
      "saving model with loss 1.8185373544692993\n",
      "Epoch 6 Batch 45500 Loss 1.8368507623672485 in 15.247142\n",
      "Epoch 6 Batch 46000 Loss 1.802896499633789 in 15.174615\n",
      "saving model with loss 1.802896499633789\n",
      "Epoch 6 Batch 46500 Loss 1.8793830871582031 in 15.251812\n",
      "Epoch 6 Batch 47000 Loss 1.8331356048583984 in 15.180848\n",
      "saving model with loss 1.8331356048583984\n",
      "Epoch 6 Batch 47500 Loss 1.7042577266693115 in 15.237767\n",
      "Epoch 6 Batch 48000 Loss 1.9330060482025146 in 15.146275\n",
      "saving model with loss 1.9330060482025146\n",
      "Epoch 6 Batch 48500 Loss 1.8016917705535889 in 15.215454\n",
      "Epoch 6 Batch 49000 Loss 1.823926568031311 in 15.152870\n",
      "saving model with loss 1.823926568031311\n",
      "Epoch 6 Batch 49500 Loss 1.8016910552978516 in 15.242756\n",
      "Epoch 6 Batch 50000 Loss 1.955866813659668 in 15.151575\n",
      "saving model with loss 1.955866813659668\n",
      "Epoch 6 Batch 50500 Loss 1.9156379699707031 in 15.240659\n",
      "Epoch 6 Batch 51000 Loss 1.845070242881775 in 15.176697\n",
      "saving model with loss 1.845070242881775\n",
      "Epoch 6 Batch 51500 Loss 1.897884726524353 in 15.249296\n",
      "Epoch 6 Batch 52000 Loss 1.8265869617462158 in 15.159542\n",
      "saving model with loss 1.8265869617462158\n",
      "Epoch 6 Batch 52500 Loss 1.8767859935760498 in 15.239676\n",
      "Epoch 6 Batch 53000 Loss 1.8521575927734375 in 15.171865\n",
      "saving model with loss 1.8521575927734375\n",
      "Epoch 6 Batch 53500 Loss 1.756415605545044 in 15.244172\n",
      "Epoch 6 Batch 54000 Loss 1.733703851699829 in 15.161062\n",
      "saving model with loss 1.733703851699829\n",
      "Epoch 6 Batch 54500 Loss 1.8938862085342407 in 15.234478\n",
      "Epoch 6 Batch 55000 Loss 1.6793267726898193 in 15.155916\n",
      "saving model with loss 1.6793267726898193\n",
      "Epoch 6 Batch 55500 Loss 1.9137579202651978 in 15.239713\n",
      "Epoch 6 Batch 56000 Loss 1.7904125452041626 in 15.171877\n",
      "saving model with loss 1.7904125452041626\n",
      "Epoch 6 Batch 56500 Loss 1.8808132410049438 in 15.217523\n",
      "Epoch 6 Batch 57000 Loss 1.8323822021484375 in 15.163937\n",
      "saving model with loss 1.8323822021484375\n",
      "Epoch 6 Batch 57500 Loss 1.8195686340332031 in 15.260162\n",
      "Epoch 6 Batch 58000 Loss 1.8600772619247437 in 15.179225\n",
      "saving model with loss 1.8600772619247437\n",
      "Epoch 6 Batch 58500 Loss 1.7524330615997314 in 15.215693\n",
      "Epoch 6 Batch 59000 Loss 2.0508060455322266 in 15.150450\n",
      "saving model with loss 2.0508060455322266\n",
      "Epoch 6 Batch 59500 Loss 1.8427953720092773 in 15.244231\n",
      "Epoch 6 Batch 60000 Loss 1.7596967220306396 in 15.150585\n",
      "saving model with loss 1.7596967220306396\n",
      "Epoch 6 Batch 60500 Loss 1.874670386314392 in 15.243766\n",
      "Epoch 6 Batch 61000 Loss 1.8328583240509033 in 15.215914\n",
      "saving model with loss 1.8328583240509033\n",
      "Epoch 6 Batch 61500 Loss 1.8430328369140625 in 15.240835\n",
      "Epoch 6 Batch 62000 Loss 1.94942307472229 in 15.178046\n",
      "saving model with loss 1.94942307472229\n",
      "Epoch 6 Batch 62500 Loss 1.6655070781707764 in 15.243877\n",
      "Epoch 6 Batch 63000 Loss 1.8118627071380615 in 15.181134\n",
      "saving model with loss 1.8118627071380615\n",
      "Epoch 6 Batch 63500 Loss 1.7361366748809814 in 15.267857\n",
      "Epoch 6 Batch 64000 Loss 1.735162377357483 in 15.184930\n",
      "saving model with loss 1.735162377357483\n",
      "Epoch 6 Batch 64500 Loss 1.8635587692260742 in 15.224136\n",
      "Epoch 6 Batch 65000 Loss 1.7384731769561768 in 15.171398\n",
      "saving model with loss 1.7384731769561768\n",
      "Epoch 6 Batch 65500 Loss 1.83657705783844 in 15.240934\n",
      "Epoch 6 Batch 66000 Loss 2.0886168479919434 in 15.178083\n",
      "saving model with loss 2.0886168479919434\n",
      "Epoch 6 Batch 66500 Loss 1.9118379354476929 in 15.221507\n",
      "Epoch 6 Batch 67000 Loss 1.8276965618133545 in 15.178478\n",
      "saving model with loss 1.8276965618133545\n",
      "Epoch 6 Batch 67500 Loss 1.758025884628296 in 15.241866\n",
      "Epoch 6 Batch 68000 Loss 1.842484712600708 in 15.198080\n",
      "saving model with loss 1.842484712600708\n",
      "Epoch 6 Batch 68500 Loss 1.895326852798462 in 15.230087\n",
      "Epoch 6 Batch 69000 Loss 1.8527940511703491 in 15.176919\n",
      "saving model with loss 1.8527940511703491\n",
      "Epoch 6 Batch 69500 Loss 1.8491395711898804 in 15.213424\n",
      "Epoch 6 Batch 70000 Loss 1.9325460195541382 in 15.155616\n",
      "saving model with loss 1.9325460195541382\n",
      "Epoch 6 Batch 70500 Loss 1.9163414239883423 in 15.240240\n",
      "Epoch 7 Loss 1.689663290977478\n",
      "Time taken for 1 epoch 2146.976076364517\n",
      "number of batches : 70553\n",
      "Epoch 7 Batch 0 Loss 1.785614013671875 in 1.172560\n",
      "saving model with loss 1.785614013671875\n",
      "Epoch 7 Batch 500 Loss 1.8104660511016846 in 15.236638\n",
      "Epoch 7 Batch 1000 Loss 1.8021113872528076 in 15.181006\n",
      "saving model with loss 1.8021113872528076\n",
      "Epoch 7 Batch 1500 Loss 1.7508385181427002 in 15.265056\n",
      "Epoch 7 Batch 2000 Loss 1.6527332067489624 in 15.158367\n",
      "saving model with loss 1.6527332067489624\n",
      "Epoch 7 Batch 2500 Loss 1.8575321435928345 in 15.243466\n",
      "Epoch 7 Batch 3000 Loss 1.8798646926879883 in 15.183715\n",
      "saving model with loss 1.8798646926879883\n",
      "Epoch 7 Batch 3500 Loss 1.664038062095642 in 15.245326\n",
      "Epoch 7 Batch 4000 Loss 1.9241126775741577 in 15.180696\n",
      "saving model with loss 1.9241126775741577\n",
      "Epoch 7 Batch 4500 Loss 1.8371076583862305 in 15.212995\n",
      "Epoch 7 Batch 5000 Loss 1.6947479248046875 in 15.179606\n",
      "saving model with loss 1.6947479248046875\n",
      "Epoch 7 Batch 5500 Loss 1.9053579568862915 in 15.214631\n",
      "Epoch 7 Batch 6000 Loss 1.7732629776000977 in 15.164032\n",
      "saving model with loss 1.7732629776000977\n",
      "Epoch 7 Batch 6500 Loss 1.7946789264678955 in 15.230507\n",
      "Epoch 7 Batch 7000 Loss 1.6977962255477905 in 15.146335\n",
      "saving model with loss 1.6977962255477905\n",
      "Epoch 7 Batch 7500 Loss 1.8987815380096436 in 15.241678\n",
      "Epoch 7 Batch 8000 Loss 1.8158782720565796 in 15.155546\n",
      "saving model with loss 1.8158782720565796\n",
      "Epoch 7 Batch 8500 Loss 1.791210412979126 in 15.254099\n",
      "Epoch 7 Batch 9000 Loss 1.800694227218628 in 15.178547\n",
      "saving model with loss 1.800694227218628\n",
      "Epoch 7 Batch 9500 Loss 1.9378273487091064 in 15.249623\n",
      "Epoch 7 Batch 10000 Loss 1.8752491474151611 in 15.173906\n",
      "saving model with loss 1.8752491474151611\n",
      "Epoch 7 Batch 10500 Loss 1.8557307720184326 in 15.231906\n",
      "Epoch 7 Batch 11000 Loss 1.8414714336395264 in 15.176560\n",
      "saving model with loss 1.8414714336395264\n",
      "Epoch 7 Batch 11500 Loss 1.7127249240875244 in 15.234406\n",
      "Epoch 7 Batch 12000 Loss 1.749943733215332 in 15.179531\n",
      "saving model with loss 1.749943733215332\n",
      "Epoch 7 Batch 12500 Loss 1.8774827718734741 in 15.244989\n",
      "Epoch 7 Batch 13000 Loss 1.9351532459259033 in 15.180939\n",
      "saving model with loss 1.9351532459259033\n",
      "Epoch 7 Batch 13500 Loss 1.8261886835098267 in 15.245948\n",
      "Epoch 7 Batch 14000 Loss 1.9510295391082764 in 15.191771\n",
      "saving model with loss 1.9510295391082764\n",
      "Epoch 7 Batch 14500 Loss 1.8644979000091553 in 15.244068\n",
      "Epoch 7 Batch 15000 Loss 1.9285728931427002 in 15.175152\n",
      "saving model with loss 1.9285728931427002\n",
      "Epoch 7 Batch 15500 Loss 1.8437436819076538 in 15.215661\n",
      "Epoch 7 Batch 16000 Loss 1.786656379699707 in 15.154501\n",
      "saving model with loss 1.786656379699707\n",
      "Epoch 7 Batch 16500 Loss 1.7317394018173218 in 15.244241\n",
      "Epoch 7 Batch 17000 Loss 1.8043947219848633 in 15.178926\n",
      "saving model with loss 1.8043947219848633\n",
      "Epoch 7 Batch 17500 Loss 1.7521241903305054 in 15.238609\n",
      "Epoch 7 Batch 18000 Loss 2.0394585132598877 in 15.182917\n",
      "saving model with loss 2.0394585132598877\n",
      "Epoch 7 Batch 18500 Loss 1.8573917150497437 in 15.243547\n",
      "Epoch 7 Batch 19000 Loss 1.9111213684082031 in 15.170129\n",
      "saving model with loss 1.9111213684082031\n",
      "Epoch 7 Batch 19500 Loss 2.0404253005981445 in 15.235710\n",
      "Epoch 7 Batch 20000 Loss 1.8333593606948853 in 15.176925\n",
      "saving model with loss 1.8333593606948853\n",
      "Epoch 7 Batch 20500 Loss 1.9584271907806396 in 15.237745\n",
      "Epoch 7 Batch 21000 Loss 1.7688541412353516 in 15.149719\n",
      "saving model with loss 1.7688541412353516\n",
      "Epoch 7 Batch 21500 Loss 1.790353775024414 in 15.255549\n",
      "Epoch 7 Batch 22000 Loss 1.802132248878479 in 15.168358\n",
      "saving model with loss 1.802132248878479\n",
      "Epoch 7 Batch 22500 Loss 1.7801768779754639 in 15.247592\n",
      "Epoch 7 Batch 23000 Loss 1.8178001642227173 in 15.178368\n",
      "saving model with loss 1.8178001642227173\n",
      "Epoch 7 Batch 23500 Loss 1.7833153009414673 in 15.251110\n",
      "Epoch 7 Batch 24000 Loss 1.961301565170288 in 15.175199\n",
      "saving model with loss 1.961301565170288\n",
      "Epoch 7 Batch 24500 Loss 1.9303042888641357 in 15.238094\n",
      "Epoch 7 Batch 25000 Loss 1.7317407131195068 in 15.149208\n",
      "saving model with loss 1.7317407131195068\n",
      "Epoch 7 Batch 25500 Loss 1.7847074270248413 in 15.302093\n",
      "Epoch 7 Batch 26000 Loss 1.8569930791854858 in 15.150209\n",
      "saving model with loss 1.8569930791854858\n",
      "Epoch 7 Batch 26500 Loss 1.9353357553482056 in 15.211202\n",
      "Epoch 7 Batch 27000 Loss 1.79293954372406 in 15.553659\n",
      "saving model with loss 1.79293954372406\n",
      "Epoch 7 Batch 27500 Loss 1.7599157094955444 in 15.251521\n",
      "Epoch 7 Batch 28000 Loss 1.9205878973007202 in 15.171402\n",
      "saving model with loss 1.9205878973007202\n",
      "Epoch 7 Batch 28500 Loss 1.9007526636123657 in 15.237321\n",
      "Epoch 7 Batch 29000 Loss 1.9451525211334229 in 15.158317\n",
      "saving model with loss 1.9451525211334229\n",
      "Epoch 7 Batch 29500 Loss 1.7824056148529053 in 15.236457\n",
      "Epoch 7 Batch 30000 Loss 1.7687866687774658 in 15.165972\n",
      "saving model with loss 1.7687866687774658\n",
      "Epoch 7 Batch 30500 Loss 1.997645378112793 in 15.226650\n",
      "Epoch 7 Batch 31000 Loss 1.8759645223617554 in 15.174114\n",
      "saving model with loss 1.8759645223617554\n",
      "Epoch 7 Batch 31500 Loss 1.8570880889892578 in 15.221438\n",
      "Epoch 7 Batch 32000 Loss 1.876699447631836 in 15.181220\n",
      "saving model with loss 1.876699447631836\n",
      "Epoch 7 Batch 32500 Loss 2.0242927074432373 in 15.244649\n",
      "Epoch 7 Batch 33000 Loss 1.709191083908081 in 15.146990\n",
      "saving model with loss 1.709191083908081\n",
      "Epoch 7 Batch 33500 Loss 1.9376258850097656 in 15.244788\n",
      "Epoch 7 Batch 34000 Loss 1.6845929622650146 in 15.180084\n",
      "saving model with loss 1.6845929622650146\n",
      "Epoch 7 Batch 34500 Loss 1.7738415002822876 in 15.264404\n",
      "Epoch 7 Batch 35000 Loss 1.7849223613739014 in 15.211294\n",
      "saving model with loss 1.7849223613739014\n",
      "Epoch 7 Batch 35500 Loss 1.826957106590271 in 15.241205\n",
      "Epoch 7 Batch 36000 Loss 1.8290345668792725 in 15.165516\n",
      "saving model with loss 1.8290345668792725\n",
      "Epoch 7 Batch 36500 Loss 1.8323856592178345 in 15.262391\n",
      "Epoch 7 Batch 37000 Loss 1.8545942306518555 in 15.188228\n",
      "saving model with loss 1.8545942306518555\n",
      "Epoch 7 Batch 37500 Loss 1.9591948986053467 in 15.242115\n",
      "Epoch 7 Batch 38000 Loss 1.9763696193695068 in 15.174079\n",
      "saving model with loss 1.9763696193695068\n",
      "Epoch 7 Batch 38500 Loss 2.0407583713531494 in 15.238507\n",
      "Epoch 7 Batch 39000 Loss 1.701515793800354 in 15.179188\n",
      "saving model with loss 1.701515793800354\n",
      "Epoch 7 Batch 39500 Loss 1.7204844951629639 in 15.254421\n",
      "Epoch 7 Batch 40000 Loss 1.8025516271591187 in 15.173059\n",
      "saving model with loss 1.8025516271591187\n",
      "Epoch 7 Batch 40500 Loss 1.9054571390151978 in 15.241960\n",
      "Epoch 7 Batch 41000 Loss 1.9658714532852173 in 15.152777\n",
      "saving model with loss 1.9658714532852173\n",
      "Epoch 7 Batch 41500 Loss 1.8115622997283936 in 15.261571\n",
      "Epoch 7 Batch 42000 Loss 1.7778812646865845 in 15.162491\n",
      "saving model with loss 1.7778812646865845\n",
      "Epoch 7 Batch 42500 Loss 1.740115761756897 in 15.252044\n",
      "Epoch 7 Batch 43000 Loss 1.8212625980377197 in 15.181680\n",
      "saving model with loss 1.8212625980377197\n",
      "Epoch 7 Batch 43500 Loss 1.8602731227874756 in 15.260451\n",
      "Epoch 7 Batch 44000 Loss 1.8324830532073975 in 15.189182\n",
      "saving model with loss 1.8324830532073975\n",
      "Epoch 7 Batch 44500 Loss 1.8081769943237305 in 15.237781\n",
      "Epoch 7 Batch 45000 Loss 1.7610340118408203 in 15.148722\n",
      "saving model with loss 1.7610340118408203\n",
      "Epoch 7 Batch 45500 Loss 1.9139524698257446 in 15.257796\n",
      "Epoch 7 Batch 46000 Loss 1.6504939794540405 in 15.158703\n",
      "saving model with loss 1.6504939794540405\n",
      "Epoch 7 Batch 46500 Loss 1.844627022743225 in 15.245841\n",
      "Epoch 7 Batch 47000 Loss 1.7522733211517334 in 15.183920\n",
      "saving model with loss 1.7522733211517334\n",
      "Epoch 7 Batch 47500 Loss 1.8442554473876953 in 15.242142\n",
      "Epoch 7 Batch 48000 Loss 1.776145577430725 in 15.176468\n",
      "saving model with loss 1.776145577430725\n",
      "Epoch 7 Batch 48500 Loss 1.8078359365463257 in 15.238472\n",
      "Epoch 7 Batch 49000 Loss 1.9103447198867798 in 15.149395\n",
      "saving model with loss 1.9103447198867798\n",
      "Epoch 7 Batch 49500 Loss 1.7480617761611938 in 15.264423\n",
      "Epoch 7 Batch 50000 Loss 1.904409646987915 in 15.185722\n",
      "saving model with loss 1.904409646987915\n",
      "Epoch 7 Batch 50500 Loss 1.8079421520233154 in 15.258078\n",
      "Epoch 7 Batch 51000 Loss 1.8448066711425781 in 15.170399\n",
      "saving model with loss 1.8448066711425781\n",
      "Epoch 7 Batch 51500 Loss 1.9656455516815186 in 15.255514\n",
      "Epoch 7 Batch 52000 Loss 1.6970405578613281 in 15.175105\n",
      "saving model with loss 1.6970405578613281\n",
      "Epoch 7 Batch 52500 Loss 1.8253990411758423 in 15.248009\n",
      "Epoch 7 Batch 53000 Loss 1.8186604976654053 in 15.178614\n",
      "saving model with loss 1.8186604976654053\n",
      "Epoch 7 Batch 53500 Loss 1.9184949398040771 in 15.230059\n",
      "Epoch 7 Batch 54000 Loss 1.7497379779815674 in 15.174915\n",
      "saving model with loss 1.7497379779815674\n",
      "Epoch 7 Batch 54500 Loss 1.6534401178359985 in 15.238202\n",
      "Epoch 7 Batch 55000 Loss 1.9292573928833008 in 15.179269\n",
      "saving model with loss 1.9292573928833008\n",
      "Epoch 7 Batch 55500 Loss 1.830368995666504 in 15.298839\n",
      "Epoch 7 Batch 56000 Loss 1.8400169610977173 in 15.154720\n",
      "saving model with loss 1.8400169610977173\n",
      "Epoch 7 Batch 56500 Loss 1.7923895120620728 in 15.211851\n",
      "Epoch 7 Batch 57000 Loss 1.90605890750885 in 15.149725\n",
      "saving model with loss 1.90605890750885\n",
      "Epoch 7 Batch 57500 Loss 1.8975780010223389 in 15.212746\n",
      "Epoch 7 Batch 58000 Loss 1.840031385421753 in 15.150611\n",
      "saving model with loss 1.840031385421753\n",
      "Epoch 7 Batch 58500 Loss 1.876272201538086 in 15.228283\n",
      "Epoch 7 Batch 59000 Loss 1.8122345209121704 in 15.153571\n",
      "saving model with loss 1.8122345209121704\n",
      "Epoch 7 Batch 59500 Loss 1.8966974020004272 in 15.206232\n",
      "Epoch 7 Batch 60000 Loss 1.9856255054473877 in 15.152035\n",
      "saving model with loss 1.9856255054473877\n",
      "Epoch 7 Batch 60500 Loss 1.8059686422348022 in 15.200987\n",
      "Epoch 7 Batch 61000 Loss 1.8228886127471924 in 15.150818\n",
      "saving model with loss 1.8228886127471924\n",
      "Epoch 7 Batch 61500 Loss 1.8295215368270874 in 15.215428\n",
      "Epoch 7 Batch 62000 Loss 1.6916640996932983 in 15.150804\n",
      "saving model with loss 1.6916640996932983\n",
      "Epoch 7 Batch 62500 Loss 1.8023675680160522 in 15.208680\n",
      "Epoch 7 Batch 63000 Loss 1.6582313776016235 in 15.148966\n",
      "saving model with loss 1.6582313776016235\n",
      "Epoch 7 Batch 63500 Loss 1.8434665203094482 in 15.219996\n",
      "Epoch 7 Batch 64000 Loss 1.926304578781128 in 15.148738\n",
      "saving model with loss 1.926304578781128\n",
      "Epoch 7 Batch 64500 Loss 1.9120115041732788 in 15.212203\n",
      "Epoch 7 Batch 65000 Loss 1.8462355136871338 in 15.150616\n",
      "saving model with loss 1.8462355136871338\n",
      "Epoch 7 Batch 65500 Loss 1.8942234516143799 in 15.210410\n",
      "Epoch 7 Batch 66000 Loss 1.860876441001892 in 15.150553\n",
      "saving model with loss 1.860876441001892\n",
      "Epoch 7 Batch 66500 Loss 1.8277578353881836 in 15.215089\n",
      "Epoch 7 Batch 67000 Loss 1.8806232213974 in 15.153806\n",
      "saving model with loss 1.8806232213974\n",
      "Epoch 7 Batch 67500 Loss 1.7975057363510132 in 15.210694\n",
      "Epoch 7 Batch 68000 Loss 1.748935341835022 in 15.147599\n",
      "saving model with loss 1.748935341835022\n",
      "Epoch 7 Batch 68500 Loss 1.8818308115005493 in 15.214777\n",
      "Epoch 7 Batch 69000 Loss 1.8253759145736694 in 15.152873\n",
      "saving model with loss 1.8253759145736694\n",
      "Epoch 7 Batch 69500 Loss 1.8742506504058838 in 15.211125\n",
      "Epoch 7 Batch 70000 Loss 1.7118356227874756 in 15.152728\n",
      "saving model with loss 1.7118356227874756\n",
      "Epoch 7 Batch 70500 Loss 1.8287750482559204 in 15.209665\n",
      "Epoch 8 Loss 1.8389179706573486\n",
      "Time taken for 1 epoch 2146.8002376556396\n",
      "number of batches : 70553\n",
      "Epoch 8 Batch 0 Loss 1.7739801406860352 in 1.300742\n",
      "saving model with loss 1.7739801406860352\n",
      "Epoch 8 Batch 500 Loss 1.841764211654663 in 15.200988\n",
      "Epoch 8 Batch 1000 Loss 1.86184823513031 in 15.146385\n",
      "saving model with loss 1.86184823513031\n",
      "Epoch 8 Batch 1500 Loss 1.876970887184143 in 15.218818\n",
      "Epoch 8 Batch 2000 Loss 1.8300912380218506 in 15.135042\n",
      "saving model with loss 1.8300912380218506\n",
      "Epoch 8 Batch 2500 Loss 1.984581708908081 in 15.227293\n",
      "Epoch 8 Batch 3000 Loss 1.7507740259170532 in 15.147227\n",
      "saving model with loss 1.7507740259170532\n",
      "Epoch 8 Batch 3500 Loss 1.7721576690673828 in 15.205872\n",
      "Epoch 8 Batch 4000 Loss 1.9859195947647095 in 15.138340\n",
      "saving model with loss 1.9859195947647095\n",
      "Epoch 8 Batch 4500 Loss 1.8840935230255127 in 15.218678\n",
      "Epoch 8 Batch 5000 Loss 1.8380153179168701 in 15.150933\n",
      "saving model with loss 1.8380153179168701\n",
      "Epoch 8 Batch 5500 Loss 1.8938701152801514 in 15.213975\n",
      "Epoch 8 Batch 6000 Loss 1.677901268005371 in 15.150194\n",
      "saving model with loss 1.677901268005371\n",
      "Epoch 8 Batch 6500 Loss 1.7382415533065796 in 15.211840\n",
      "Epoch 8 Batch 7000 Loss 1.8411407470703125 in 15.148051\n",
      "saving model with loss 1.8411407470703125\n",
      "Epoch 8 Batch 7500 Loss 1.8003559112548828 in 15.212239\n",
      "Epoch 8 Batch 8000 Loss 1.8102531433105469 in 15.153823\n",
      "saving model with loss 1.8102531433105469\n",
      "Epoch 8 Batch 8500 Loss 1.7296597957611084 in 15.272307\n",
      "Epoch 8 Batch 9000 Loss 1.831875205039978 in 15.324044\n",
      "saving model with loss 1.831875205039978\n",
      "Epoch 8 Batch 9500 Loss 1.767764687538147 in 15.220622\n",
      "Epoch 8 Batch 10000 Loss 1.8962745666503906 in 15.155208\n",
      "saving model with loss 1.8962745666503906\n",
      "Epoch 8 Batch 10500 Loss 1.8644212484359741 in 15.210162\n",
      "Epoch 8 Batch 11000 Loss 1.9061561822891235 in 15.150770\n",
      "saving model with loss 1.9061561822891235\n",
      "Epoch 8 Batch 11500 Loss 1.8936922550201416 in 15.208251\n",
      "Epoch 8 Batch 12000 Loss 1.8160597085952759 in 15.151877\n",
      "saving model with loss 1.8160597085952759\n",
      "Epoch 8 Batch 12500 Loss 1.7945654392242432 in 15.215621\n",
      "Epoch 8 Batch 13000 Loss 1.7426350116729736 in 15.150543\n",
      "saving model with loss 1.7426350116729736\n",
      "Epoch 8 Batch 13500 Loss 1.7080280780792236 in 15.614093\n",
      "Epoch 8 Batch 14000 Loss 1.8220916986465454 in 15.183879\n",
      "saving model with loss 1.8220916986465454\n",
      "Epoch 8 Batch 14500 Loss 1.8549003601074219 in 15.244836\n",
      "Epoch 8 Batch 15000 Loss 1.71621572971344 in 15.174740\n",
      "saving model with loss 1.71621572971344\n",
      "Epoch 8 Batch 15500 Loss 1.836646318435669 in 15.246737\n",
      "Epoch 8 Batch 16000 Loss 1.8257696628570557 in 15.176824\n",
      "saving model with loss 1.8257696628570557\n",
      "Epoch 8 Batch 16500 Loss 1.8848931789398193 in 15.244276\n",
      "Epoch 8 Batch 17000 Loss 1.8848005533218384 in 15.176858\n",
      "saving model with loss 1.8848005533218384\n",
      "Epoch 8 Batch 17500 Loss 1.7914644479751587 in 15.247185\n",
      "Epoch 8 Batch 18000 Loss 1.7386623620986938 in 15.147134\n",
      "saving model with loss 1.7386623620986938\n",
      "Epoch 8 Batch 18500 Loss 1.799068808555603 in 15.246999\n",
      "Epoch 8 Batch 19000 Loss 1.9014098644256592 in 15.150527\n",
      "saving model with loss 1.9014098644256592\n",
      "Epoch 8 Batch 19500 Loss 1.9818226099014282 in 15.242382\n",
      "Epoch 8 Batch 20000 Loss 1.8196691274642944 in 15.180278\n",
      "saving model with loss 1.8196691274642944\n",
      "Epoch 8 Batch 20500 Loss 1.767156958580017 in 15.253479\n",
      "Epoch 8 Batch 21000 Loss 1.7138694524765015 in 15.162439\n",
      "saving model with loss 1.7138694524765015\n",
      "Epoch 8 Batch 21500 Loss 1.8159959316253662 in 15.241060\n",
      "Epoch 8 Batch 22000 Loss 1.8305330276489258 in 15.147380\n",
      "saving model with loss 1.8305330276489258\n",
      "Epoch 8 Batch 22500 Loss 1.8615471124649048 in 15.241760\n",
      "Epoch 8 Batch 23000 Loss 1.8310073614120483 in 15.159967\n",
      "saving model with loss 1.8310073614120483\n",
      "Epoch 8 Batch 23500 Loss 1.7909057140350342 in 15.245104\n",
      "Epoch 8 Batch 24000 Loss 1.873056173324585 in 15.172933\n",
      "saving model with loss 1.873056173324585\n",
      "Epoch 8 Batch 24500 Loss 1.7721036672592163 in 15.246410\n",
      "Epoch 8 Batch 25000 Loss 1.8865282535552979 in 15.172839\n",
      "saving model with loss 1.8865282535552979\n",
      "Epoch 8 Batch 25500 Loss 1.6576400995254517 in 15.250323\n",
      "Epoch 8 Batch 26000 Loss 1.856432318687439 in 15.173954\n",
      "saving model with loss 1.856432318687439\n",
      "Epoch 8 Batch 26500 Loss 1.917292594909668 in 15.244182\n",
      "Epoch 8 Batch 27000 Loss 1.84773850440979 in 15.151883\n",
      "saving model with loss 1.84773850440979\n",
      "Epoch 8 Batch 27500 Loss 1.6700830459594727 in 15.245056\n",
      "Epoch 8 Batch 28000 Loss 1.8392629623413086 in 15.177229\n",
      "saving model with loss 1.8392629623413086\n",
      "Epoch 8 Batch 28500 Loss 1.897350549697876 in 15.250879\n",
      "Epoch 8 Batch 29000 Loss 1.7740724086761475 in 15.175822\n",
      "saving model with loss 1.7740724086761475\n",
      "Epoch 8 Batch 29500 Loss 1.8244695663452148 in 15.268281\n",
      "Epoch 8 Batch 30000 Loss 1.7850230932235718 in 15.183873\n",
      "saving model with loss 1.7850230932235718\n",
      "Epoch 8 Batch 30500 Loss 1.9002454280853271 in 15.243125\n",
      "Epoch 8 Batch 31000 Loss 1.6566346883773804 in 15.155117\n",
      "saving model with loss 1.6566346883773804\n",
      "Epoch 8 Batch 31500 Loss 1.9081001281738281 in 15.243929\n",
      "Epoch 8 Batch 32000 Loss 1.8704204559326172 in 15.189132\n",
      "saving model with loss 1.8704204559326172\n",
      "Epoch 8 Batch 32500 Loss 1.7493808269500732 in 15.226312\n",
      "Epoch 8 Batch 33000 Loss 1.8042529821395874 in 15.158167\n",
      "saving model with loss 1.8042529821395874\n",
      "Epoch 8 Batch 33500 Loss 1.8542855978012085 in 15.237140\n",
      "Epoch 8 Batch 34000 Loss 1.9575836658477783 in 15.153412\n",
      "saving model with loss 1.9575836658477783\n",
      "Epoch 8 Batch 34500 Loss 1.7038929462432861 in 15.224174\n",
      "Epoch 8 Batch 35000 Loss 1.949359655380249 in 15.153131\n",
      "saving model with loss 1.949359655380249\n",
      "Epoch 8 Batch 35500 Loss 1.9251257181167603 in 15.252334\n",
      "Epoch 8 Batch 36000 Loss 1.8813762664794922 in 15.168555\n",
      "saving model with loss 1.8813762664794922\n",
      "Epoch 8 Batch 36500 Loss 1.9743770360946655 in 15.227489\n",
      "Epoch 8 Batch 37000 Loss 1.8257766962051392 in 15.172723\n",
      "saving model with loss 1.8257766962051392\n",
      "Epoch 8 Batch 37500 Loss 1.7592674493789673 in 15.241818\n",
      "Epoch 8 Batch 38000 Loss 1.8646671772003174 in 15.212184\n",
      "saving model with loss 1.8646671772003174\n",
      "Epoch 8 Batch 38500 Loss 1.810469627380371 in 15.261417\n",
      "Epoch 8 Batch 39000 Loss 1.9531748294830322 in 15.159368\n",
      "saving model with loss 1.9531748294830322\n",
      "Epoch 8 Batch 39500 Loss 1.7488315105438232 in 15.246160\n",
      "Epoch 8 Batch 40000 Loss 1.8745687007904053 in 15.174708\n",
      "saving model with loss 1.8745687007904053\n",
      "Epoch 8 Batch 40500 Loss 1.9334766864776611 in 15.246184\n",
      "Epoch 8 Batch 41000 Loss 1.734391450881958 in 15.180841\n",
      "saving model with loss 1.734391450881958\n",
      "Epoch 8 Batch 41500 Loss 1.7077878713607788 in 15.287002\n",
      "Epoch 8 Batch 42000 Loss 1.835706353187561 in 15.162737\n",
      "saving model with loss 1.835706353187561\n",
      "Epoch 8 Batch 42500 Loss 1.8221981525421143 in 15.208139\n",
      "Epoch 8 Batch 43000 Loss 1.9228160381317139 in 15.154177\n",
      "saving model with loss 1.9228160381317139\n",
      "Epoch 8 Batch 43500 Loss 1.9834944009780884 in 15.215091\n",
      "Epoch 8 Batch 44000 Loss 1.9565690755844116 in 15.152115\n",
      "saving model with loss 1.9565690755844116\n",
      "Epoch 8 Batch 44500 Loss 1.7933632135391235 in 15.207732\n",
      "Epoch 8 Batch 45000 Loss 1.909487009048462 in 15.152623\n",
      "saving model with loss 1.909487009048462\n",
      "Epoch 8 Batch 45500 Loss 1.7322394847869873 in 15.210632\n",
      "Epoch 8 Batch 46000 Loss 1.963077187538147 in 15.153714\n",
      "saving model with loss 1.963077187538147\n",
      "Epoch 8 Batch 46500 Loss 1.8290733098983765 in 15.212434\n",
      "Epoch 8 Batch 47000 Loss 1.8413047790527344 in 15.149029\n",
      "saving model with loss 1.8413047790527344\n",
      "Epoch 8 Batch 47500 Loss 1.7703777551651 in 15.211304\n",
      "Epoch 8 Batch 48000 Loss 1.937755823135376 in 15.151724\n",
      "saving model with loss 1.937755823135376\n",
      "Epoch 8 Batch 48500 Loss 1.7556381225585938 in 15.213639\n",
      "Epoch 8 Batch 49000 Loss 1.721779227256775 in 15.154838\n",
      "saving model with loss 1.721779227256775\n",
      "Epoch 8 Batch 49500 Loss 1.8034042119979858 in 15.210107\n",
      "Epoch 8 Batch 50000 Loss 1.7504736185073853 in 15.151676\n",
      "saving model with loss 1.7504736185073853\n",
      "Epoch 8 Batch 50500 Loss 1.6756155490875244 in 15.212199\n",
      "Epoch 8 Batch 51000 Loss 1.982356309890747 in 15.147547\n",
      "saving model with loss 1.982356309890747\n",
      "Epoch 8 Batch 51500 Loss 1.9221493005752563 in 15.218955\n",
      "Epoch 8 Batch 52000 Loss 1.818547010421753 in 15.146946\n",
      "saving model with loss 1.818547010421753\n",
      "Epoch 8 Batch 52500 Loss 1.782815933227539 in 15.207182\n",
      "Epoch 8 Batch 53000 Loss 1.9704217910766602 in 15.155848\n",
      "saving model with loss 1.9704217910766602\n",
      "Epoch 8 Batch 53500 Loss 1.782867193222046 in 15.213560\n",
      "Epoch 8 Batch 54000 Loss 1.845568060874939 in 15.148386\n",
      "saving model with loss 1.845568060874939\n",
      "Epoch 8 Batch 54500 Loss 1.9837547540664673 in 15.216559\n",
      "Epoch 8 Batch 55000 Loss 1.792946457862854 in 15.148157\n",
      "saving model with loss 1.792946457862854\n",
      "Epoch 8 Batch 55500 Loss 1.855798363685608 in 15.211533\n",
      "Epoch 8 Batch 56000 Loss 1.947652816772461 in 15.153826\n",
      "saving model with loss 1.947652816772461\n",
      "Epoch 8 Batch 56500 Loss 1.9309409856796265 in 15.209795\n",
      "Epoch 8 Batch 57000 Loss 1.8306868076324463 in 15.156789\n",
      "saving model with loss 1.8306868076324463\n",
      "Epoch 8 Batch 57500 Loss 1.8088912963867188 in 15.211853\n",
      "Epoch 8 Batch 58000 Loss 1.7188680171966553 in 15.147544\n",
      "saving model with loss 1.7188680171966553\n",
      "Epoch 8 Batch 58500 Loss 1.7479270696640015 in 15.212479\n",
      "Epoch 8 Batch 59000 Loss 1.8106355667114258 in 15.148909\n",
      "saving model with loss 1.8106355667114258\n",
      "Epoch 8 Batch 59500 Loss 1.8951890468597412 in 15.216144\n",
      "Epoch 8 Batch 60000 Loss 1.8349268436431885 in 15.151563\n",
      "saving model with loss 1.8349268436431885\n",
      "Epoch 8 Batch 60500 Loss 2.0171632766723633 in 15.212368\n",
      "Epoch 8 Batch 61000 Loss 1.733609914779663 in 15.150151\n",
      "saving model with loss 1.733609914779663\n",
      "Epoch 8 Batch 61500 Loss 1.8512537479400635 in 15.211375\n",
      "Epoch 8 Batch 62000 Loss 1.8604648113250732 in 15.148960\n",
      "saving model with loss 1.8604648113250732\n",
      "Epoch 8 Batch 62500 Loss 1.7604129314422607 in 15.615700\n",
      "Epoch 8 Batch 63000 Loss 1.8497669696807861 in 15.162258\n",
      "saving model with loss 1.8497669696807861\n",
      "Epoch 8 Batch 63500 Loss 1.939385175704956 in 15.240867\n",
      "Epoch 8 Batch 64000 Loss 1.8310158252716064 in 15.157311\n",
      "saving model with loss 1.8310158252716064\n",
      "Epoch 8 Batch 64500 Loss 2.002211093902588 in 15.242246\n",
      "Epoch 8 Batch 65000 Loss 1.9015966653823853 in 15.210102\n",
      "saving model with loss 1.9015966653823853\n",
      "Epoch 8 Batch 65500 Loss 1.8029630184173584 in 15.243873\n",
      "Epoch 8 Batch 66000 Loss 1.7741937637329102 in 15.180436\n",
      "saving model with loss 1.7741937637329102\n",
      "Epoch 8 Batch 66500 Loss 1.945735216140747 in 15.213328\n",
      "Epoch 8 Batch 67000 Loss 1.8629980087280273 in 15.151623\n",
      "saving model with loss 1.8629980087280273\n",
      "Epoch 8 Batch 67500 Loss 2.087139844894409 in 15.241154\n",
      "Epoch 8 Batch 68000 Loss 1.8816356658935547 in 15.168867\n",
      "saving model with loss 1.8816356658935547\n",
      "Epoch 8 Batch 68500 Loss 1.8437983989715576 in 15.221813\n",
      "Epoch 8 Batch 69000 Loss 1.9070879220962524 in 15.157301\n",
      "saving model with loss 1.9070879220962524\n",
      "Epoch 8 Batch 69500 Loss 1.82758367061615 in 15.241405\n",
      "Epoch 8 Batch 70000 Loss 1.783266305923462 in 15.177363\n",
      "saving model with loss 1.783266305923462\n",
      "Epoch 8 Batch 70500 Loss 1.8340661525726318 in 15.245519\n",
      "Epoch 9 Loss 1.8729079961776733\n",
      "Time taken for 1 epoch 2146.4417662620544\n",
      "number of batches : 70553\n",
      "Epoch 9 Batch 0 Loss 1.6939070224761963 in 1.174917\n",
      "saving model with loss 1.6939070224761963\n",
      "Epoch 9 Batch 500 Loss 1.7888154983520508 in 15.222080\n",
      "Epoch 9 Batch 1000 Loss 1.686989426612854 in 15.154145\n",
      "saving model with loss 1.686989426612854\n",
      "Epoch 9 Batch 1500 Loss 1.7823619842529297 in 15.249382\n",
      "Epoch 9 Batch 2000 Loss 1.8212093114852905 in 15.183804\n",
      "saving model with loss 1.8212093114852905\n",
      "Epoch 9 Batch 2500 Loss 1.976483941078186 in 15.233653\n",
      "Epoch 9 Batch 3000 Loss 1.9024616479873657 in 15.150641\n",
      "saving model with loss 1.9024616479873657\n",
      "Epoch 9 Batch 3500 Loss 1.9254640340805054 in 15.255430\n",
      "Epoch 9 Batch 4000 Loss 1.7701127529144287 in 15.172377\n",
      "saving model with loss 1.7701127529144287\n",
      "Epoch 9 Batch 4500 Loss 1.9063056707382202 in 15.266447\n",
      "Epoch 9 Batch 5000 Loss 1.7126200199127197 in 15.178747\n",
      "saving model with loss 1.7126200199127197\n",
      "Epoch 9 Batch 5500 Loss 1.962916374206543 in 15.251693\n",
      "Epoch 9 Batch 6000 Loss 1.8822872638702393 in 15.165073\n",
      "saving model with loss 1.8822872638702393\n",
      "Epoch 9 Batch 6500 Loss 1.8659511804580688 in 15.249152\n",
      "Epoch 9 Batch 7000 Loss 1.9458144903182983 in 15.169413\n",
      "saving model with loss 1.9458144903182983\n",
      "Epoch 9 Batch 7500 Loss 1.8751423358917236 in 15.229234\n",
      "Epoch 9 Batch 8000 Loss 1.720597505569458 in 15.177257\n",
      "saving model with loss 1.720597505569458\n",
      "Epoch 9 Batch 8500 Loss 1.792824387550354 in 15.236775\n",
      "Epoch 9 Batch 9000 Loss 1.7985126972198486 in 15.150261\n",
      "saving model with loss 1.7985126972198486\n",
      "Epoch 9 Batch 9500 Loss 2.0120084285736084 in 15.242445\n",
      "Epoch 9 Batch 10000 Loss 1.8550266027450562 in 15.175120\n",
      "saving model with loss 1.8550266027450562\n",
      "Epoch 9 Batch 10500 Loss 1.9750735759735107 in 15.231655\n",
      "Epoch 9 Batch 11000 Loss 1.7733306884765625 in 15.152900\n",
      "saving model with loss 1.7733306884765625\n",
      "Epoch 9 Batch 11500 Loss 1.7772737741470337 in 15.239575\n",
      "Epoch 9 Batch 12000 Loss 1.8399460315704346 in 15.151321\n",
      "saving model with loss 1.8399460315704346\n",
      "Epoch 9 Batch 12500 Loss 1.7399282455444336 in 15.239213\n",
      "Epoch 9 Batch 13000 Loss 1.688249945640564 in 15.185581\n",
      "saving model with loss 1.688249945640564\n",
      "Epoch 9 Batch 13500 Loss 1.861006498336792 in 15.244113\n",
      "Epoch 9 Batch 14000 Loss 1.8273754119873047 in 15.178612\n",
      "saving model with loss 1.8273754119873047\n",
      "Epoch 9 Batch 14500 Loss 1.882206678390503 in 15.243239\n",
      "Epoch 9 Batch 15000 Loss 1.8353290557861328 in 15.186428\n",
      "saving model with loss 1.8353290557861328\n",
      "Epoch 9 Batch 15500 Loss 1.8677451610565186 in 15.236469\n",
      "Epoch 9 Batch 16000 Loss 1.8544343709945679 in 15.155761\n",
      "saving model with loss 1.8544343709945679\n",
      "Epoch 9 Batch 16500 Loss 1.8467590808868408 in 15.238169\n",
      "Epoch 9 Batch 17000 Loss 1.7696861028671265 in 15.184630\n",
      "saving model with loss 1.7696861028671265\n",
      "Epoch 9 Batch 17500 Loss 1.8877685070037842 in 15.251393\n",
      "Epoch 9 Batch 18000 Loss 1.8064124584197998 in 15.170058\n",
      "saving model with loss 1.8064124584197998\n",
      "Epoch 9 Batch 18500 Loss 1.8391754627227783 in 15.245073\n",
      "Epoch 9 Batch 19000 Loss 1.8412336111068726 in 15.178100\n",
      "saving model with loss 1.8412336111068726\n",
      "Epoch 9 Batch 19500 Loss 1.8779857158660889 in 15.244091\n",
      "Epoch 9 Batch 20000 Loss 1.733575463294983 in 15.150339\n",
      "saving model with loss 1.733575463294983\n",
      "Epoch 9 Batch 20500 Loss 1.9108940362930298 in 15.261553\n",
      "Epoch 9 Batch 21000 Loss 1.9063888788223267 in 15.180291\n",
      "saving model with loss 1.9063888788223267\n",
      "Epoch 9 Batch 21500 Loss 1.8031631708145142 in 15.232433\n",
      "Epoch 9 Batch 22000 Loss 1.7318483591079712 in 15.168944\n",
      "saving model with loss 1.7318483591079712\n",
      "Epoch 9 Batch 22500 Loss 1.8581498861312866 in 15.223281\n",
      "Epoch 9 Batch 23000 Loss 1.8092294931411743 in 15.174976\n",
      "saving model with loss 1.8092294931411743\n",
      "Epoch 9 Batch 23500 Loss 1.8855087757110596 in 15.214434\n",
      "Epoch 9 Batch 24000 Loss 1.9344379901885986 in 15.150137\n",
      "saving model with loss 1.9344379901885986\n",
      "Epoch 9 Batch 24500 Loss 1.9289581775665283 in 15.240844\n",
      "Epoch 9 Batch 25000 Loss 1.8094857931137085 in 15.153886\n",
      "saving model with loss 1.8094857931137085\n",
      "Epoch 9 Batch 25500 Loss 1.8312925100326538 in 15.246424\n",
      "Epoch 9 Batch 26000 Loss 1.8098695278167725 in 15.161813\n",
      "saving model with loss 1.8098695278167725\n",
      "Epoch 9 Batch 26500 Loss 1.7533925771713257 in 15.281441\n",
      "Epoch 9 Batch 27000 Loss 1.8768314123153687 in 15.213578\n",
      "saving model with loss 1.8768314123153687\n",
      "Epoch 9 Batch 27500 Loss 1.9540507793426514 in 15.210809\n",
      "Epoch 9 Batch 28000 Loss 1.7189662456512451 in 15.151239\n",
      "saving model with loss 1.7189662456512451\n",
      "Epoch 9 Batch 28500 Loss 1.9800889492034912 in 15.214113\n",
      "Epoch 9 Batch 29000 Loss 1.8851144313812256 in 15.150017\n",
      "saving model with loss 1.8851144313812256\n",
      "Epoch 9 Batch 29500 Loss 1.8860023021697998 in 15.209345\n",
      "Epoch 9 Batch 30000 Loss 1.7727903127670288 in 15.154259\n",
      "saving model with loss 1.7727903127670288\n",
      "Epoch 9 Batch 30500 Loss 1.975942611694336 in 15.211066\n",
      "Epoch 9 Batch 31000 Loss 1.7888027429580688 in 15.150122\n",
      "saving model with loss 1.7888027429580688\n",
      "Epoch 9 Batch 31500 Loss 1.7572141885757446 in 15.229738\n",
      "Epoch 9 Batch 32000 Loss 1.9071638584136963 in 15.532944\n",
      "saving model with loss 1.9071638584136963\n",
      "Epoch 9 Batch 32500 Loss 1.8569790124893188 in 15.247215\n",
      "Epoch 9 Batch 33000 Loss 1.9103606939315796 in 15.182358\n",
      "saving model with loss 1.9103606939315796\n",
      "Epoch 9 Batch 33500 Loss 1.875187635421753 in 15.236329\n",
      "Epoch 9 Batch 34000 Loss 1.7917978763580322 in 15.185688\n",
      "saving model with loss 1.7917978763580322\n",
      "Epoch 9 Batch 34500 Loss 1.842018485069275 in 15.258940\n",
      "Epoch 9 Batch 35000 Loss 1.8069744110107422 in 15.166113\n",
      "saving model with loss 1.8069744110107422\n",
      "Epoch 9 Batch 35500 Loss 1.892037034034729 in 15.240044\n",
      "Epoch 9 Batch 36000 Loss 1.6832573413848877 in 15.146587\n",
      "saving model with loss 1.6832573413848877\n",
      "Epoch 9 Batch 36500 Loss 1.7124292850494385 in 15.245337\n",
      "Epoch 9 Batch 37000 Loss 1.925777792930603 in 15.149855\n",
      "saving model with loss 1.925777792930603\n",
      "Epoch 9 Batch 37500 Loss 1.680546760559082 in 15.247417\n",
      "Epoch 9 Batch 38000 Loss 1.8393722772598267 in 15.147190\n",
      "saving model with loss 1.8393722772598267\n",
      "Epoch 9 Batch 38500 Loss 1.9742629528045654 in 15.238437\n",
      "Epoch 9 Batch 39000 Loss 1.8203303813934326 in 15.183944\n",
      "saving model with loss 1.8203303813934326\n",
      "Epoch 9 Batch 39500 Loss 1.833923578262329 in 15.241058\n",
      "Epoch 9 Batch 40000 Loss 1.779157042503357 in 15.155653\n",
      "saving model with loss 1.779157042503357\n",
      "Epoch 9 Batch 40500 Loss 1.9584343433380127 in 15.235308\n",
      "Epoch 9 Batch 41000 Loss 1.7639617919921875 in 15.348715\n",
      "saving model with loss 1.7639617919921875\n",
      "Epoch 9 Batch 41500 Loss 1.8646929264068604 in 15.386167\n",
      "Epoch 9 Batch 42000 Loss 1.8202053308486938 in 15.218421\n",
      "saving model with loss 1.8202053308486938\n",
      "Epoch 9 Batch 42500 Loss 1.8765323162078857 in 15.273778\n",
      "Epoch 9 Batch 43000 Loss 1.7775652408599854 in 15.244561\n",
      "saving model with loss 1.7775652408599854\n",
      "Epoch 9 Batch 43500 Loss 1.830834150314331 in 15.325685\n",
      "Epoch 9 Batch 44000 Loss 1.941509485244751 in 15.194036\n",
      "saving model with loss 1.941509485244751\n",
      "Epoch 9 Batch 44500 Loss 1.9573965072631836 in 15.270681\n",
      "Epoch 9 Batch 45000 Loss 1.7700891494750977 in 15.199458\n",
      "saving model with loss 1.7700891494750977\n",
      "Epoch 9 Batch 45500 Loss 1.8316932916641235 in 15.270492\n",
      "Epoch 9 Batch 46000 Loss 2.0950326919555664 in 15.186727\n",
      "saving model with loss 2.0950326919555664\n",
      "Epoch 9 Batch 46500 Loss 1.9020986557006836 in 15.251208\n",
      "Epoch 9 Batch 47000 Loss 1.824528694152832 in 15.178995\n",
      "saving model with loss 1.824528694152832\n",
      "Epoch 9 Batch 47500 Loss 1.8722026348114014 in 15.290112\n",
      "Epoch 9 Batch 48000 Loss 1.8930962085723877 in 15.193066\n",
      "saving model with loss 1.8930962085723877\n",
      "Epoch 9 Batch 48500 Loss 1.8461072444915771 in 15.262213\n",
      "Epoch 9 Batch 49000 Loss 1.813812017440796 in 15.193067\n",
      "saving model with loss 1.813812017440796\n",
      "Epoch 9 Batch 49500 Loss 1.949628472328186 in 15.260850\n",
      "Epoch 9 Batch 50000 Loss 1.798640489578247 in 15.196365\n",
      "saving model with loss 1.798640489578247\n",
      "Epoch 9 Batch 50500 Loss 1.7753593921661377 in 15.269962\n",
      "Epoch 9 Batch 51000 Loss 1.832790732383728 in 15.181494\n",
      "saving model with loss 1.832790732383728\n",
      "Epoch 9 Batch 51500 Loss 1.7377874851226807 in 15.281109\n",
      "Epoch 9 Batch 52000 Loss 1.9727693796157837 in 15.205499\n",
      "saving model with loss 1.9727693796157837\n",
      "Epoch 9 Batch 52500 Loss 1.8781229257583618 in 15.268002\n",
      "Epoch 9 Batch 53000 Loss 1.794702172279358 in 15.215887\n",
      "saving model with loss 1.794702172279358\n",
      "Epoch 9 Batch 53500 Loss 1.7899034023284912 in 15.211030\n",
      "Epoch 9 Batch 54000 Loss 1.8229162693023682 in 15.142542\n",
      "saving model with loss 1.8229162693023682\n",
      "Epoch 9 Batch 54500 Loss 1.8972256183624268 in 15.629250\n",
      "Epoch 9 Batch 55000 Loss 1.8981115818023682 in 15.184964\n",
      "saving model with loss 1.8981115818023682\n",
      "Epoch 9 Batch 55500 Loss 1.8345310688018799 in 15.280350\n",
      "Epoch 9 Batch 56000 Loss 1.7107559442520142 in 15.186486\n",
      "saving model with loss 1.7107559442520142\n",
      "Epoch 9 Batch 56500 Loss 1.8609672784805298 in 15.431087\n",
      "Epoch 9 Batch 57000 Loss 1.8451707363128662 in 15.274743\n",
      "saving model with loss 1.8451707363128662\n",
      "Epoch 9 Batch 57500 Loss 1.7456563711166382 in 15.251827\n",
      "Epoch 9 Batch 58000 Loss 1.7911512851715088 in 15.192111\n",
      "saving model with loss 1.7911512851715088\n",
      "Epoch 9 Batch 58500 Loss 1.838029146194458 in 15.258120\n",
      "Epoch 9 Batch 59000 Loss 1.7096971273422241 in 15.192290\n",
      "saving model with loss 1.7096971273422241\n",
      "Epoch 9 Batch 59500 Loss 1.8174337148666382 in 15.266964\n",
      "Epoch 9 Batch 60000 Loss 1.8362743854522705 in 15.194198\n",
      "saving model with loss 1.8362743854522705\n",
      "Epoch 9 Batch 60500 Loss 2.1018192768096924 in 15.269971\n",
      "Epoch 9 Batch 61000 Loss 1.8735692501068115 in 15.220402\n",
      "saving model with loss 1.8735692501068115\n",
      "Epoch 9 Batch 61500 Loss 1.9167743921279907 in 15.345542\n",
      "Epoch 9 Batch 62000 Loss 1.8916311264038086 in 15.347995\n",
      "saving model with loss 1.8916311264038086\n",
      "Epoch 9 Batch 62500 Loss 1.8072036504745483 in 15.375401\n",
      "Epoch 9 Batch 63000 Loss 1.9426472187042236 in 15.229077\n",
      "saving model with loss 1.9426472187042236\n",
      "Epoch 9 Batch 63500 Loss 1.7915270328521729 in 15.270868\n",
      "Epoch 9 Batch 64000 Loss 1.7734800577163696 in 15.206110\n",
      "saving model with loss 1.7734800577163696\n",
      "Epoch 9 Batch 64500 Loss 1.7302017211914062 in 15.279763\n",
      "Epoch 9 Batch 65000 Loss 1.9426733255386353 in 15.196143\n",
      "saving model with loss 1.9426733255386353\n",
      "Epoch 9 Batch 65500 Loss 1.831423044204712 in 15.286683\n",
      "Epoch 9 Batch 66000 Loss 1.786707878112793 in 15.208711\n",
      "saving model with loss 1.786707878112793\n",
      "Epoch 9 Batch 66500 Loss 1.9952751398086548 in 15.272688\n",
      "Epoch 9 Batch 67000 Loss 1.7722444534301758 in 15.212987\n",
      "saving model with loss 1.7722444534301758\n",
      "Epoch 9 Batch 67500 Loss 1.7667757272720337 in 15.275198\n",
      "Epoch 9 Batch 68000 Loss 1.8819986581802368 in 15.202669\n",
      "saving model with loss 1.8819986581802368\n",
      "Epoch 9 Batch 68500 Loss 1.9192943572998047 in 15.276269\n",
      "Epoch 9 Batch 69000 Loss 1.8393802642822266 in 15.223836\n",
      "saving model with loss 1.8393802642822266\n",
      "Epoch 9 Batch 69500 Loss 1.9654842615127563 in 15.286200\n",
      "Epoch 9 Batch 70000 Loss 1.862372636795044 in 15.216401\n",
      "saving model with loss 1.862372636795044\n",
      "Epoch 9 Batch 70500 Loss 1.7001535892486572 in 15.296758\n",
      "Epoch 10 Loss 1.7262767553329468\n",
      "Time taken for 1 epoch 2150.0255455970764\n",
      "number of batches : 70553\n",
      "Epoch 10 Batch 0 Loss 1.814666509628296 in 1.175519\n",
      "saving model with loss 1.814666509628296\n",
      "Epoch 10 Batch 500 Loss 1.7632780075073242 in 15.270886\n",
      "Epoch 10 Batch 1000 Loss 1.8257834911346436 in 15.199154\n",
      "saving model with loss 1.8257834911346436\n",
      "Epoch 10 Batch 1500 Loss 1.8484073877334595 in 15.263432\n",
      "Epoch 10 Batch 2000 Loss 1.8695513010025024 in 15.218135\n",
      "saving model with loss 1.8695513010025024\n",
      "Epoch 10 Batch 2500 Loss 1.7875980138778687 in 15.279764\n",
      "Epoch 10 Batch 3000 Loss 1.9082053899765015 in 15.205441\n",
      "saving model with loss 1.9082053899765015\n",
      "Epoch 10 Batch 3500 Loss 1.90329909324646 in 15.276218\n",
      "Epoch 10 Batch 4000 Loss 1.7619556188583374 in 15.206721\n",
      "saving model with loss 1.7619556188583374\n",
      "Epoch 10 Batch 4500 Loss 1.7693185806274414 in 15.266752\n",
      "Epoch 10 Batch 5000 Loss 1.8175138235092163 in 15.206273\n",
      "saving model with loss 1.8175138235092163\n",
      "Epoch 10 Batch 5500 Loss 1.9156577587127686 in 15.283481\n",
      "Epoch 10 Batch 6000 Loss 1.9866228103637695 in 15.213085\n",
      "saving model with loss 1.9866228103637695\n",
      "Epoch 10 Batch 6500 Loss 1.9564402103424072 in 15.290945\n",
      "Epoch 10 Batch 7000 Loss 1.8321481943130493 in 15.191887\n",
      "saving model with loss 1.8321481943130493\n",
      "Epoch 10 Batch 7500 Loss 1.902425765991211 in 15.294671\n",
      "Epoch 10 Batch 8000 Loss 1.7840124368667603 in 15.193063\n",
      "saving model with loss 1.7840124368667603\n",
      "Epoch 10 Batch 8500 Loss 1.9192558526992798 in 15.289656\n",
      "Epoch 10 Batch 9000 Loss 1.897382378578186 in 15.214945\n",
      "saving model with loss 1.897382378578186\n",
      "Epoch 10 Batch 9500 Loss 1.924980878829956 in 15.281103\n",
      "Epoch 10 Batch 10000 Loss 1.9253976345062256 in 15.206177\n",
      "saving model with loss 1.9253976345062256\n",
      "Epoch 10 Batch 10500 Loss 1.8923633098602295 in 15.275277\n",
      "Epoch 10 Batch 11000 Loss 1.857109785079956 in 15.209267\n",
      "saving model with loss 1.857109785079956\n",
      "Epoch 10 Batch 11500 Loss 1.9391673803329468 in 15.333887\n",
      "Epoch 10 Batch 12000 Loss 1.8052688837051392 in 15.208435\n",
      "saving model with loss 1.8052688837051392\n",
      "Epoch 10 Batch 12500 Loss 1.8700249195098877 in 15.277851\n",
      "Epoch 10 Batch 13000 Loss 1.7099905014038086 in 15.187660\n",
      "saving model with loss 1.7099905014038086\n",
      "Epoch 10 Batch 13500 Loss 1.7937800884246826 in 15.271888\n",
      "Epoch 10 Batch 14000 Loss 1.8398551940917969 in 15.210996\n",
      "saving model with loss 1.8398551940917969\n",
      "Epoch 10 Batch 14500 Loss 1.7785961627960205 in 15.263369\n",
      "Epoch 10 Batch 15000 Loss 1.7593061923980713 in 15.208296\n",
      "saving model with loss 1.7593061923980713\n",
      "Epoch 10 Batch 15500 Loss 1.7990525960922241 in 15.302984\n",
      "Epoch 10 Batch 16000 Loss 1.8561022281646729 in 15.203191\n",
      "saving model with loss 1.8561022281646729\n",
      "Epoch 10 Batch 16500 Loss 1.9944531917572021 in 15.270709\n",
      "Epoch 10 Batch 17000 Loss 1.9140602350234985 in 15.215572\n",
      "saving model with loss 1.9140602350234985\n",
      "Epoch 10 Batch 17500 Loss 1.9739983081817627 in 15.272265\n",
      "Epoch 10 Batch 18000 Loss 1.9510717391967773 in 15.220546\n",
      "saving model with loss 1.9510717391967773\n",
      "Epoch 10 Batch 18500 Loss 2.005852222442627 in 15.258672\n",
      "Epoch 10 Batch 19000 Loss 1.8755528926849365 in 15.221762\n",
      "saving model with loss 1.8755528926849365\n",
      "Epoch 10 Batch 19500 Loss 1.8008006811141968 in 15.290644\n",
      "Epoch 10 Batch 20000 Loss 1.8497469425201416 in 15.205199\n",
      "saving model with loss 1.8497469425201416\n",
      "Epoch 10 Batch 20500 Loss 1.8421818017959595 in 15.292127\n",
      "Epoch 10 Batch 21000 Loss 1.9018360376358032 in 15.214233\n",
      "saving model with loss 1.9018360376358032\n",
      "Epoch 10 Batch 21500 Loss 1.8838655948638916 in 15.278483\n",
      "Epoch 10 Batch 22000 Loss 1.8641029596328735 in 15.208744\n",
      "saving model with loss 1.8641029596328735\n",
      "Epoch 10 Batch 22500 Loss 1.7461172342300415 in 15.266684\n",
      "Epoch 10 Batch 23000 Loss 1.7098747491836548 in 15.222554\n",
      "saving model with loss 1.7098747491836548\n",
      "Epoch 10 Batch 23500 Loss 1.8863041400909424 in 15.272546\n",
      "Epoch 10 Batch 24000 Loss 2.0161972045898438 in 15.203027\n",
      "saving model with loss 2.0161972045898438\n",
      "Epoch 10 Batch 24500 Loss 1.9137672185897827 in 15.278705\n",
      "Epoch 10 Batch 25000 Loss 1.701055884361267 in 15.221828\n",
      "saving model with loss 1.701055884361267\n",
      "Epoch 10 Batch 25500 Loss 1.9421145915985107 in 15.278707\n",
      "Epoch 10 Batch 26000 Loss 1.6998875141143799 in 15.206442\n",
      "saving model with loss 1.6998875141143799\n",
      "Epoch 10 Batch 26500 Loss 1.9159374237060547 in 15.290800\n",
      "Epoch 10 Batch 27000 Loss 1.8518505096435547 in 15.200552\n",
      "saving model with loss 1.8518505096435547\n",
      "Epoch 10 Batch 27500 Loss 1.9166755676269531 in 15.286761\n",
      "Epoch 10 Batch 28000 Loss 1.8295271396636963 in 15.209521\n",
      "saving model with loss 1.8295271396636963\n",
      "Epoch 10 Batch 28500 Loss 1.7899802923202515 in 15.280890\n",
      "Epoch 10 Batch 29000 Loss 2.029881477355957 in 15.205544\n",
      "saving model with loss 2.029881477355957\n",
      "Epoch 10 Batch 29500 Loss 1.845004677772522 in 15.266155\n",
      "Epoch 10 Batch 30000 Loss 1.6531829833984375 in 15.215041\n",
      "saving model with loss 1.6531829833984375\n",
      "Epoch 10 Batch 30500 Loss 1.8228490352630615 in 15.285120\n",
      "Epoch 10 Batch 31000 Loss 1.9583736658096313 in 15.226678\n",
      "saving model with loss 1.9583736658096313\n",
      "Epoch 10 Batch 31500 Loss 1.7749649286270142 in 15.282872\n",
      "Epoch 10 Batch 32000 Loss 1.874138593673706 in 15.202798\n",
      "saving model with loss 1.874138593673706\n",
      "Epoch 10 Batch 32500 Loss 1.8177659511566162 in 15.283640\n",
      "Epoch 10 Batch 33000 Loss 1.8766963481903076 in 15.230455\n",
      "saving model with loss 1.8766963481903076\n",
      "Epoch 10 Batch 33500 Loss 1.8086540699005127 in 15.283324\n",
      "Epoch 10 Batch 34000 Loss 1.812481164932251 in 15.220049\n",
      "saving model with loss 1.812481164932251\n",
      "Epoch 10 Batch 34500 Loss 1.8069026470184326 in 15.291286\n",
      "Epoch 10 Batch 35000 Loss 1.8349937200546265 in 15.216890\n",
      "saving model with loss 1.8349937200546265\n",
      "Epoch 10 Batch 35500 Loss 1.8454859256744385 in 15.285739\n",
      "Epoch 10 Batch 36000 Loss 1.9192043542861938 in 15.235817\n",
      "saving model with loss 1.9192043542861938\n",
      "Epoch 10 Batch 36500 Loss 1.9844738245010376 in 15.276053\n",
      "Epoch 10 Batch 37000 Loss 1.7423830032348633 in 15.228615\n",
      "saving model with loss 1.7423830032348633\n",
      "Epoch 10 Batch 37500 Loss 1.914754867553711 in 15.280602\n",
      "Epoch 10 Batch 38000 Loss 1.750329613685608 in 15.223031\n",
      "saving model with loss 1.750329613685608\n",
      "Epoch 10 Batch 38500 Loss 1.9735863208770752 in 15.281619\n",
      "Epoch 10 Batch 39000 Loss 1.7153186798095703 in 15.215751\n",
      "saving model with loss 1.7153186798095703\n",
      "Epoch 10 Batch 39500 Loss 1.9812580347061157 in 15.278706\n",
      "Epoch 10 Batch 40000 Loss 1.737656831741333 in 15.234005\n",
      "saving model with loss 1.737656831741333\n",
      "Epoch 10 Batch 40500 Loss 1.7694365978240967 in 15.279136\n",
      "Epoch 10 Batch 41000 Loss 1.8267968893051147 in 15.203254\n",
      "saving model with loss 1.8267968893051147\n",
      "Epoch 10 Batch 41500 Loss 1.8207423686981201 in 15.276599\n",
      "Epoch 10 Batch 42000 Loss 1.7837040424346924 in 15.242490\n",
      "saving model with loss 1.7837040424346924\n",
      "Epoch 10 Batch 42500 Loss 1.9564917087554932 in 15.306361\n",
      "Epoch 10 Batch 43000 Loss 1.8984375 in 15.210741\n",
      "saving model with loss 1.8984375\n",
      "Epoch 10 Batch 43500 Loss 1.8400806188583374 in 15.280063\n",
      "Epoch 10 Batch 44000 Loss 1.8060461282730103 in 15.205252\n",
      "saving model with loss 1.8060461282730103\n",
      "Epoch 10 Batch 44500 Loss 1.8776378631591797 in 15.291641\n",
      "Epoch 10 Batch 45000 Loss 1.9609668254852295 in 15.224742\n",
      "saving model with loss 1.9609668254852295\n",
      "Epoch 10 Batch 45500 Loss 1.7468290328979492 in 15.299350\n",
      "Epoch 10 Batch 46000 Loss 1.8232333660125732 in 15.219855\n",
      "saving model with loss 1.8232333660125732\n",
      "Epoch 10 Batch 46500 Loss 1.8153235912322998 in 15.299465\n",
      "Epoch 10 Batch 47000 Loss 1.8199001550674438 in 15.215150\n",
      "saving model with loss 1.8199001550674438\n",
      "Epoch 10 Batch 47500 Loss 1.8096599578857422 in 15.289879\n",
      "Epoch 10 Batch 48000 Loss 1.8374145030975342 in 15.230212\n",
      "saving model with loss 1.8374145030975342\n",
      "Epoch 10 Batch 48500 Loss 1.7845020294189453 in 15.274734\n",
      "Epoch 10 Batch 49000 Loss 1.832726240158081 in 15.221383\n",
      "saving model with loss 1.832726240158081\n",
      "Epoch 10 Batch 49500 Loss 1.878152847290039 in 15.277822\n",
      "Epoch 10 Batch 50000 Loss 1.8000675439834595 in 15.211072\n",
      "saving model with loss 1.8000675439834595\n",
      "Epoch 10 Batch 50500 Loss 2.0054097175598145 in 15.298683\n",
      "Epoch 10 Batch 51000 Loss 2.005774974822998 in 15.215124\n",
      "saving model with loss 2.005774974822998\n",
      "Epoch 10 Batch 51500 Loss 1.770754098892212 in 15.304435\n",
      "Epoch 10 Batch 52000 Loss 1.8082523345947266 in 15.215166\n",
      "saving model with loss 1.8082523345947266\n",
      "Epoch 10 Batch 52500 Loss 1.9392716884613037 in 15.303253\n",
      "Epoch 10 Batch 53000 Loss 1.8039443492889404 in 15.218008\n",
      "saving model with loss 1.8039443492889404\n",
      "Epoch 10 Batch 53500 Loss 1.7952067852020264 in 15.296734\n",
      "Epoch 10 Batch 54000 Loss 1.830129861831665 in 15.217937\n",
      "saving model with loss 1.830129861831665\n",
      "Epoch 10 Batch 54500 Loss 1.9500089883804321 in 15.277102\n",
      "Epoch 10 Batch 55000 Loss 1.8351809978485107 in 15.209058\n",
      "saving model with loss 1.8351809978485107\n",
      "Epoch 10 Batch 55500 Loss 1.8877503871917725 in 15.290225\n",
      "Epoch 10 Batch 56000 Loss 1.8797099590301514 in 15.215871\n",
      "saving model with loss 1.8797099590301514\n",
      "Epoch 10 Batch 56500 Loss 1.958152174949646 in 15.278123\n",
      "Epoch 10 Batch 57000 Loss 1.810703992843628 in 15.237226\n",
      "saving model with loss 1.810703992843628\n",
      "Epoch 10 Batch 57500 Loss 1.905735731124878 in 15.296458\n",
      "Epoch 10 Batch 58000 Loss 1.7753528356552124 in 15.226322\n",
      "saving model with loss 1.7753528356552124\n",
      "Epoch 10 Batch 58500 Loss 1.8069671392440796 in 15.296621\n",
      "Epoch 10 Batch 59000 Loss 2.0307183265686035 in 15.219446\n",
      "saving model with loss 2.0307183265686035\n",
      "Epoch 10 Batch 59500 Loss 1.843732476234436 in 15.295479\n",
      "Epoch 10 Batch 60000 Loss 1.8625017404556274 in 15.215817\n",
      "saving model with loss 1.8625017404556274\n",
      "Epoch 10 Batch 60500 Loss 1.8576148748397827 in 15.274446\n",
      "Epoch 10 Batch 61000 Loss 1.7958753108978271 in 15.215800\n",
      "saving model with loss 1.7958753108978271\n",
      "Epoch 10 Batch 61500 Loss 1.8213398456573486 in 15.286980\n",
      "Epoch 10 Batch 62000 Loss 1.8271243572235107 in 15.225246\n",
      "saving model with loss 1.8271243572235107\n",
      "Epoch 10 Batch 62500 Loss 1.871405005455017 in 15.280600\n",
      "Epoch 10 Batch 63000 Loss 1.9237829446792603 in 15.208241\n",
      "saving model with loss 1.9237829446792603\n",
      "Epoch 10 Batch 63500 Loss 1.86640202999115 in 15.303875\n",
      "Epoch 10 Batch 64000 Loss 1.8083810806274414 in 15.218088\n",
      "saving model with loss 1.8083810806274414\n",
      "Epoch 10 Batch 64500 Loss 1.8402507305145264 in 15.290995\n",
      "Epoch 10 Batch 65000 Loss 1.9449236392974854 in 15.226897\n",
      "saving model with loss 1.9449236392974854\n",
      "Epoch 10 Batch 65500 Loss 1.9309351444244385 in 15.308789\n",
      "Epoch 10 Batch 66000 Loss 1.979238510131836 in 15.218392\n",
      "saving model with loss 1.979238510131836\n",
      "Epoch 10 Batch 66500 Loss 1.8812248706817627 in 15.324327\n",
      "Epoch 10 Batch 67000 Loss 1.774903655052185 in 15.226980\n",
      "saving model with loss 1.774903655052185\n",
      "Epoch 10 Batch 67500 Loss 1.9418048858642578 in 15.295009\n",
      "Epoch 10 Batch 68000 Loss 1.7795299291610718 in 15.217914\n",
      "saving model with loss 1.7795299291610718\n",
      "Epoch 10 Batch 68500 Loss 1.9249293804168701 in 15.308497\n",
      "Epoch 10 Batch 69000 Loss 1.791860818862915 in 15.222591\n",
      "saving model with loss 1.791860818862915\n",
      "Epoch 10 Batch 69500 Loss 1.8352673053741455 in 15.300639\n",
      "Epoch 10 Batch 70000 Loss 1.8262455463409424 in 15.238389\n",
      "saving model with loss 1.8262455463409424\n",
      "Epoch 10 Batch 70500 Loss 1.8103634119033813 in 15.278814\n",
      "Epoch 11 Loss 1.8059619665145874\n",
      "Time taken for 1 epoch 2153.1994440555573\n",
      "number of batches : 70553\n",
      "Epoch 11 Batch 0 Loss 1.708299994468689 in 1.174011\n",
      "saving model with loss 1.708299994468689\n",
      "Epoch 11 Batch 500 Loss 1.7724882364273071 in 15.286101\n",
      "Epoch 11 Batch 1000 Loss 1.6984975337982178 in 15.201118\n",
      "saving model with loss 1.6984975337982178\n",
      "Epoch 11 Batch 1500 Loss 1.9223343133926392 in 15.271170\n",
      "Epoch 11 Batch 2000 Loss 1.9174766540527344 in 15.203804\n",
      "saving model with loss 1.9174766540527344\n",
      "Epoch 11 Batch 2500 Loss 1.9022678136825562 in 15.297260\n",
      "Epoch 11 Batch 3000 Loss 1.8510907888412476 in 15.230092\n",
      "saving model with loss 1.8510907888412476\n",
      "Epoch 11 Batch 3500 Loss 1.7633037567138672 in 15.303248\n",
      "Epoch 11 Batch 4000 Loss 1.9205272197723389 in 15.220405\n",
      "saving model with loss 1.9205272197723389\n",
      "Epoch 11 Batch 4500 Loss 1.868299126625061 in 15.291360\n",
      "Epoch 11 Batch 5000 Loss 1.956823706626892 in 15.221174\n",
      "saving model with loss 1.956823706626892\n",
      "Epoch 11 Batch 5500 Loss 1.9072024822235107 in 15.284299\n",
      "Epoch 11 Batch 6000 Loss 1.896759033203125 in 15.216607\n",
      "saving model with loss 1.896759033203125\n",
      "Epoch 11 Batch 6500 Loss 1.825060248374939 in 15.272085\n",
      "Epoch 11 Batch 7000 Loss 1.9674224853515625 in 15.210122\n",
      "saving model with loss 1.9674224853515625\n",
      "Epoch 11 Batch 7500 Loss 1.7510147094726562 in 15.281375\n",
      "Epoch 11 Batch 8000 Loss 1.9138473272323608 in 15.214914\n",
      "saving model with loss 1.9138473272323608\n",
      "Epoch 11 Batch 8500 Loss 1.8861751556396484 in 15.277626\n",
      "Epoch 11 Batch 9000 Loss 1.9399755001068115 in 15.207453\n",
      "saving model with loss 1.9399755001068115\n",
      "Epoch 11 Batch 9500 Loss 1.8442277908325195 in 15.277737\n",
      "Epoch 11 Batch 10000 Loss 1.7789396047592163 in 15.214325\n",
      "saving model with loss 1.7789396047592163\n",
      "Epoch 11 Batch 10500 Loss 1.8060764074325562 in 15.276091\n",
      "Epoch 11 Batch 11000 Loss 1.9672939777374268 in 15.235351\n",
      "saving model with loss 1.9672939777374268\n",
      "Epoch 11 Batch 11500 Loss 1.822525978088379 in 15.235794\n",
      "Epoch 11 Batch 12000 Loss 1.9594663381576538 in 15.183031\n",
      "saving model with loss 1.9594663381576538\n",
      "Epoch 11 Batch 12500 Loss 1.8180652856826782 in 15.243838\n",
      "Epoch 11 Batch 13000 Loss 1.7399168014526367 in 15.185369\n",
      "saving model with loss 1.7399168014526367\n",
      "Epoch 11 Batch 13500 Loss 1.8824924230575562 in 15.241052\n",
      "Epoch 11 Batch 14000 Loss 1.7869762182235718 in 15.180411\n",
      "saving model with loss 1.7869762182235718\n",
      "Epoch 11 Batch 14500 Loss 1.7877662181854248 in 15.241716\n",
      "Epoch 11 Batch 15000 Loss 1.8439801931381226 in 15.165554\n",
      "saving model with loss 1.8439801931381226\n",
      "Epoch 11 Batch 15500 Loss 1.8749881982803345 in 15.230737\n",
      "Epoch 11 Batch 16000 Loss 1.8815033435821533 in 15.175572\n",
      "saving model with loss 1.8815033435821533\n",
      "Epoch 11 Batch 16500 Loss 2.010468006134033 in 15.253901\n",
      "Epoch 11 Batch 17000 Loss 1.800276756286621 in 15.193820\n",
      "saving model with loss 1.800276756286621\n",
      "Epoch 11 Batch 17500 Loss 1.8135335445404053 in 15.252713\n",
      "Epoch 11 Batch 18000 Loss 1.926865816116333 in 15.165957\n",
      "saving model with loss 1.926865816116333\n",
      "Epoch 11 Batch 18500 Loss 1.9282726049423218 in 15.254851\n",
      "Epoch 11 Batch 19000 Loss 1.8083200454711914 in 15.189485\n",
      "saving model with loss 1.8083200454711914\n",
      "Epoch 11 Batch 19500 Loss 1.9110653400421143 in 15.250730\n",
      "Epoch 11 Batch 20000 Loss 1.806517243385315 in 15.182896\n",
      "saving model with loss 1.806517243385315\n",
      "Epoch 11 Batch 20500 Loss 1.7687991857528687 in 15.258374\n",
      "Epoch 11 Batch 21000 Loss 1.79337477684021 in 15.180042\n",
      "saving model with loss 1.79337477684021\n",
      "Epoch 11 Batch 21500 Loss 1.7736093997955322 in 15.227047\n",
      "Epoch 11 Batch 22000 Loss 1.9303823709487915 in 15.182894\n",
      "saving model with loss 1.9303823709487915\n",
      "Epoch 11 Batch 22500 Loss 1.9820659160614014 in 15.258070\n",
      "Epoch 11 Batch 23000 Loss 1.9233652353286743 in 15.175644\n",
      "saving model with loss 1.9233652353286743\n",
      "Epoch 11 Batch 23500 Loss 1.7974141836166382 in 15.656008\n",
      "Epoch 11 Batch 24000 Loss 1.6532102823257446 in 15.214695\n",
      "saving model with loss 1.6532102823257446\n",
      "Epoch 11 Batch 24500 Loss 1.8548866510391235 in 15.284095\n",
      "Epoch 11 Batch 25000 Loss 1.785831093788147 in 15.212006\n",
      "saving model with loss 1.785831093788147\n",
      "Epoch 11 Batch 25500 Loss 1.6586341857910156 in 15.260806\n",
      "Epoch 11 Batch 26000 Loss 1.8786194324493408 in 15.223200\n",
      "saving model with loss 1.8786194324493408\n",
      "Epoch 11 Batch 26500 Loss 1.8299169540405273 in 15.287201\n",
      "Epoch 11 Batch 27000 Loss 1.869168996810913 in 15.229080\n",
      "saving model with loss 1.869168996810913\n",
      "Epoch 11 Batch 27500 Loss 1.9020042419433594 in 15.276045\n",
      "Epoch 11 Batch 28000 Loss 1.8535845279693604 in 15.217114\n",
      "saving model with loss 1.8535845279693604\n",
      "Epoch 11 Batch 28500 Loss 1.8551127910614014 in 15.291251\n",
      "Epoch 11 Batch 29000 Loss 1.9183460474014282 in 15.204772\n",
      "saving model with loss 1.9183460474014282\n",
      "Epoch 11 Batch 29500 Loss 1.7897942066192627 in 15.287667\n",
      "Epoch 11 Batch 30000 Loss 1.7499563694000244 in 15.210249\n",
      "saving model with loss 1.7499563694000244\n",
      "Epoch 11 Batch 30500 Loss 2.0653367042541504 in 15.290997\n",
      "Epoch 11 Batch 31000 Loss 1.959495186805725 in 15.203817\n",
      "saving model with loss 1.959495186805725\n",
      "Epoch 11 Batch 31500 Loss 1.853886604309082 in 15.278068\n",
      "Epoch 11 Batch 32000 Loss 1.8216001987457275 in 15.212335\n",
      "saving model with loss 1.8216001987457275\n",
      "Epoch 11 Batch 32500 Loss 1.8883731365203857 in 15.286819\n",
      "Epoch 11 Batch 33000 Loss 1.9049797058105469 in 15.249902\n",
      "saving model with loss 1.9049797058105469\n",
      "Epoch 11 Batch 33500 Loss 1.7019011974334717 in 15.479966\n",
      "Epoch 11 Batch 34000 Loss 1.9261808395385742 in 15.227006\n",
      "saving model with loss 1.9261808395385742\n",
      "Epoch 11 Batch 34500 Loss 1.8437427282333374 in 15.287651\n",
      "Epoch 11 Batch 35000 Loss 1.8054240942001343 in 15.228704\n",
      "saving model with loss 1.8054240942001343\n",
      "Epoch 11 Batch 35500 Loss 1.7641369104385376 in 15.334580\n",
      "Epoch 11 Batch 36000 Loss 1.881403923034668 in 15.216004\n",
      "saving model with loss 1.881403923034668\n",
      "Epoch 11 Batch 36500 Loss 1.8600666522979736 in 15.272340\n",
      "Epoch 11 Batch 37000 Loss 1.8916209936141968 in 15.209162\n",
      "saving model with loss 1.8916209936141968\n",
      "Epoch 11 Batch 37500 Loss 1.898021936416626 in 15.279282\n",
      "Epoch 11 Batch 38000 Loss 1.8990228176116943 in 15.216304\n",
      "saving model with loss 1.8990228176116943\n",
      "Epoch 11 Batch 38500 Loss 1.9478397369384766 in 15.279421\n",
      "Epoch 11 Batch 39000 Loss 1.7483946084976196 in 15.224634\n",
      "saving model with loss 1.7483946084976196\n",
      "Epoch 11 Batch 39500 Loss 1.809637427330017 in 15.284859\n",
      "Epoch 11 Batch 40000 Loss 1.7294210195541382 in 15.206115\n",
      "saving model with loss 1.7294210195541382\n",
      "Epoch 11 Batch 40500 Loss 1.8153045177459717 in 15.298898\n",
      "Epoch 11 Batch 41000 Loss 1.9034515619277954 in 15.211237\n",
      "saving model with loss 1.9034515619277954\n",
      "Epoch 11 Batch 41500 Loss 1.8259952068328857 in 15.293495\n",
      "Epoch 11 Batch 42000 Loss 1.854718565940857 in 15.223819\n",
      "saving model with loss 1.854718565940857\n",
      "Epoch 11 Batch 42500 Loss 1.8056167364120483 in 15.277581\n",
      "Epoch 11 Batch 43000 Loss 1.6869672536849976 in 15.220277\n",
      "saving model with loss 1.6869672536849976\n",
      "Epoch 11 Batch 43500 Loss 1.8499606847763062 in 15.287428\n",
      "Epoch 11 Batch 44000 Loss 1.8243005275726318 in 15.212495\n",
      "saving model with loss 1.8243005275726318\n",
      "Epoch 11 Batch 44500 Loss 1.970415711402893 in 15.295684\n",
      "Epoch 11 Batch 45000 Loss 1.92925226688385 in 15.213157\n",
      "saving model with loss 1.92925226688385\n",
      "Epoch 11 Batch 45500 Loss 1.8617388010025024 in 15.286042\n",
      "Epoch 11 Batch 46000 Loss 1.8989269733428955 in 15.223783\n",
      "saving model with loss 1.8989269733428955\n",
      "Epoch 11 Batch 46500 Loss 2.0034565925598145 in 15.295542\n",
      "Epoch 11 Batch 47000 Loss 1.9614322185516357 in 15.213894\n",
      "saving model with loss 1.9614322185516357\n",
      "Epoch 11 Batch 47500 Loss 1.751549482345581 in 15.293998\n",
      "Epoch 11 Batch 48000 Loss 1.8339561223983765 in 15.218597\n",
      "saving model with loss 1.8339561223983765\n",
      "Epoch 11 Batch 48500 Loss 1.8281141519546509 in 15.297342\n",
      "Epoch 11 Batch 49000 Loss 1.8318166732788086 in 15.208300\n",
      "saving model with loss 1.8318166732788086\n",
      "Epoch 11 Batch 49500 Loss 1.806592583656311 in 15.290570\n",
      "Epoch 11 Batch 50000 Loss 1.702545404434204 in 15.231021\n",
      "saving model with loss 1.702545404434204\n",
      "Epoch 11 Batch 50500 Loss 1.7841453552246094 in 15.289008\n",
      "Epoch 11 Batch 51000 Loss 1.886519432067871 in 15.263222\n",
      "saving model with loss 1.886519432067871\n",
      "Epoch 11 Batch 51500 Loss 1.90628981590271 in 15.310916\n",
      "Epoch 11 Batch 52000 Loss 1.89435613155365 in 15.214436\n",
      "saving model with loss 1.89435613155365\n",
      "Epoch 11 Batch 52500 Loss 1.9442851543426514 in 15.282602\n",
      "Epoch 11 Batch 53000 Loss 1.9430081844329834 in 15.222149\n",
      "saving model with loss 1.9430081844329834\n",
      "Epoch 11 Batch 53500 Loss 1.9058496952056885 in 15.289914\n",
      "Epoch 11 Batch 54000 Loss 1.7325712442398071 in 15.219628\n",
      "saving model with loss 1.7325712442398071\n",
      "Epoch 11 Batch 54500 Loss 1.6696147918701172 in 15.309815\n",
      "Epoch 11 Batch 55000 Loss 2.019275426864624 in 15.219248\n",
      "saving model with loss 2.019275426864624\n",
      "Epoch 11 Batch 55500 Loss 1.9437484741210938 in 15.279408\n",
      "Epoch 11 Batch 56000 Loss 1.860518455505371 in 15.212673\n",
      "saving model with loss 1.860518455505371\n",
      "Epoch 11 Batch 56500 Loss 1.9711151123046875 in 15.291574\n",
      "Epoch 11 Batch 57000 Loss 1.8062002658843994 in 15.213842\n",
      "saving model with loss 1.8062002658843994\n",
      "Epoch 11 Batch 57500 Loss 1.8611023426055908 in 15.290401\n",
      "Epoch 11 Batch 58000 Loss 1.9106613397598267 in 15.221827\n",
      "saving model with loss 1.9106613397598267\n",
      "Epoch 11 Batch 58500 Loss 1.9816772937774658 in 15.300489\n",
      "Epoch 11 Batch 59000 Loss 1.8746345043182373 in 15.199503\n",
      "saving model with loss 1.8746345043182373\n",
      "Epoch 11 Batch 59500 Loss 1.8091037273406982 in 15.296462\n",
      "Epoch 11 Batch 60000 Loss 1.6400340795516968 in 15.217224\n",
      "saving model with loss 1.6400340795516968\n",
      "Epoch 11 Batch 60500 Loss 1.85296630859375 in 15.290345\n",
      "Epoch 11 Batch 61000 Loss 1.886885643005371 in 15.214840\n",
      "saving model with loss 1.886885643005371\n",
      "Epoch 11 Batch 61500 Loss 1.8269695043563843 in 15.281545\n",
      "Epoch 11 Batch 62000 Loss 1.9167038202285767 in 15.208863\n",
      "saving model with loss 1.9167038202285767\n",
      "Epoch 11 Batch 62500 Loss 1.9368829727172852 in 15.292435\n",
      "Epoch 11 Batch 63000 Loss 1.942360281944275 in 15.214131\n",
      "saving model with loss 1.942360281944275\n",
      "Epoch 11 Batch 63500 Loss 1.8962322473526 in 15.283581\n",
      "Epoch 11 Batch 64000 Loss 1.826013207435608 in 15.213421\n",
      "saving model with loss 1.826013207435608\n",
      "Epoch 11 Batch 64500 Loss 1.8859506845474243 in 15.277272\n",
      "Epoch 11 Batch 65000 Loss 1.873882532119751 in 15.230714\n",
      "saving model with loss 1.873882532119751\n",
      "Epoch 11 Batch 65500 Loss 1.7457592487335205 in 15.288366\n",
      "Epoch 11 Batch 66000 Loss 1.9810781478881836 in 15.212718\n",
      "saving model with loss 1.9810781478881836\n",
      "Epoch 11 Batch 66500 Loss 1.9793236255645752 in 15.276912\n",
      "Epoch 11 Batch 67000 Loss 1.8937000036239624 in 15.210481\n",
      "saving model with loss 1.8937000036239624\n",
      "Epoch 11 Batch 67500 Loss 1.7900657653808594 in 15.298526\n",
      "Epoch 11 Batch 68000 Loss 1.895006537437439 in 15.219838\n",
      "saving model with loss 1.895006537437439\n",
      "Epoch 11 Batch 68500 Loss 1.8913577795028687 in 15.298201\n",
      "Epoch 11 Batch 69000 Loss 2.097630023956299 in 15.230021\n",
      "saving model with loss 2.097630023956299\n",
      "Epoch 11 Batch 69500 Loss 1.9717706441879272 in 15.294640\n",
      "Epoch 11 Batch 70000 Loss 1.9775766134262085 in 15.221623\n",
      "saving model with loss 1.9775766134262085\n",
      "Epoch 11 Batch 70500 Loss 1.8426930904388428 in 15.281379\n",
      "Epoch 12 Loss 1.8754342794418335\n",
      "Time taken for 1 epoch 2153.1231150627136\n",
      "number of batches : 70553\n",
      "Epoch 12 Batch 0 Loss 1.8381710052490234 in 1.172893\n",
      "saving model with loss 1.8381710052490234\n",
      "Epoch 12 Batch 500 Loss 1.9080997705459595 in 15.293942\n",
      "Epoch 12 Batch 1000 Loss 1.9530627727508545 in 15.209787\n",
      "saving model with loss 1.9530627727508545\n",
      "Epoch 12 Batch 1500 Loss 1.8431907892227173 in 15.277209\n",
      "Epoch 12 Batch 2000 Loss 1.7071346044540405 in 15.235590\n",
      "saving model with loss 1.7071346044540405\n",
      "Epoch 12 Batch 2500 Loss 1.7586454153060913 in 15.289698\n",
      "Epoch 12 Batch 3000 Loss 1.8579204082489014 in 15.228557\n",
      "saving model with loss 1.8579204082489014\n",
      "Epoch 12 Batch 3500 Loss 1.89711594581604 in 15.268428\n",
      "Epoch 12 Batch 4000 Loss 1.766140341758728 in 15.217504\n",
      "saving model with loss 1.766140341758728\n",
      "Epoch 12 Batch 4500 Loss 1.8667036294937134 in 15.296410\n",
      "Epoch 12 Batch 5000 Loss 1.82025945186615 in 15.210294\n",
      "saving model with loss 1.82025945186615\n",
      "Epoch 12 Batch 5500 Loss 1.7991454601287842 in 15.295249\n",
      "Epoch 12 Batch 6000 Loss 1.8048994541168213 in 15.257367\n",
      "saving model with loss 1.8048994541168213\n",
      "Epoch 12 Batch 6500 Loss 1.9387891292572021 in 15.299719\n",
      "Epoch 12 Batch 7000 Loss 1.841442346572876 in 15.222281\n",
      "saving model with loss 1.841442346572876\n",
      "Epoch 12 Batch 7500 Loss 1.8670774698257446 in 15.279542\n",
      "Epoch 12 Batch 8000 Loss 1.8477494716644287 in 15.216816\n",
      "saving model with loss 1.8477494716644287\n",
      "Epoch 12 Batch 8500 Loss 1.8301862478256226 in 15.282423\n",
      "Epoch 12 Batch 9000 Loss 1.8951737880706787 in 15.224239\n",
      "saving model with loss 1.8951737880706787\n",
      "Epoch 12 Batch 9500 Loss 1.9525601863861084 in 15.280560\n",
      "Epoch 12 Batch 10000 Loss 1.7630653381347656 in 15.238537\n",
      "saving model with loss 1.7630653381347656\n",
      "Epoch 12 Batch 10500 Loss 1.837425947189331 in 15.297050\n",
      "Epoch 12 Batch 11000 Loss 2.016941785812378 in 15.239365\n",
      "saving model with loss 2.016941785812378\n",
      "Epoch 12 Batch 11500 Loss 1.826474905014038 in 15.271772\n",
      "Epoch 12 Batch 12000 Loss 1.8493400812149048 in 15.227402\n",
      "saving model with loss 1.8493400812149048\n",
      "Epoch 12 Batch 12500 Loss 1.7710758447647095 in 15.302313\n",
      "Epoch 12 Batch 13000 Loss 1.9139083623886108 in 15.235967\n",
      "saving model with loss 1.9139083623886108\n",
      "Epoch 12 Batch 13500 Loss 1.8191986083984375 in 15.262259\n",
      "Epoch 12 Batch 14000 Loss 1.8619121313095093 in 15.210439\n",
      "saving model with loss 1.8619121313095093\n",
      "Epoch 12 Batch 14500 Loss 1.8863770961761475 in 15.291552\n",
      "Epoch 12 Batch 15000 Loss 1.9910837411880493 in 15.197409\n",
      "saving model with loss 1.9910837411880493\n",
      "Epoch 12 Batch 15500 Loss 1.8301122188568115 in 15.285012\n",
      "Epoch 12 Batch 16000 Loss 2.0426197052001953 in 15.203085\n",
      "saving model with loss 2.0426197052001953\n",
      "Epoch 12 Batch 16500 Loss 1.927898645401001 in 15.283300\n",
      "Epoch 12 Batch 17000 Loss 1.864169716835022 in 15.210167\n",
      "saving model with loss 1.864169716835022\n",
      "Epoch 12 Batch 17500 Loss 1.8650095462799072 in 15.337327\n",
      "Epoch 12 Batch 18000 Loss 1.7010536193847656 in 15.177873\n",
      "saving model with loss 1.7010536193847656\n",
      "Epoch 12 Batch 18500 Loss 1.7665431499481201 in 15.249868\n",
      "Epoch 12 Batch 19000 Loss 1.9849523305892944 in 15.184575\n",
      "saving model with loss 1.9849523305892944\n",
      "Epoch 12 Batch 19500 Loss 1.8131229877471924 in 15.227109\n",
      "Epoch 12 Batch 20000 Loss 1.7909456491470337 in 15.180357\n",
      "saving model with loss 1.7909456491470337\n",
      "Epoch 12 Batch 20500 Loss 1.7944552898406982 in 15.244138\n",
      "Epoch 12 Batch 21000 Loss 1.689800500869751 in 15.174483\n",
      "saving model with loss 1.689800500869751\n",
      "Epoch 12 Batch 21500 Loss 1.8410108089447021 in 15.227173\n",
      "Epoch 12 Batch 22000 Loss 1.939297080039978 in 15.178124\n",
      "saving model with loss 1.939297080039978\n",
      "Epoch 12 Batch 22500 Loss 1.8495280742645264 in 15.248730\n",
      "Epoch 12 Batch 23000 Loss 1.986079454421997 in 15.172266\n",
      "saving model with loss 1.986079454421997\n",
      "Epoch 12 Batch 23500 Loss 1.8582271337509155 in 15.241222\n",
      "Epoch 12 Batch 24000 Loss 1.8677656650543213 in 15.180762\n",
      "saving model with loss 1.8677656650543213\n",
      "Epoch 12 Batch 24500 Loss 1.7679443359375 in 15.248174\n",
      "Epoch 12 Batch 25000 Loss 1.8946784734725952 in 15.177309\n",
      "saving model with loss 1.8946784734725952\n",
      "Epoch 12 Batch 25500 Loss 1.9223285913467407 in 15.254573\n",
      "Epoch 12 Batch 26000 Loss 1.9001390933990479 in 15.186216\n",
      "saving model with loss 1.9001390933990479\n",
      "Epoch 12 Batch 26500 Loss 1.9005422592163086 in 15.249614\n",
      "Epoch 12 Batch 27000 Loss 1.8626245260238647 in 15.183908\n",
      "saving model with loss 1.8626245260238647\n",
      "Epoch 12 Batch 27500 Loss 1.8724485635757446 in 15.664830\n",
      "Epoch 12 Batch 28000 Loss 1.819586157798767 in 15.223112\n",
      "saving model with loss 1.819586157798767\n",
      "Epoch 12 Batch 28500 Loss 1.7704805135726929 in 15.274486\n",
      "Epoch 12 Batch 29000 Loss 1.7970073223114014 in 15.225684\n",
      "saving model with loss 1.7970073223114014\n",
      "Epoch 12 Batch 29500 Loss 1.7485955953598022 in 15.287926\n",
      "Epoch 12 Batch 30000 Loss 1.7643321752548218 in 15.228616\n",
      "saving model with loss 1.7643321752548218\n",
      "Epoch 12 Batch 30500 Loss 1.9541429281234741 in 15.288673\n",
      "Epoch 12 Batch 31000 Loss 1.7448303699493408 in 15.213758\n",
      "saving model with loss 1.7448303699493408\n",
      "Epoch 12 Batch 31500 Loss 1.8315461874008179 in 15.281317\n",
      "Epoch 12 Batch 32000 Loss 1.8667879104614258 in 15.199184\n",
      "saving model with loss 1.8667879104614258\n",
      "Epoch 12 Batch 32500 Loss 2.1180167198181152 in 15.297029\n",
      "Epoch 12 Batch 33000 Loss 1.959718108177185 in 15.227236\n",
      "saving model with loss 1.959718108177185\n",
      "Epoch 12 Batch 33500 Loss 1.821964979171753 in 15.297113\n",
      "Epoch 12 Batch 34000 Loss 1.8544906377792358 in 15.209786\n",
      "saving model with loss 1.8544906377792358\n",
      "Epoch 12 Batch 34500 Loss 1.9376659393310547 in 15.279782\n",
      "Epoch 12 Batch 35000 Loss 1.9504438638687134 in 15.204615\n",
      "saving model with loss 1.9504438638687134\n",
      "Epoch 12 Batch 35500 Loss 1.877606987953186 in 15.322781\n",
      "Epoch 12 Batch 36000 Loss 1.8041448593139648 in 15.206068\n",
      "saving model with loss 1.8041448593139648\n",
      "Epoch 12 Batch 36500 Loss 1.9218820333480835 in 15.286059\n",
      "Epoch 12 Batch 37000 Loss 1.8494682312011719 in 15.227791\n",
      "saving model with loss 1.8494682312011719\n",
      "Epoch 12 Batch 37500 Loss 2.0051615238189697 in 15.286011\n",
      "Epoch 12 Batch 38000 Loss 1.782315969467163 in 15.213321\n",
      "saving model with loss 1.782315969467163\n",
      "Epoch 12 Batch 38500 Loss 1.9195350408554077 in 15.277557\n",
      "Epoch 12 Batch 39000 Loss 1.896929383277893 in 15.215675\n",
      "saving model with loss 1.896929383277893\n",
      "Epoch 12 Batch 39500 Loss 1.9636480808258057 in 15.274885\n",
      "Epoch 12 Batch 40000 Loss 1.8720859289169312 in 15.215135\n",
      "saving model with loss 1.8720859289169312\n",
      "Epoch 12 Batch 40500 Loss 1.9061059951782227 in 15.285782\n",
      "Epoch 12 Batch 41000 Loss 1.7399917840957642 in 15.204734\n",
      "saving model with loss 1.7399917840957642\n",
      "Epoch 12 Batch 41500 Loss 1.8629909753799438 in 15.290510\n",
      "Epoch 12 Batch 42000 Loss 1.8498163223266602 in 15.231139\n",
      "saving model with loss 1.8498163223266602\n",
      "Epoch 12 Batch 42500 Loss 2.0404393672943115 in 15.296453\n",
      "Epoch 12 Batch 43000 Loss 1.8505789041519165 in 15.219759\n",
      "saving model with loss 1.8505789041519165\n",
      "Epoch 12 Batch 43500 Loss 1.8013620376586914 in 15.282600\n",
      "Epoch 12 Batch 44000 Loss 1.777522325515747 in 15.215546\n",
      "saving model with loss 1.777522325515747\n",
      "Epoch 12 Batch 44500 Loss 1.895772933959961 in 15.287935\n",
      "Epoch 12 Batch 45000 Loss 1.89838445186615 in 15.225575\n",
      "saving model with loss 1.89838445186615\n",
      "Epoch 12 Batch 45500 Loss 1.803725004196167 in 15.283234\n",
      "Epoch 12 Batch 46000 Loss 1.9442994594573975 in 15.219675\n",
      "saving model with loss 1.9442994594573975\n",
      "Epoch 12 Batch 46500 Loss 1.9193298816680908 in 15.287246\n",
      "Epoch 12 Batch 47000 Loss 1.781934142112732 in 15.241236\n",
      "saving model with loss 1.781934142112732\n",
      "Epoch 12 Batch 47500 Loss 1.9196990728378296 in 15.280648\n",
      "Epoch 12 Batch 48000 Loss 1.6760509014129639 in 15.252097\n",
      "saving model with loss 1.6760509014129639\n",
      "Epoch 12 Batch 48500 Loss 1.8008196353912354 in 15.307196\n",
      "Epoch 12 Batch 49000 Loss 1.7739887237548828 in 15.207414\n",
      "saving model with loss 1.7739887237548828\n",
      "Epoch 12 Batch 49500 Loss 1.965848684310913 in 15.290272\n",
      "Epoch 12 Batch 50000 Loss 1.9867503643035889 in 15.212495\n",
      "saving model with loss 1.9867503643035889\n",
      "Epoch 12 Batch 50500 Loss 1.8262313604354858 in 15.287019\n",
      "Epoch 12 Batch 51000 Loss 1.8720983266830444 in 15.234126\n",
      "saving model with loss 1.8720983266830444\n",
      "Epoch 12 Batch 51500 Loss 1.9384580850601196 in 15.289882\n",
      "Epoch 12 Batch 52000 Loss 1.8562179803848267 in 15.218453\n",
      "saving model with loss 1.8562179803848267\n",
      "Epoch 12 Batch 52500 Loss 1.7615346908569336 in 15.290889\n",
      "Epoch 12 Batch 53000 Loss 1.8651784658432007 in 15.225833\n",
      "saving model with loss 1.8651784658432007\n",
      "Epoch 12 Batch 53500 Loss 1.8437860012054443 in 15.296485\n",
      "Epoch 12 Batch 54000 Loss 1.7960360050201416 in 15.248620\n",
      "saving model with loss 1.7960360050201416\n",
      "Epoch 12 Batch 54500 Loss 1.9012610912322998 in 15.289685\n",
      "Epoch 12 Batch 55000 Loss 1.8253567218780518 in 15.218304\n",
      "saving model with loss 1.8253567218780518\n",
      "Epoch 12 Batch 55500 Loss 1.720492959022522 in 15.285280\n",
      "Epoch 12 Batch 56000 Loss 1.7480852603912354 in 15.213465\n",
      "saving model with loss 1.7480852603912354\n",
      "Epoch 12 Batch 56500 Loss 2.044309139251709 in 15.277607\n",
      "Epoch 12 Batch 57000 Loss 1.9501527547836304 in 15.249996\n",
      "saving model with loss 1.9501527547836304\n",
      "Epoch 12 Batch 57500 Loss 1.902902364730835 in 15.308692\n",
      "Epoch 12 Batch 58000 Loss 1.8515316247940063 in 15.218003\n",
      "saving model with loss 1.8515316247940063\n",
      "Epoch 12 Batch 58500 Loss 1.9217983484268188 in 15.285035\n",
      "Epoch 12 Batch 59000 Loss 1.8313430547714233 in 15.232834\n",
      "saving model with loss 1.8313430547714233\n",
      "Epoch 12 Batch 59500 Loss 1.9912335872650146 in 15.282713\n",
      "Epoch 12 Batch 60000 Loss 1.845963716506958 in 15.226945\n",
      "saving model with loss 1.845963716506958\n",
      "Epoch 12 Batch 60500 Loss 1.9258997440338135 in 15.281831\n",
      "Epoch 12 Batch 61000 Loss 1.6781723499298096 in 15.213945\n",
      "saving model with loss 1.6781723499298096\n",
      "Epoch 12 Batch 61500 Loss 1.9058020114898682 in 15.293940\n",
      "Epoch 12 Batch 62000 Loss 1.850205659866333 in 15.231319\n",
      "saving model with loss 1.850205659866333\n",
      "Epoch 12 Batch 62500 Loss 1.844458818435669 in 15.285794\n",
      "Epoch 12 Batch 63000 Loss 1.7880017757415771 in 15.220811\n",
      "saving model with loss 1.7880017757415771\n",
      "Epoch 12 Batch 63500 Loss 1.8612205982208252 in 15.271658\n",
      "Epoch 12 Batch 64000 Loss 1.9364116191864014 in 15.220908\n",
      "saving model with loss 1.9364116191864014\n",
      "Epoch 12 Batch 64500 Loss 1.931362509727478 in 15.294570\n",
      "Epoch 12 Batch 65000 Loss 1.898838758468628 in 15.223866\n",
      "saving model with loss 1.898838758468628\n",
      "Epoch 12 Batch 65500 Loss 1.7560371160507202 in 15.293997\n",
      "Epoch 12 Batch 66000 Loss 1.8603570461273193 in 15.224139\n",
      "saving model with loss 1.8603570461273193\n",
      "Epoch 12 Batch 66500 Loss 1.9064838886260986 in 15.292212\n",
      "Epoch 12 Batch 67000 Loss 1.933545708656311 in 15.220308\n",
      "saving model with loss 1.933545708656311\n",
      "Epoch 12 Batch 67500 Loss 1.83597731590271 in 15.301337\n",
      "Epoch 12 Batch 68000 Loss 1.8110942840576172 in 15.218058\n",
      "saving model with loss 1.8110942840576172\n",
      "Epoch 12 Batch 68500 Loss 1.9012397527694702 in 15.283539\n",
      "Epoch 12 Batch 69000 Loss 2.0078461170196533 in 15.252045\n",
      "saving model with loss 2.0078461170196533\n",
      "Epoch 12 Batch 69500 Loss 2.0126776695251465 in 15.250201\n",
      "Epoch 12 Batch 70000 Loss 1.926187515258789 in 15.177801\n",
      "saving model with loss 1.926187515258789\n",
      "Epoch 12 Batch 70500 Loss 1.9648189544677734 in 15.252877\n",
      "Epoch 13 Loss 1.9586721658706665\n",
      "Time taken for 1 epoch 2153.3099315166473\n",
      "number of batches : 70553\n",
      "Epoch 13 Batch 0 Loss 1.769949197769165 in 1.310378\n",
      "saving model with loss 1.769949197769165\n",
      "Epoch 13 Batch 500 Loss 1.897730827331543 in 15.246140\n",
      "Epoch 13 Batch 1000 Loss 1.8904783725738525 in 15.179970\n",
      "saving model with loss 1.8904783725738525\n",
      "Epoch 13 Batch 1500 Loss 1.8501899242401123 in 15.245909\n",
      "Epoch 13 Batch 2000 Loss 1.8438701629638672 in 15.178632\n",
      "saving model with loss 1.8438701629638672\n",
      "Epoch 13 Batch 2500 Loss 1.9379148483276367 in 15.253666\n",
      "Epoch 13 Batch 3000 Loss 2.0598013401031494 in 15.178468\n",
      "saving model with loss 2.0598013401031494\n",
      "Epoch 13 Batch 3500 Loss 2.037097454071045 in 15.239360\n",
      "Epoch 13 Batch 4000 Loss 1.8161122798919678 in 15.171088\n",
      "saving model with loss 1.8161122798919678\n",
      "Epoch 13 Batch 4500 Loss 1.9095451831817627 in 15.239396\n",
      "Epoch 13 Batch 5000 Loss 1.8036682605743408 in 15.156152\n",
      "saving model with loss 1.8036682605743408\n",
      "Epoch 13 Batch 5500 Loss 1.887434959411621 in 15.243990\n",
      "Epoch 13 Batch 6000 Loss 1.8628761768341064 in 15.151339\n",
      "saving model with loss 1.8628761768341064\n",
      "Epoch 13 Batch 6500 Loss 1.8819143772125244 in 15.212139\n",
      "Epoch 13 Batch 7000 Loss 1.9639842510223389 in 15.163364\n",
      "saving model with loss 1.9639842510223389\n",
      "Epoch 13 Batch 7500 Loss 1.9410892724990845 in 15.255704\n",
      "Epoch 13 Batch 8000 Loss 1.7513694763183594 in 15.557431\n",
      "saving model with loss 1.7513694763183594\n",
      "Epoch 13 Batch 8500 Loss 1.9396820068359375 in 15.297362\n",
      "Epoch 13 Batch 9000 Loss 1.9278675317764282 in 15.208313\n",
      "saving model with loss 1.9278675317764282\n",
      "Epoch 13 Batch 9500 Loss 1.9125988483428955 in 15.280737\n",
      "Epoch 13 Batch 10000 Loss 1.9054925441741943 in 15.318058\n",
      "saving model with loss 1.9054925441741943\n",
      "Epoch 13 Batch 10500 Loss 1.945455551147461 in 15.405756\n",
      "Epoch 13 Batch 11000 Loss 1.9539648294448853 in 15.214204\n",
      "saving model with loss 1.9539648294448853\n",
      "Epoch 13 Batch 11500 Loss 2.0423948764801025 in 15.275621\n",
      "Epoch 13 Batch 12000 Loss 1.8825805187225342 in 15.219277\n",
      "saving model with loss 1.8825805187225342\n",
      "Epoch 13 Batch 12500 Loss 1.8194305896759033 in 15.290751\n",
      "Epoch 13 Batch 13000 Loss 1.857377290725708 in 15.201808\n",
      "saving model with loss 1.857377290725708\n",
      "Epoch 13 Batch 13500 Loss 1.9621593952178955 in 15.281440\n",
      "Epoch 13 Batch 14000 Loss 1.8849351406097412 in 15.208959\n",
      "saving model with loss 1.8849351406097412\n",
      "Epoch 13 Batch 14500 Loss 1.8922609090805054 in 15.253884\n",
      "Epoch 13 Batch 15000 Loss 1.9244587421417236 in 15.226792\n",
      "saving model with loss 1.9244587421417236\n",
      "Epoch 13 Batch 15500 Loss 1.9056885242462158 in 15.285327\n",
      "Epoch 13 Batch 16000 Loss 2.0369486808776855 in 15.219414\n",
      "saving model with loss 2.0369486808776855\n",
      "Epoch 13 Batch 16500 Loss 1.814918875694275 in 15.300562\n",
      "Epoch 13 Batch 17000 Loss 1.9308700561523438 in 15.212472\n",
      "saving model with loss 1.9308700561523438\n",
      "Epoch 13 Batch 17500 Loss 1.915243148803711 in 15.300293\n",
      "Epoch 13 Batch 18000 Loss 1.8962113857269287 in 15.229883\n",
      "saving model with loss 1.8962113857269287\n",
      "Epoch 13 Batch 18500 Loss 1.7354100942611694 in 15.301966\n",
      "Epoch 13 Batch 19000 Loss 1.9876981973648071 in 15.215336\n",
      "saving model with loss 1.9876981973648071\n",
      "Epoch 13 Batch 19500 Loss 1.8792387247085571 in 15.280837\n",
      "Epoch 13 Batch 20000 Loss 1.9510990381240845 in 15.250815\n",
      "saving model with loss 1.9510990381240845\n",
      "Epoch 13 Batch 20500 Loss 1.9362386465072632 in 15.285013\n",
      "Epoch 13 Batch 21000 Loss 1.9100767374038696 in 15.231943\n",
      "saving model with loss 1.9100767374038696\n",
      "Epoch 13 Batch 21500 Loss 1.8515655994415283 in 15.281314\n",
      "Epoch 13 Batch 22000 Loss 1.8191293478012085 in 15.228899\n",
      "saving model with loss 1.8191293478012085\n",
      "Epoch 13 Batch 22500 Loss 1.870517373085022 in 15.294405\n",
      "Epoch 13 Batch 23000 Loss 1.8598912954330444 in 15.210143\n",
      "saving model with loss 1.8598912954330444\n",
      "Epoch 13 Batch 23500 Loss 1.8836923837661743 in 15.285887\n",
      "Epoch 13 Batch 24000 Loss 1.9786456823349 in 15.218158\n",
      "saving model with loss 1.9786456823349\n",
      "Epoch 13 Batch 24500 Loss 1.8679535388946533 in 15.302136\n",
      "Epoch 13 Batch 25000 Loss 1.9004125595092773 in 15.264881\n",
      "saving model with loss 1.9004125595092773\n",
      "Epoch 13 Batch 25500 Loss 1.8808867931365967 in 15.308730\n",
      "Epoch 13 Batch 26000 Loss 1.786913275718689 in 15.226055\n",
      "saving model with loss 1.786913275718689\n",
      "Epoch 13 Batch 26500 Loss 1.9569355249404907 in 15.289753\n",
      "Epoch 13 Batch 27000 Loss 1.8872381448745728 in 15.226556\n",
      "saving model with loss 1.8872381448745728\n",
      "Epoch 13 Batch 27500 Loss 1.9496692419052124 in 15.294288\n",
      "Epoch 13 Batch 28000 Loss 1.9684922695159912 in 15.215003\n",
      "saving model with loss 1.9684922695159912\n",
      "Epoch 13 Batch 28500 Loss 1.9808084964752197 in 15.319338\n",
      "Epoch 13 Batch 29000 Loss 1.976880669593811 in 15.218193\n",
      "saving model with loss 1.976880669593811\n",
      "Epoch 13 Batch 29500 Loss 1.9780696630477905 in 15.280617\n",
      "Epoch 13 Batch 30000 Loss 1.9050174951553345 in 15.228902\n",
      "saving model with loss 1.9050174951553345\n",
      "Epoch 13 Batch 30500 Loss 1.8657872676849365 in 15.302051\n",
      "Epoch 13 Batch 31000 Loss 1.8558269739151 in 15.210985\n",
      "saving model with loss 1.8558269739151\n",
      "Epoch 13 Batch 31500 Loss 1.9143526554107666 in 15.288926\n",
      "Epoch 13 Batch 32000 Loss 1.9617878198623657 in 15.213967\n",
      "saving model with loss 1.9617878198623657\n",
      "Epoch 13 Batch 32500 Loss 1.9426476955413818 in 15.292641\n",
      "Epoch 13 Batch 33000 Loss 1.8471457958221436 in 15.230453\n",
      "saving model with loss 1.8471457958221436\n",
      "Epoch 13 Batch 33500 Loss 1.87583327293396 in 15.285520\n",
      "Epoch 13 Batch 34000 Loss 1.912923812866211 in 15.220913\n",
      "saving model with loss 1.912923812866211\n",
      "Epoch 13 Batch 34500 Loss 1.8130706548690796 in 15.301338\n",
      "Epoch 13 Batch 35000 Loss 1.7995061874389648 in 15.223250\n",
      "saving model with loss 1.7995061874389648\n",
      "Epoch 13 Batch 35500 Loss 1.8343994617462158 in 15.295987\n",
      "Epoch 13 Batch 36000 Loss 1.8924453258514404 in 15.212546\n",
      "saving model with loss 1.8924453258514404\n",
      "Epoch 13 Batch 36500 Loss 1.8833439350128174 in 15.285802\n",
      "Epoch 13 Batch 37000 Loss 1.7254383563995361 in 15.224133\n",
      "saving model with loss 1.7254383563995361\n",
      "Epoch 13 Batch 37500 Loss 2.0513463020324707 in 15.276848\n",
      "Epoch 13 Batch 38000 Loss 1.9054721593856812 in 15.210400\n",
      "saving model with loss 1.9054721593856812\n",
      "Epoch 13 Batch 38500 Loss 2.0821197032928467 in 15.299759\n",
      "Epoch 13 Batch 39000 Loss 2.0224459171295166 in 15.214036\n",
      "saving model with loss 2.0224459171295166\n",
      "Epoch 13 Batch 39500 Loss 1.9868762493133545 in 15.283290\n",
      "Epoch 13 Batch 40000 Loss 1.836745023727417 in 15.205492\n",
      "saving model with loss 1.836745023727417\n",
      "Epoch 13 Batch 40500 Loss 1.8830732107162476 in 15.292574\n",
      "Epoch 13 Batch 41000 Loss 1.9895941019058228 in 15.235054\n",
      "saving model with loss 1.9895941019058228\n",
      "Epoch 13 Batch 41500 Loss 1.8686449527740479 in 15.276580\n",
      "Epoch 13 Batch 42000 Loss 1.8904931545257568 in 15.224316\n",
      "saving model with loss 1.8904931545257568\n",
      "Epoch 13 Batch 42500 Loss 1.8225657939910889 in 15.284274\n",
      "Epoch 13 Batch 43000 Loss 1.9503780603408813 in 15.228314\n",
      "saving model with loss 1.9503780603408813\n",
      "Epoch 13 Batch 43500 Loss 2.0464932918548584 in 15.277996\n",
      "Epoch 13 Batch 44000 Loss 1.895490288734436 in 15.215323\n",
      "saving model with loss 1.895490288734436\n",
      "Epoch 13 Batch 44500 Loss 1.9625362157821655 in 15.274547\n",
      "Epoch 13 Batch 45000 Loss 2.0068869590759277 in 15.223538\n",
      "saving model with loss 2.0068869590759277\n",
      "Epoch 13 Batch 45500 Loss 1.9656059741973877 in 15.287830\n",
      "Epoch 13 Batch 46000 Loss 1.8650808334350586 in 15.235719\n",
      "saving model with loss 1.8650808334350586\n",
      "Epoch 13 Batch 46500 Loss 1.9133479595184326 in 15.298059\n",
      "Epoch 13 Batch 47000 Loss 1.970454216003418 in 15.217570\n",
      "saving model with loss 1.970454216003418\n",
      "Epoch 13 Batch 47500 Loss 1.7993686199188232 in 15.294070\n",
      "Epoch 13 Batch 48000 Loss 1.9237325191497803 in 15.232287\n",
      "saving model with loss 1.9237325191497803\n",
      "Epoch 13 Batch 48500 Loss 1.9822883605957031 in 15.292904\n",
      "Epoch 13 Batch 49000 Loss 1.9213082790374756 in 15.209043\n",
      "saving model with loss 1.9213082790374756\n",
      "Epoch 13 Batch 49500 Loss 1.8969780206680298 in 15.285537\n",
      "Epoch 13 Batch 50000 Loss 1.8145157098770142 in 15.222381\n",
      "saving model with loss 1.8145157098770142\n",
      "Epoch 13 Batch 50500 Loss 1.84878671169281 in 15.291554\n",
      "Epoch 13 Batch 51000 Loss 1.906881332397461 in 15.243302\n",
      "saving model with loss 1.906881332397461\n",
      "Epoch 13 Batch 51500 Loss 1.9591047763824463 in 15.294959\n",
      "Epoch 13 Batch 52000 Loss 1.9353011846542358 in 15.226284\n",
      "saving model with loss 1.9353011846542358\n",
      "Epoch 13 Batch 52500 Loss 1.8342561721801758 in 15.281164\n",
      "Epoch 13 Batch 53000 Loss 1.844698190689087 in 15.207206\n",
      "saving model with loss 1.844698190689087\n",
      "Epoch 13 Batch 53500 Loss 1.88772714138031 in 15.281512\n",
      "Epoch 13 Batch 54000 Loss 1.9978233575820923 in 15.222352\n",
      "saving model with loss 1.9978233575820923\n",
      "Epoch 13 Batch 54500 Loss 1.8771593570709229 in 15.292283\n",
      "Epoch 13 Batch 55000 Loss 1.9340521097183228 in 15.222552\n",
      "saving model with loss 1.9340521097183228\n",
      "Epoch 13 Batch 55500 Loss 1.7090909481048584 in 15.294611\n",
      "Epoch 13 Batch 56000 Loss 2.1861627101898193 in 15.211550\n",
      "saving model with loss 2.1861627101898193\n",
      "Epoch 13 Batch 56500 Loss 2.012212038040161 in 15.293122\n",
      "Epoch 13 Batch 57000 Loss 1.9036197662353516 in 15.215064\n",
      "saving model with loss 1.9036197662353516\n",
      "Epoch 13 Batch 57500 Loss 1.8755466938018799 in 15.284363\n",
      "Epoch 13 Batch 58000 Loss 1.8473527431488037 in 15.215618\n",
      "saving model with loss 1.8473527431488037\n",
      "Epoch 13 Batch 58500 Loss 1.965611219406128 in 15.285194\n",
      "Epoch 13 Batch 59000 Loss 1.8506414890289307 in 15.214360\n",
      "saving model with loss 1.8506414890289307\n",
      "Epoch 13 Batch 59500 Loss 1.9881751537322998 in 15.293977\n",
      "Epoch 13 Batch 60000 Loss 1.9659208059310913 in 15.234861\n",
      "saving model with loss 1.9659208059310913\n",
      "Epoch 13 Batch 60500 Loss 1.9348081350326538 in 15.288573\n",
      "Epoch 13 Batch 61000 Loss 1.9231469631195068 in 15.205426\n",
      "saving model with loss 1.9231469631195068\n",
      "Epoch 13 Batch 61500 Loss 1.988828420639038 in 15.269297\n",
      "Epoch 13 Batch 62000 Loss 1.9224885702133179 in 15.266296\n",
      "saving model with loss 1.9224885702133179\n",
      "Epoch 13 Batch 62500 Loss 1.778937578201294 in 15.292829\n",
      "Epoch 13 Batch 63000 Loss 1.8777490854263306 in 15.213254\n",
      "saving model with loss 1.8777490854263306\n",
      "Epoch 13 Batch 63500 Loss 1.9598878622055054 in 15.296070\n",
      "Epoch 13 Batch 64000 Loss 1.912988305091858 in 15.216720\n",
      "saving model with loss 1.912988305091858\n",
      "Epoch 13 Batch 64500 Loss 1.9050337076187134 in 15.281033\n",
      "Epoch 13 Batch 65000 Loss 1.8588157892227173 in 15.220335\n",
      "saving model with loss 1.8588157892227173\n",
      "Epoch 13 Batch 65500 Loss 1.9296283721923828 in 15.275618\n",
      "Epoch 13 Batch 66000 Loss 1.9633302688598633 in 15.232099\n",
      "saving model with loss 1.9633302688598633\n",
      "Epoch 13 Batch 66500 Loss 1.8848375082015991 in 15.283749\n",
      "Epoch 13 Batch 67000 Loss 1.846213698387146 in 15.229661\n",
      "saving model with loss 1.846213698387146\n",
      "Epoch 13 Batch 67500 Loss 1.932451605796814 in 15.292382\n",
      "Epoch 13 Batch 68000 Loss 1.8276888132095337 in 15.245282\n",
      "saving model with loss 1.8276888132095337\n",
      "Epoch 13 Batch 68500 Loss 2.0545663833618164 in 15.280320\n",
      "Epoch 13 Batch 69000 Loss 1.8626924753189087 in 15.211875\n",
      "saving model with loss 1.8626924753189087\n",
      "Epoch 13 Batch 69500 Loss 1.9334022998809814 in 15.292346\n",
      "Epoch 13 Batch 70000 Loss 1.8475395441055298 in 15.227572\n",
      "saving model with loss 1.8475395441055298\n",
      "Epoch 13 Batch 70500 Loss 1.808882474899292 in 15.299870\n",
      "Epoch 14 Loss 1.9124715328216553\n",
      "Time taken for 1 epoch 2153.8276715278625\n",
      "number of batches : 70553\n",
      "Epoch 14 Batch 0 Loss 1.7605390548706055 in 1.178998\n",
      "saving model with loss 1.7605390548706055\n",
      "Epoch 14 Batch 500 Loss 1.8445262908935547 in 15.292769\n",
      "Epoch 14 Batch 1000 Loss 1.8984663486480713 in 15.212063\n",
      "saving model with loss 1.8984663486480713\n",
      "Epoch 14 Batch 1500 Loss 1.9783289432525635 in 15.301104\n",
      "Epoch 14 Batch 2000 Loss 1.8822076320648193 in 15.210948\n",
      "saving model with loss 1.8822076320648193\n",
      "Epoch 14 Batch 2500 Loss 1.9624913930892944 in 15.277248\n",
      "Epoch 14 Batch 3000 Loss 1.9748783111572266 in 15.206289\n",
      "saving model with loss 1.9748783111572266\n",
      "Epoch 14 Batch 3500 Loss 1.9699351787567139 in 15.282839\n",
      "Epoch 14 Batch 4000 Loss 1.9285587072372437 in 15.209611\n",
      "saving model with loss 1.9285587072372437\n",
      "Epoch 14 Batch 4500 Loss 1.844537377357483 in 15.281198\n",
      "Epoch 14 Batch 5000 Loss 1.8062645196914673 in 15.226500\n",
      "saving model with loss 1.8062645196914673\n",
      "Epoch 14 Batch 5500 Loss 1.702742576599121 in 15.300971\n",
      "Epoch 14 Batch 6000 Loss 1.8824093341827393 in 15.221475\n",
      "saving model with loss 1.8824093341827393\n",
      "Epoch 14 Batch 6500 Loss 1.885026216506958 in 15.300203\n",
      "Epoch 14 Batch 7000 Loss 1.871853232383728 in 15.211594\n",
      "saving model with loss 1.871853232383728\n",
      "Epoch 14 Batch 7500 Loss 1.8680915832519531 in 15.308630\n",
      "Epoch 14 Batch 8000 Loss 1.9306020736694336 in 15.221557\n",
      "saving model with loss 1.9306020736694336\n",
      "Epoch 14 Batch 8500 Loss 1.944340467453003 in 15.268343\n",
      "Epoch 14 Batch 9000 Loss 1.907003402709961 in 15.205013\n",
      "saving model with loss 1.907003402709961\n",
      "Epoch 14 Batch 9500 Loss 1.9152218103408813 in 15.300503\n",
      "Epoch 14 Batch 10000 Loss 1.9119112491607666 in 15.222558\n",
      "saving model with loss 1.9119112491607666\n",
      "Epoch 14 Batch 10500 Loss 1.9025249481201172 in 15.289511\n",
      "Epoch 14 Batch 11000 Loss 1.9938573837280273 in 15.224130\n",
      "saving model with loss 1.9938573837280273\n",
      "Epoch 14 Batch 11500 Loss 1.9243669509887695 in 15.264999\n",
      "Epoch 14 Batch 12000 Loss 1.96564519405365 in 15.228404\n",
      "saving model with loss 1.96564519405365\n",
      "Epoch 14 Batch 12500 Loss 1.9218699932098389 in 15.268615\n",
      "Epoch 14 Batch 13000 Loss 1.8221746683120728 in 15.203643\n",
      "saving model with loss 1.8221746683120728\n",
      "Epoch 14 Batch 13500 Loss 1.9599641561508179 in 15.293124\n",
      "Epoch 14 Batch 14000 Loss 1.954766869544983 in 15.210759\n",
      "saving model with loss 1.954766869544983\n",
      "Epoch 14 Batch 14500 Loss 1.8491630554199219 in 15.285317\n",
      "Epoch 14 Batch 15000 Loss 1.8589956760406494 in 15.224531\n",
      "saving model with loss 1.8589956760406494\n",
      "Epoch 14 Batch 15500 Loss 1.8556104898452759 in 15.305757\n",
      "Epoch 14 Batch 16000 Loss 1.870357871055603 in 15.219557\n",
      "saving model with loss 1.870357871055603\n",
      "Epoch 14 Batch 16500 Loss 2.0211803913116455 in 15.281658\n",
      "Epoch 14 Batch 17000 Loss 1.9529997110366821 in 15.211680\n",
      "saving model with loss 1.9529997110366821\n",
      "Epoch 14 Batch 17500 Loss 1.8916285037994385 in 15.297807\n",
      "Epoch 14 Batch 18000 Loss 1.935097098350525 in 15.212570\n",
      "saving model with loss 1.935097098350525\n",
      "Epoch 14 Batch 18500 Loss 1.9999405145645142 in 15.314948\n",
      "Epoch 14 Batch 19000 Loss 1.835108995437622 in 15.236444\n",
      "saving model with loss 1.835108995437622\n",
      "Epoch 14 Batch 19500 Loss 1.8077071905136108 in 15.297403\n",
      "Epoch 14 Batch 20000 Loss 1.900103211402893 in 15.218849\n",
      "saving model with loss 1.900103211402893\n",
      "Epoch 14 Batch 20500 Loss 1.9895966053009033 in 15.280283\n",
      "Epoch 14 Batch 21000 Loss 1.9670900106430054 in 15.230831\n",
      "saving model with loss 1.9670900106430054\n",
      "Epoch 14 Batch 21500 Loss 1.9141842126846313 in 15.294765\n",
      "Epoch 14 Batch 22000 Loss 1.7899205684661865 in 15.242295\n",
      "saving model with loss 1.7899205684661865\n",
      "Epoch 14 Batch 22500 Loss 1.8808876276016235 in 15.305209\n",
      "Epoch 14 Batch 23000 Loss 1.897547960281372 in 15.211137\n",
      "saving model with loss 1.897547960281372\n",
      "Epoch 14 Batch 23500 Loss 2.041527271270752 in 15.277535\n",
      "Epoch 14 Batch 24000 Loss 1.9161049127578735 in 15.212987\n",
      "saving model with loss 1.9161049127578735\n",
      "Epoch 14 Batch 24500 Loss 2.0688748359680176 in 15.290220\n",
      "Epoch 14 Batch 25000 Loss 1.9419746398925781 in 15.231348\n",
      "saving model with loss 1.9419746398925781\n",
      "Epoch 14 Batch 25500 Loss 2.02950382232666 in 15.281437\n",
      "Epoch 14 Batch 26000 Loss 1.8629783391952515 in 15.233902\n",
      "saving model with loss 1.8629783391952515\n",
      "Epoch 14 Batch 26500 Loss 1.9460203647613525 in 15.293465\n",
      "Epoch 14 Batch 27000 Loss 1.8154900074005127 in 15.269982\n",
      "saving model with loss 1.8154900074005127\n",
      "Epoch 14 Batch 27500 Loss 1.7908525466918945 in 15.298796\n",
      "Epoch 14 Batch 28000 Loss 1.9183557033538818 in 15.265162\n",
      "saving model with loss 1.9183557033538818\n",
      "Epoch 14 Batch 28500 Loss 1.9901790618896484 in 15.293997\n",
      "Epoch 14 Batch 29000 Loss 1.8837225437164307 in 15.236816\n",
      "saving model with loss 1.8837225437164307\n",
      "Epoch 14 Batch 29500 Loss 1.9894473552703857 in 15.312510\n",
      "Epoch 14 Batch 30000 Loss 1.9151328802108765 in 15.257582\n",
      "saving model with loss 1.9151328802108765\n",
      "Epoch 14 Batch 30500 Loss 1.9389508962631226 in 15.284771\n",
      "Epoch 14 Batch 31000 Loss 1.9462299346923828 in 15.221995\n",
      "saving model with loss 1.9462299346923828\n",
      "Epoch 14 Batch 31500 Loss 1.9996464252471924 in 15.294110\n",
      "Epoch 14 Batch 32000 Loss 2.0047459602355957 in 15.229479\n",
      "saving model with loss 2.0047459602355957\n",
      "Epoch 14 Batch 32500 Loss 1.9549424648284912 in 15.284594\n",
      "Epoch 14 Batch 33000 Loss 1.8982683420181274 in 15.221444\n",
      "saving model with loss 1.8982683420181274\n",
      "Epoch 14 Batch 33500 Loss 2.099794626235962 in 15.291917\n",
      "Epoch 14 Batch 34000 Loss 1.9995464086532593 in 15.207548\n",
      "saving model with loss 1.9995464086532593\n",
      "Epoch 14 Batch 34500 Loss 2.0277132987976074 in 15.274335\n",
      "Epoch 14 Batch 35000 Loss 1.9206368923187256 in 15.214374\n",
      "saving model with loss 1.9206368923187256\n",
      "Epoch 14 Batch 35500 Loss 1.9619829654693604 in 15.289396\n",
      "Epoch 14 Batch 36000 Loss 2.023151397705078 in 15.239872\n",
      "saving model with loss 2.023151397705078\n",
      "Epoch 14 Batch 36500 Loss 1.9047282934188843 in 15.289146\n",
      "Epoch 14 Batch 37000 Loss 1.9972953796386719 in 15.233649\n",
      "saving model with loss 1.9972953796386719\n",
      "Epoch 14 Batch 37500 Loss 1.996840476989746 in 15.325419\n",
      "Epoch 14 Batch 38000 Loss 1.8111118078231812 in 15.226059\n",
      "saving model with loss 1.8111118078231812\n",
      "Epoch 14 Batch 38500 Loss 1.8413909673690796 in 15.292268\n",
      "Epoch 14 Batch 39000 Loss 1.8543840646743774 in 15.211198\n",
      "saving model with loss 1.8543840646743774\n",
      "Epoch 14 Batch 39500 Loss 2.0190834999084473 in 15.294014\n",
      "Epoch 14 Batch 40000 Loss 1.9384161233901978 in 15.229434\n",
      "saving model with loss 1.9384161233901978\n",
      "Epoch 14 Batch 40500 Loss 1.9001495838165283 in 15.296779\n",
      "Epoch 14 Batch 41000 Loss 1.9644416570663452 in 15.207275\n",
      "saving model with loss 1.9644416570663452\n",
      "Epoch 14 Batch 41500 Loss 1.9893487691879272 in 15.296258\n",
      "Epoch 14 Batch 42000 Loss 1.9417051076889038 in 15.210943\n",
      "saving model with loss 1.9417051076889038\n",
      "Epoch 14 Batch 42500 Loss 1.8200273513793945 in 15.283401\n",
      "Epoch 14 Batch 43000 Loss 1.979842185974121 in 15.229452\n",
      "saving model with loss 1.979842185974121\n",
      "Epoch 14 Batch 43500 Loss 1.8518152236938477 in 15.281718\n",
      "Epoch 14 Batch 44000 Loss 1.8735284805297852 in 15.224945\n",
      "saving model with loss 1.8735284805297852\n",
      "Epoch 14 Batch 44500 Loss 2.00152850151062 in 15.275633\n",
      "Epoch 14 Batch 45000 Loss 1.7670961618423462 in 15.212404\n",
      "saving model with loss 1.7670961618423462\n",
      "Epoch 14 Batch 45500 Loss 1.9364280700683594 in 15.303595\n",
      "Epoch 14 Batch 46000 Loss 1.8906558752059937 in 15.225024\n",
      "saving model with loss 1.8906558752059937\n",
      "Epoch 14 Batch 46500 Loss 2.078734874725342 in 15.294585\n",
      "Epoch 14 Batch 47000 Loss 1.9767773151397705 in 15.219567\n",
      "saving model with loss 1.9767773151397705\n",
      "Epoch 14 Batch 47500 Loss 1.946096658706665 in 15.291627\n",
      "Epoch 14 Batch 48000 Loss 2.038567543029785 in 15.217093\n",
      "saving model with loss 2.038567543029785\n",
      "Epoch 14 Batch 48500 Loss 1.9991357326507568 in 15.298647\n",
      "Epoch 14 Batch 49000 Loss 1.867139220237732 in 15.214880\n",
      "saving model with loss 1.867139220237732\n",
      "Epoch 14 Batch 49500 Loss 1.920568823814392 in 15.286608\n",
      "Epoch 14 Batch 50000 Loss 2.0038599967956543 in 15.203487\n",
      "saving model with loss 2.0038599967956543\n",
      "Epoch 14 Batch 50500 Loss 2.0054659843444824 in 15.290185\n",
      "Epoch 14 Batch 51000 Loss 1.8830373287200928 in 15.221390\n",
      "saving model with loss 1.8830373287200928\n",
      "Epoch 14 Batch 51500 Loss 1.9148906469345093 in 15.287727\n",
      "Epoch 14 Batch 52000 Loss 1.904842734336853 in 15.215529\n",
      "saving model with loss 1.904842734336853\n",
      "Epoch 14 Batch 52500 Loss 1.811362624168396 in 15.284591\n",
      "Epoch 14 Batch 53000 Loss 2.073115825653076 in 15.222869\n",
      "saving model with loss 2.073115825653076\n",
      "Epoch 14 Batch 53500 Loss 1.8536407947540283 in 15.289967\n",
      "Epoch 14 Batch 54000 Loss 1.9036400318145752 in 15.231766\n",
      "saving model with loss 1.9036400318145752\n",
      "Epoch 14 Batch 54500 Loss 1.918210744857788 in 15.295484\n",
      "Epoch 14 Batch 55000 Loss 2.06923246383667 in 15.212250\n",
      "saving model with loss 2.06923246383667\n",
      "Epoch 14 Batch 55500 Loss 1.8808300495147705 in 15.296818\n",
      "Epoch 14 Batch 56000 Loss 1.8843450546264648 in 15.217619\n",
      "saving model with loss 1.8843450546264648\n",
      "Epoch 14 Batch 56500 Loss 2.001279830932617 in 15.289541\n",
      "Epoch 14 Batch 57000 Loss 1.8792378902435303 in 15.235505\n",
      "saving model with loss 1.8792378902435303\n",
      "Epoch 14 Batch 57500 Loss 1.8603589534759521 in 15.497415\n",
      "Epoch 14 Batch 58000 Loss 1.9691002368927002 in 15.305083\n",
      "saving model with loss 1.9691002368927002\n",
      "Epoch 14 Batch 58500 Loss 1.9584343433380127 in 15.237165\n",
      "Epoch 14 Batch 59000 Loss 1.9854217767715454 in 15.183366\n",
      "saving model with loss 1.9854217767715454\n",
      "Epoch 14 Batch 59500 Loss 1.9395523071289062 in 15.246507\n",
      "Epoch 14 Batch 60000 Loss 2.007416248321533 in 15.194133\n",
      "saving model with loss 2.007416248321533\n",
      "Epoch 14 Batch 60500 Loss 1.8737252950668335 in 15.265806\n",
      "Epoch 14 Batch 61000 Loss 1.8404839038848877 in 15.184045\n",
      "saving model with loss 1.8404839038848877\n",
      "Epoch 14 Batch 61500 Loss 1.968585729598999 in 15.662688\n",
      "Epoch 14 Batch 62000 Loss 1.8374614715576172 in 15.222715\n",
      "saving model with loss 1.8374614715576172\n",
      "Epoch 14 Batch 62500 Loss 2.1406068801879883 in 15.290381\n",
      "Epoch 14 Batch 63000 Loss 1.9513301849365234 in 15.227607\n",
      "saving model with loss 1.9513301849365234\n",
      "Epoch 14 Batch 63500 Loss 1.9115724563598633 in 15.290338\n",
      "Epoch 14 Batch 64000 Loss 1.7703641653060913 in 15.215662\n",
      "saving model with loss 1.7703641653060913\n",
      "Epoch 14 Batch 64500 Loss 1.9249382019042969 in 15.291208\n",
      "Epoch 14 Batch 65000 Loss 1.944331169128418 in 15.230063\n",
      "saving model with loss 1.944331169128418\n",
      "Epoch 14 Batch 65500 Loss 1.9284451007843018 in 15.284104\n",
      "Epoch 14 Batch 66000 Loss 1.9494937658309937 in 15.226481\n",
      "saving model with loss 1.9494937658309937\n",
      "Epoch 14 Batch 66500 Loss 1.8547569513320923 in 15.299039\n",
      "Epoch 14 Batch 67000 Loss 1.9222370386123657 in 15.223398\n",
      "saving model with loss 1.9222370386123657\n",
      "Epoch 14 Batch 67500 Loss 1.9455190896987915 in 15.284670\n",
      "Epoch 14 Batch 68000 Loss 1.9526004791259766 in 15.245208\n",
      "saving model with loss 1.9526004791259766\n",
      "Epoch 14 Batch 68500 Loss 2.006938934326172 in 15.309065\n",
      "Epoch 14 Batch 69000 Loss 2.051053524017334 in 15.228106\n",
      "saving model with loss 2.051053524017334\n",
      "Epoch 14 Batch 69500 Loss 1.761871576309204 in 15.283728\n",
      "Epoch 14 Batch 70000 Loss 1.9678198099136353 in 15.235230\n",
      "saving model with loss 1.9678198099136353\n",
      "Epoch 14 Batch 70500 Loss 2.0167386531829834 in 15.299297\n",
      "Epoch 15 Loss 2.083559513092041\n",
      "Time taken for 1 epoch 2154.564629793167\n",
      "number of batches : 70553\n",
      "Epoch 15 Batch 0 Loss 1.8289276361465454 in 1.174920\n",
      "saving model with loss 1.8289276361465454\n",
      "Epoch 15 Batch 500 Loss 1.996091604232788 in 15.292458\n",
      "Epoch 15 Batch 1000 Loss 1.8793087005615234 in 15.209174\n",
      "saving model with loss 1.8793087005615234\n",
      "Epoch 15 Batch 1500 Loss 1.8193708658218384 in 15.281168\n",
      "Epoch 15 Batch 2000 Loss 1.9959819316864014 in 15.223742\n",
      "saving model with loss 1.9959819316864014\n",
      "Epoch 15 Batch 2500 Loss 1.9566943645477295 in 15.290572\n",
      "Epoch 15 Batch 3000 Loss 1.9098331928253174 in 15.216591\n",
      "saving model with loss 1.9098331928253174\n",
      "Epoch 15 Batch 3500 Loss 1.9139829874038696 in 15.292380\n",
      "Epoch 15 Batch 4000 Loss 1.8746343851089478 in 15.251802\n",
      "saving model with loss 1.8746343851089478\n",
      "Epoch 15 Batch 4500 Loss 1.7912613153457642 in 15.289110\n",
      "Epoch 15 Batch 5000 Loss 1.9423519372940063 in 15.210406\n",
      "saving model with loss 1.9423519372940063\n",
      "Epoch 15 Batch 5500 Loss 1.8632138967514038 in 15.304691\n",
      "Epoch 15 Batch 6000 Loss 1.9252424240112305 in 15.212841\n",
      "saving model with loss 1.9252424240112305\n",
      "Epoch 15 Batch 6500 Loss 1.936131238937378 in 15.301360\n",
      "Epoch 15 Batch 7000 Loss 1.9275861978530884 in 15.242310\n",
      "saving model with loss 1.9275861978530884\n",
      "Epoch 15 Batch 7500 Loss 2.0064101219177246 in 15.299587\n",
      "Epoch 15 Batch 8000 Loss 1.8788020610809326 in 15.232004\n",
      "saving model with loss 1.8788020610809326\n",
      "Epoch 15 Batch 8500 Loss 1.877332091331482 in 15.311078\n",
      "Epoch 15 Batch 9000 Loss 2.0767478942871094 in 15.230753\n",
      "saving model with loss 2.0767478942871094\n",
      "Epoch 15 Batch 9500 Loss 1.8254873752593994 in 15.280417\n",
      "Epoch 15 Batch 10000 Loss 1.815800428390503 in 15.222251\n",
      "saving model with loss 1.815800428390503\n",
      "Epoch 15 Batch 10500 Loss 1.8053362369537354 in 15.281066\n",
      "Epoch 15 Batch 11000 Loss 2.0288708209991455 in 15.213168\n",
      "saving model with loss 2.0288708209991455\n",
      "Epoch 15 Batch 11500 Loss 1.9348104000091553 in 15.291352\n",
      "Epoch 15 Batch 12000 Loss 1.9257776737213135 in 15.229958\n",
      "saving model with loss 1.9257776737213135\n",
      "Epoch 15 Batch 12500 Loss 2.1355435848236084 in 15.296795\n",
      "Epoch 15 Batch 13000 Loss 1.883676528930664 in 15.221356\n",
      "saving model with loss 1.883676528930664\n",
      "Epoch 15 Batch 13500 Loss 1.9120943546295166 in 15.296008\n",
      "Epoch 15 Batch 14000 Loss 1.9128706455230713 in 15.223897\n",
      "saving model with loss 1.9128706455230713\n",
      "Epoch 15 Batch 14500 Loss 1.9160900115966797 in 15.294691\n",
      "Epoch 15 Batch 15000 Loss 1.8962719440460205 in 15.207909\n",
      "saving model with loss 1.8962719440460205\n",
      "Epoch 15 Batch 15500 Loss 1.9067074060440063 in 15.293761\n",
      "Epoch 15 Batch 16000 Loss 1.9809125661849976 in 15.217935\n",
      "saving model with loss 1.9809125661849976\n",
      "Epoch 15 Batch 16500 Loss 1.9946836233139038 in 15.337138\n",
      "Epoch 15 Batch 17000 Loss 2.0404112339019775 in 15.235219\n",
      "saving model with loss 2.0404112339019775\n",
      "Epoch 15 Batch 17500 Loss 2.0726752281188965 in 15.284805\n",
      "Epoch 15 Batch 18000 Loss 2.0112204551696777 in 15.226580\n",
      "saving model with loss 2.0112204551696777\n",
      "Epoch 15 Batch 18500 Loss 1.850252389907837 in 15.279947\n",
      "Epoch 15 Batch 19000 Loss 1.8838098049163818 in 15.234018\n",
      "saving model with loss 1.8838098049163818\n",
      "Epoch 15 Batch 19500 Loss 1.8719971179962158 in 15.281353\n",
      "Epoch 15 Batch 20000 Loss 1.9443957805633545 in 15.216574\n",
      "saving model with loss 1.9443957805633545\n",
      "Epoch 15 Batch 20500 Loss 1.8864914178848267 in 15.279637\n",
      "Epoch 15 Batch 21000 Loss 1.9809690713882446 in 15.228104\n",
      "saving model with loss 1.9809690713882446\n",
      "Epoch 15 Batch 21500 Loss 1.899592638015747 in 15.289202\n",
      "Epoch 15 Batch 22000 Loss 1.9227015972137451 in 15.228911\n",
      "saving model with loss 1.9227015972137451\n",
      "Epoch 15 Batch 22500 Loss 1.8164631128311157 in 15.303305\n",
      "Epoch 15 Batch 23000 Loss 2.0299973487854004 in 15.213662\n",
      "saving model with loss 2.0299973487854004\n",
      "Epoch 15 Batch 23500 Loss 1.977243423461914 in 15.298769\n",
      "Epoch 15 Batch 24000 Loss 1.859636664390564 in 15.220512\n",
      "saving model with loss 1.859636664390564\n",
      "Epoch 15 Batch 24500 Loss 1.8370224237442017 in 15.300201\n",
      "Epoch 15 Batch 25000 Loss 1.965675950050354 in 15.229127\n",
      "saving model with loss 1.965675950050354\n",
      "Epoch 15 Batch 25500 Loss 1.9262568950653076 in 15.294627\n",
      "Epoch 15 Batch 26000 Loss 2.0171117782592773 in 15.236397\n",
      "saving model with loss 2.0171117782592773\n",
      "Epoch 15 Batch 26500 Loss 1.9864017963409424 in 15.304702\n",
      "Epoch 15 Batch 27000 Loss 1.9910436868667603 in 15.220006\n",
      "saving model with loss 1.9910436868667603\n",
      "Epoch 15 Batch 27500 Loss 1.8774001598358154 in 15.304393\n",
      "Epoch 15 Batch 28000 Loss 1.9845069646835327 in 15.211987\n",
      "saving model with loss 1.9845069646835327\n",
      "Epoch 15 Batch 28500 Loss 1.9095637798309326 in 15.287831\n",
      "Epoch 15 Batch 29000 Loss 1.8297860622406006 in 15.223177\n",
      "saving model with loss 1.8297860622406006\n",
      "Epoch 15 Batch 29500 Loss 1.8781063556671143 in 15.312073\n",
      "Epoch 15 Batch 30000 Loss 1.9072182178497314 in 15.219329\n",
      "saving model with loss 1.9072182178497314\n",
      "Epoch 15 Batch 30500 Loss 1.8853416442871094 in 15.310453\n",
      "Epoch 15 Batch 31000 Loss 1.7812926769256592 in 15.268372\n",
      "saving model with loss 1.7812926769256592\n",
      "Epoch 15 Batch 31500 Loss 2.006143569946289 in 15.303083\n",
      "Epoch 15 Batch 32000 Loss 2.0848965644836426 in 15.254581\n",
      "saving model with loss 2.0848965644836426\n",
      "Epoch 15 Batch 32500 Loss 1.9123073816299438 in 15.287293\n",
      "Epoch 15 Batch 33000 Loss 2.157142400741577 in 15.232926\n",
      "saving model with loss 2.157142400741577\n",
      "Epoch 15 Batch 33500 Loss 1.8004789352416992 in 15.300360\n",
      "Epoch 15 Batch 34000 Loss 1.9884952306747437 in 15.222445\n",
      "saving model with loss 1.9884952306747437\n",
      "Epoch 15 Batch 34500 Loss 1.969722032546997 in 15.290083\n",
      "Epoch 15 Batch 35000 Loss 1.8403505086898804 in 15.227904\n",
      "saving model with loss 1.8403505086898804\n",
      "Epoch 15 Batch 35500 Loss 1.9239193201065063 in 15.294433\n",
      "Epoch 15 Batch 36000 Loss 1.9819806814193726 in 15.226128\n",
      "saving model with loss 1.9819806814193726\n",
      "Epoch 15 Batch 36500 Loss 2.077239513397217 in 15.299367\n",
      "Epoch 15 Batch 37000 Loss 1.9556297063827515 in 15.236204\n",
      "saving model with loss 1.9556297063827515\n",
      "Epoch 15 Batch 37500 Loss 1.9082521200180054 in 15.290590\n",
      "Epoch 15 Batch 38000 Loss 1.9318807125091553 in 15.226158\n",
      "saving model with loss 1.9318807125091553\n",
      "Epoch 15 Batch 38500 Loss 1.8793838024139404 in 15.268646\n",
      "Epoch 15 Batch 39000 Loss 1.7704585790634155 in 15.223450\n",
      "saving model with loss 1.7704585790634155\n",
      "Epoch 15 Batch 39500 Loss 1.9573736190795898 in 15.283598\n",
      "Epoch 15 Batch 40000 Loss 1.889772653579712 in 15.214246\n",
      "saving model with loss 1.889772653579712\n",
      "Epoch 15 Batch 40500 Loss 1.92942214012146 in 15.287535\n",
      "Epoch 15 Batch 41000 Loss 2.0097544193267822 in 15.211025\n",
      "saving model with loss 2.0097544193267822\n",
      "Epoch 15 Batch 41500 Loss 1.9777660369873047 in 15.275938\n",
      "Epoch 15 Batch 42000 Loss 2.0283682346343994 in 15.235344\n",
      "saving model with loss 2.0283682346343994\n",
      "Epoch 15 Batch 42500 Loss 1.8780386447906494 in 15.346894\n",
      "Epoch 15 Batch 43000 Loss 1.967542290687561 in 15.187942\n",
      "saving model with loss 1.967542290687561\n",
      "Epoch 15 Batch 43500 Loss 1.9746897220611572 in 15.629198\n",
      "Epoch 15 Batch 44000 Loss 2.0134265422821045 in 15.223017\n",
      "saving model with loss 2.0134265422821045\n",
      "Epoch 15 Batch 44500 Loss 2.014540910720825 in 15.270801\n",
      "Epoch 15 Batch 45000 Loss 1.8843427896499634 in 15.222890\n",
      "saving model with loss 1.8843427896499634\n",
      "Epoch 15 Batch 45500 Loss 1.8984928131103516 in 15.301855\n",
      "Epoch 15 Batch 46000 Loss 1.922806978225708 in 15.224823\n",
      "saving model with loss 1.922806978225708\n",
      "Epoch 15 Batch 46500 Loss 1.855269432067871 in 15.304739\n",
      "Epoch 15 Batch 47000 Loss 1.8847999572753906 in 15.226115\n",
      "saving model with loss 1.8847999572753906\n",
      "Epoch 15 Batch 47500 Loss 1.9843047857284546 in 15.309614\n",
      "Epoch 15 Batch 48000 Loss 1.8904329538345337 in 15.219376\n",
      "saving model with loss 1.8904329538345337\n",
      "Epoch 15 Batch 48500 Loss 2.016389846801758 in 15.293727\n",
      "Epoch 15 Batch 49000 Loss 1.8602123260498047 in 15.227005\n",
      "saving model with loss 1.8602123260498047\n",
      "Epoch 15 Batch 49500 Loss 2.0112340450286865 in 15.293026\n",
      "Epoch 15 Batch 50000 Loss 2.0081276893615723 in 15.220297\n",
      "saving model with loss 2.0081276893615723\n",
      "Epoch 15 Batch 50500 Loss 2.029358386993408 in 15.295752\n",
      "Epoch 15 Batch 51000 Loss 1.9549061059951782 in 15.223139\n",
      "saving model with loss 1.9549061059951782\n",
      "Epoch 15 Batch 51500 Loss 1.9029096364974976 in 15.289298\n",
      "Epoch 15 Batch 52000 Loss 1.9745689630508423 in 15.236127\n",
      "saving model with loss 1.9745689630508423\n",
      "Epoch 15 Batch 52500 Loss 1.7901990413665771 in 15.283790\n",
      "Epoch 15 Batch 53000 Loss 2.027848958969116 in 15.229162\n",
      "saving model with loss 2.027848958969116\n",
      "Epoch 15 Batch 53500 Loss 1.8232402801513672 in 15.290574\n",
      "Epoch 15 Batch 54000 Loss 2.0013391971588135 in 15.218930\n",
      "saving model with loss 2.0013391971588135\n",
      "Epoch 15 Batch 54500 Loss 1.9091955423355103 in 15.286924\n",
      "Epoch 15 Batch 55000 Loss 1.9771941900253296 in 15.240956\n",
      "saving model with loss 1.9771941900253296\n",
      "Epoch 15 Batch 55500 Loss 1.951072335243225 in 15.300644\n",
      "Epoch 15 Batch 56000 Loss 1.9216426610946655 in 15.217553\n",
      "saving model with loss 1.9216426610946655\n",
      "Epoch 15 Batch 56500 Loss 1.9321212768554688 in 15.303172\n",
      "Epoch 15 Batch 57000 Loss 2.0006465911865234 in 15.230394\n",
      "saving model with loss 2.0006465911865234\n",
      "Epoch 15 Batch 57500 Loss 1.9677966833114624 in 15.294961\n",
      "Epoch 15 Batch 58000 Loss 1.8713775873184204 in 15.228764\n",
      "saving model with loss 1.8713775873184204\n",
      "Epoch 15 Batch 58500 Loss 2.012373924255371 in 15.288391\n",
      "Epoch 15 Batch 59000 Loss 2.2051520347595215 in 15.225212\n",
      "saving model with loss 2.2051520347595215\n",
      "Epoch 15 Batch 59500 Loss 1.8083980083465576 in 15.291315\n",
      "Epoch 15 Batch 60000 Loss 1.8847434520721436 in 15.226464\n",
      "saving model with loss 1.8847434520721436\n",
      "Epoch 15 Batch 60500 Loss 2.058448314666748 in 15.336304\n",
      "Epoch 15 Batch 61000 Loss 1.9402443170547485 in 15.223072\n",
      "saving model with loss 1.9402443170547485\n",
      "Epoch 15 Batch 61500 Loss 1.9738826751708984 in 15.292937\n",
      "Epoch 15 Batch 62000 Loss 1.8741662502288818 in 15.221226\n",
      "saving model with loss 1.8741662502288818\n",
      "Epoch 15 Batch 62500 Loss 1.985817313194275 in 15.271959\n",
      "Epoch 15 Batch 63000 Loss 1.9817250967025757 in 15.218531\n",
      "saving model with loss 1.9817250967025757\n",
      "Epoch 15 Batch 63500 Loss 1.9214445352554321 in 15.296185\n",
      "Epoch 15 Batch 64000 Loss 1.9582650661468506 in 15.251812\n",
      "saving model with loss 1.9582650661468506\n",
      "Epoch 15 Batch 64500 Loss 2.071399211883545 in 15.272521\n",
      "Epoch 15 Batch 65000 Loss 1.8665311336517334 in 15.213405\n",
      "saving model with loss 1.8665311336517334\n",
      "Epoch 15 Batch 65500 Loss 1.8414640426635742 in 15.289191\n",
      "Epoch 15 Batch 66000 Loss 1.991552710533142 in 15.221487\n",
      "saving model with loss 1.991552710533142\n",
      "Epoch 15 Batch 66500 Loss 1.9542160034179688 in 15.290811\n",
      "Epoch 15 Batch 67000 Loss 1.8563659191131592 in 15.229950\n",
      "saving model with loss 1.8563659191131592\n",
      "Epoch 15 Batch 67500 Loss 1.9065845012664795 in 15.293755\n",
      "Epoch 15 Batch 68000 Loss 1.9389808177947998 in 15.233035\n",
      "saving model with loss 1.9389808177947998\n",
      "Epoch 15 Batch 68500 Loss 2.031123638153076 in 15.280579\n",
      "Epoch 15 Batch 69000 Loss 1.8835198879241943 in 15.231688\n",
      "saving model with loss 1.8835198879241943\n",
      "Epoch 15 Batch 69500 Loss 2.070629358291626 in 15.289468\n",
      "Epoch 15 Batch 70000 Loss 1.9767440557479858 in 15.240863\n",
      "saving model with loss 1.9767440557479858\n",
      "Epoch 15 Batch 70500 Loss 1.8864606618881226 in 15.307137\n",
      "Epoch 16 Loss 2.142885208129883\n",
      "Time taken for 1 epoch 2154.8663125038147\n",
      "number of batches : 70553\n",
      "Epoch 16 Batch 0 Loss 1.8592357635498047 in 1.182843\n",
      "saving model with loss 1.8592357635498047\n",
      "Epoch 16 Batch 500 Loss 1.8792781829833984 in 15.288770\n",
      "Epoch 16 Batch 1000 Loss 1.9443607330322266 in 15.203932\n",
      "saving model with loss 1.9443607330322266\n",
      "Epoch 16 Batch 1500 Loss 1.927262544631958 in 15.291172\n",
      "Epoch 16 Batch 2000 Loss 2.0274484157562256 in 15.211574\n",
      "saving model with loss 2.0274484157562256\n",
      "Epoch 16 Batch 2500 Loss 1.9371511936187744 in 15.274115\n",
      "Epoch 16 Batch 3000 Loss 2.0639946460723877 in 15.221407\n",
      "saving model with loss 2.0639946460723877\n",
      "Epoch 16 Batch 3500 Loss 1.9745934009552002 in 15.284735\n",
      "Epoch 16 Batch 4000 Loss 1.9815397262573242 in 15.219667\n",
      "saving model with loss 1.9815397262573242\n",
      "Epoch 16 Batch 4500 Loss 1.8724521398544312 in 15.373887\n",
      "Epoch 16 Batch 5000 Loss 1.9584957361221313 in 15.263452\n",
      "saving model with loss 1.9584957361221313\n",
      "Epoch 16 Batch 5500 Loss 1.9484913349151611 in 15.259401\n",
      "Epoch 16 Batch 6000 Loss 1.9615122079849243 in 15.186473\n",
      "saving model with loss 1.9615122079849243\n",
      "Epoch 16 Batch 6500 Loss 1.9969068765640259 in 15.269066\n",
      "Epoch 16 Batch 7000 Loss 1.9956598281860352 in 15.192283\n",
      "saving model with loss 1.9956598281860352\n",
      "Epoch 16 Batch 7500 Loss 1.7234197854995728 in 15.239625\n",
      "Epoch 16 Batch 8000 Loss 2.0195796489715576 in 15.173976\n",
      "saving model with loss 2.0195796489715576\n",
      "Epoch 16 Batch 8500 Loss 2.1104838848114014 in 15.261200\n",
      "Epoch 16 Batch 9000 Loss 1.8650662899017334 in 15.152734\n",
      "saving model with loss 1.8650662899017334\n",
      "Epoch 16 Batch 9500 Loss 1.8959929943084717 in 15.255114\n",
      "Epoch 16 Batch 10000 Loss 2.1059577465057373 in 15.175599\n",
      "saving model with loss 2.1059577465057373\n",
      "Epoch 16 Batch 10500 Loss 2.055410623550415 in 15.248282\n",
      "Epoch 16 Batch 11000 Loss 1.999953031539917 in 15.187521\n",
      "saving model with loss 1.999953031539917\n",
      "Epoch 16 Batch 11500 Loss 2.0814626216888428 in 15.257540\n",
      "Epoch 16 Batch 12000 Loss 1.9510724544525146 in 15.160382\n",
      "saving model with loss 1.9510724544525146\n",
      "Epoch 16 Batch 12500 Loss 1.9994226694107056 in 15.262089\n",
      "Epoch 16 Batch 13000 Loss 2.0671393871307373 in 15.186812\n",
      "saving model with loss 2.0671393871307373\n",
      "Epoch 16 Batch 13500 Loss 1.853539228439331 in 15.245525\n",
      "Epoch 16 Batch 14000 Loss 2.0402393341064453 in 15.173362\n",
      "saving model with loss 2.0402393341064453\n",
      "Epoch 16 Batch 14500 Loss 1.9628067016601562 in 15.249894\n",
      "Epoch 16 Batch 15000 Loss 1.9382057189941406 in 15.179261\n",
      "saving model with loss 1.9382057189941406\n",
      "Epoch 16 Batch 15500 Loss 1.999760389328003 in 15.253261\n",
      "Epoch 16 Batch 16000 Loss 1.9794938564300537 in 15.178575\n",
      "saving model with loss 1.9794938564300537\n",
      "Epoch 16 Batch 16500 Loss 1.946216344833374 in 15.254290\n",
      "Epoch 16 Batch 17000 Loss 1.9374345541000366 in 15.195800\n",
      "saving model with loss 1.9374345541000366\n",
      "Epoch 16 Batch 17500 Loss 1.861280083656311 in 15.250226\n",
      "Epoch 16 Batch 18000 Loss 2.019749641418457 in 15.187146\n",
      "saving model with loss 2.019749641418457\n",
      "Epoch 16 Batch 18500 Loss 1.868260145187378 in 15.264305\n",
      "Epoch 16 Batch 19000 Loss 1.8860042095184326 in 15.594402\n",
      "saving model with loss 1.8860042095184326\n",
      "Epoch 16 Batch 19500 Loss 2.0246005058288574 in 15.302258\n",
      "Epoch 16 Batch 20000 Loss 1.915709137916565 in 15.227885\n",
      "saving model with loss 1.915709137916565\n",
      "Epoch 16 Batch 20500 Loss 2.0176784992218018 in 15.296811\n",
      "Epoch 16 Batch 21000 Loss 2.071949005126953 in 15.240035\n",
      "saving model with loss 2.071949005126953\n",
      "Epoch 16 Batch 21500 Loss 1.9549897909164429 in 15.304159\n",
      "Epoch 16 Batch 22000 Loss 2.01741886138916 in 15.222344\n",
      "saving model with loss 2.01741886138916\n",
      "Epoch 16 Batch 22500 Loss 1.911054015159607 in 15.299362\n",
      "Epoch 16 Batch 23000 Loss 2.081827163696289 in 15.230340\n",
      "saving model with loss 2.081827163696289\n",
      "Epoch 16 Batch 23500 Loss 1.818852424621582 in 15.299503\n",
      "Epoch 16 Batch 24000 Loss 1.8956466913223267 in 15.230555\n",
      "saving model with loss 1.8956466913223267\n",
      "Epoch 16 Batch 24500 Loss 2.0791850090026855 in 15.289621\n",
      "Epoch 16 Batch 25000 Loss 1.8794435262680054 in 15.228002\n",
      "saving model with loss 1.8794435262680054\n",
      "Epoch 16 Batch 25500 Loss 1.8790359497070312 in 15.302032\n",
      "Epoch 16 Batch 26000 Loss 1.999961256980896 in 15.216479\n",
      "saving model with loss 1.999961256980896\n",
      "Epoch 16 Batch 26500 Loss 1.9902904033660889 in 15.301777\n",
      "Epoch 16 Batch 27000 Loss 2.058101177215576 in 15.232921\n",
      "saving model with loss 2.058101177215576\n",
      "Epoch 16 Batch 27500 Loss 1.9386088848114014 in 15.282393\n",
      "Epoch 16 Batch 28000 Loss 2.1359939575195312 in 15.207016\n",
      "saving model with loss 2.1359939575195312\n",
      "Epoch 16 Batch 28500 Loss 1.955649971961975 in 15.291782\n",
      "Epoch 16 Batch 29000 Loss 1.9849932193756104 in 15.234169\n",
      "saving model with loss 1.9849932193756104\n",
      "Epoch 16 Batch 29500 Loss 1.9755198955535889 in 15.297920\n",
      "Epoch 16 Batch 30000 Loss 2.148850440979004 in 15.215301\n",
      "saving model with loss 2.148850440979004\n",
      "Epoch 16 Batch 30500 Loss 2.048377275466919 in 15.302590\n",
      "Epoch 16 Batch 31000 Loss 2.118797779083252 in 15.233509\n",
      "saving model with loss 2.118797779083252\n",
      "Epoch 16 Batch 31500 Loss 1.8775036334991455 in 15.295802\n",
      "Epoch 16 Batch 32000 Loss 1.9445022344589233 in 15.216970\n",
      "saving model with loss 1.9445022344589233\n",
      "Epoch 16 Batch 32500 Loss 1.9215762615203857 in 15.295693\n",
      "Epoch 16 Batch 33000 Loss 1.9647715091705322 in 15.226557\n",
      "saving model with loss 1.9647715091705322\n",
      "Epoch 16 Batch 33500 Loss 2.065596580505371 in 15.287299\n",
      "Epoch 16 Batch 34000 Loss 2.073456287384033 in 15.269091\n",
      "saving model with loss 2.073456287384033\n",
      "Epoch 16 Batch 34500 Loss 2.018725872039795 in 15.512388\n",
      "Epoch 16 Batch 35000 Loss 1.7705707550048828 in 15.207181\n",
      "saving model with loss 1.7705707550048828\n",
      "Epoch 16 Batch 35500 Loss 2.024672269821167 in 15.292680\n",
      "Epoch 16 Batch 36000 Loss 1.9428821802139282 in 15.220833\n",
      "saving model with loss 1.9428821802139282\n",
      "Epoch 16 Batch 36500 Loss 1.9901472330093384 in 15.292701\n",
      "Epoch 16 Batch 37000 Loss 1.9212958812713623 in 15.221555\n",
      "saving model with loss 1.9212958812713623\n",
      "Epoch 16 Batch 37500 Loss 1.963666319847107 in 15.315064\n",
      "Epoch 16 Batch 38000 Loss 1.848231315612793 in 15.226029\n",
      "saving model with loss 1.848231315612793\n",
      "Epoch 16 Batch 38500 Loss 2.133561372756958 in 15.381307\n",
      "Epoch 16 Batch 39000 Loss 2.016916513442993 in 15.291554\n",
      "saving model with loss 2.016916513442993\n",
      "Epoch 16 Batch 39500 Loss 1.8891336917877197 in 15.664704\n",
      "Epoch 16 Batch 40000 Loss 1.9128787517547607 in 15.628370\n",
      "saving model with loss 1.9128787517547607\n",
      "Epoch 16 Batch 40500 Loss 1.938153624534607 in 15.423676\n",
      "Epoch 16 Batch 41000 Loss 1.9952583312988281 in 15.241409\n",
      "saving model with loss 1.9952583312988281\n",
      "Epoch 16 Batch 41500 Loss 2.041256904602051 in 15.304175\n",
      "Epoch 16 Batch 42000 Loss 1.9179470539093018 in 15.251034\n",
      "saving model with loss 1.9179470539093018\n",
      "Epoch 16 Batch 42500 Loss 1.9391891956329346 in 15.273061\n",
      "Epoch 16 Batch 43000 Loss 2.056776523590088 in 15.234258\n",
      "saving model with loss 2.056776523590088\n",
      "Epoch 16 Batch 43500 Loss 2.0490524768829346 in 15.318902\n",
      "Epoch 16 Batch 44000 Loss 2.067455530166626 in 15.231966\n",
      "saving model with loss 2.067455530166626\n",
      "Epoch 16 Batch 44500 Loss 1.9412333965301514 in 15.280735\n",
      "Epoch 16 Batch 45000 Loss 1.889717698097229 in 15.216842\n",
      "saving model with loss 1.889717698097229\n",
      "Epoch 16 Batch 45500 Loss 1.9490734338760376 in 15.284415\n",
      "Epoch 16 Batch 46000 Loss 1.939622163772583 in 15.238272\n",
      "saving model with loss 1.939622163772583\n",
      "Epoch 16 Batch 46500 Loss 1.9608495235443115 in 15.308309\n",
      "Epoch 16 Batch 47000 Loss 2.0299859046936035 in 15.216305\n",
      "saving model with loss 2.0299859046936035\n",
      "Epoch 16 Batch 47500 Loss 1.8902918100357056 in 15.281699\n",
      "Epoch 16 Batch 48000 Loss 2.0932586193084717 in 15.211368\n",
      "saving model with loss 2.0932586193084717\n",
      "Epoch 16 Batch 48500 Loss 1.9629955291748047 in 15.297474\n",
      "Epoch 16 Batch 49000 Loss 2.0694937705993652 in 15.214849\n",
      "saving model with loss 2.0694937705993652\n",
      "Epoch 16 Batch 49500 Loss 2.0062880516052246 in 15.297237\n",
      "Epoch 16 Batch 50000 Loss 2.0216293334960938 in 15.247109\n",
      "saving model with loss 2.0216293334960938\n",
      "Epoch 16 Batch 50500 Loss 2.1595206260681152 in 15.256472\n",
      "Epoch 16 Batch 51000 Loss 2.020548105239868 in 15.186388\n",
      "saving model with loss 2.020548105239868\n",
      "Epoch 16 Batch 51500 Loss 2.0150866508483887 in 15.253269\n",
      "Epoch 16 Batch 52000 Loss 2.143192768096924 in 15.184460\n",
      "saving model with loss 2.143192768096924\n",
      "Epoch 16 Batch 52500 Loss 2.033174991607666 in 15.249209\n",
      "Epoch 16 Batch 53000 Loss 1.9627498388290405 in 15.174910\n",
      "saving model with loss 1.9627498388290405\n",
      "Epoch 16 Batch 53500 Loss 2.0232086181640625 in 15.246712\n",
      "Epoch 16 Batch 54000 Loss 2.039903163909912 in 15.189135\n",
      "saving model with loss 2.039903163909912\n",
      "Epoch 16 Batch 54500 Loss 1.848188042640686 in 15.267354\n",
      "Epoch 16 Batch 55000 Loss 1.9715707302093506 in 15.188287\n",
      "saving model with loss 1.9715707302093506\n",
      "Epoch 16 Batch 55500 Loss 1.9272321462631226 in 15.237271\n",
      "Epoch 16 Batch 56000 Loss 1.9826691150665283 in 15.173443\n",
      "saving model with loss 1.9826691150665283\n",
      "Epoch 16 Batch 56500 Loss 2.0545897483825684 in 15.271407\n",
      "Epoch 16 Batch 57000 Loss 1.9674736261367798 in 15.183939\n",
      "saving model with loss 1.9674736261367798\n",
      "Epoch 16 Batch 57500 Loss 1.93002188205719 in 15.256263\n",
      "Epoch 16 Batch 58000 Loss 1.9752826690673828 in 15.185160\n",
      "saving model with loss 1.9752826690673828\n",
      "Epoch 16 Batch 58500 Loss 1.9911857843399048 in 15.260001\n",
      "Epoch 16 Batch 59000 Loss 2.006798267364502 in 15.193886\n",
      "saving model with loss 2.006798267364502\n",
      "Epoch 16 Batch 59500 Loss 2.010612964630127 in 15.250873\n",
      "Epoch 16 Batch 60000 Loss 2.020047187805176 in 15.196693\n",
      "saving model with loss 2.020047187805176\n",
      "Epoch 16 Batch 60500 Loss 1.9167859554290771 in 15.272757\n",
      "Epoch 16 Batch 61000 Loss 1.925661325454712 in 15.170048\n",
      "saving model with loss 1.925661325454712\n",
      "Epoch 16 Batch 61500 Loss 2.1138007640838623 in 15.260117\n",
      "Epoch 16 Batch 62000 Loss 2.0225636959075928 in 15.184374\n",
      "saving model with loss 2.0225636959075928\n",
      "Epoch 16 Batch 62500 Loss 1.9673305749893188 in 15.260720\n",
      "Epoch 16 Batch 63000 Loss 2.0792174339294434 in 15.190069\n",
      "saving model with loss 2.0792174339294434\n",
      "Epoch 16 Batch 63500 Loss 2.0205068588256836 in 15.278017\n",
      "Epoch 16 Batch 64000 Loss 2.0300135612487793 in 15.196764\n",
      "saving model with loss 2.0300135612487793\n",
      "Epoch 16 Batch 64500 Loss 2.018214464187622 in 15.267073\n",
      "Epoch 16 Batch 65000 Loss 2.016890048980713 in 15.182458\n",
      "saving model with loss 2.016890048980713\n",
      "Epoch 16 Batch 65500 Loss 1.9419934749603271 in 15.256253\n",
      "Epoch 16 Batch 66000 Loss 1.9859352111816406 in 15.183680\n",
      "saving model with loss 1.9859352111816406\n",
      "Epoch 16 Batch 66500 Loss 2.0324387550354004 in 15.246750\n",
      "Epoch 16 Batch 67000 Loss 1.8577539920806885 in 15.193126\n",
      "saving model with loss 1.8577539920806885\n",
      "Epoch 16 Batch 67500 Loss 2.028212308883667 in 15.266245\n",
      "Epoch 16 Batch 68000 Loss 2.1592342853546143 in 15.201206\n",
      "saving model with loss 2.1592342853546143\n",
      "Epoch 16 Batch 68500 Loss 1.8974040746688843 in 15.247147\n",
      "Epoch 16 Batch 69000 Loss 1.9897491931915283 in 15.179701\n",
      "saving model with loss 1.9897491931915283\n",
      "Epoch 16 Batch 69500 Loss 1.9843021631240845 in 15.247420\n",
      "Epoch 16 Batch 70000 Loss 2.0054495334625244 in 15.186491\n",
      "saving model with loss 2.0054495334625244\n",
      "Epoch 16 Batch 70500 Loss 1.9969661235809326 in 15.250658\n",
      "Epoch 17 Loss 1.8836215734481812\n",
      "Time taken for 1 epoch 2153.5791294574738\n",
      "number of batches : 70553\n",
      "Epoch 17 Batch 0 Loss 1.9408992528915405 in 1.161592\n",
      "saving model with loss 1.9408992528915405\n",
      "Epoch 17 Batch 500 Loss 1.9003807306289673 in 15.225359\n",
      "Epoch 17 Batch 1000 Loss 2.0791501998901367 in 15.151274\n",
      "saving model with loss 2.0791501998901367\n",
      "Epoch 17 Batch 1500 Loss 1.9120925664901733 in 15.208894\n",
      "Epoch 17 Batch 2000 Loss 2.041851282119751 in 15.140474\n",
      "saving model with loss 2.041851282119751\n",
      "Epoch 17 Batch 2500 Loss 1.9511120319366455 in 15.222749\n",
      "Epoch 17 Batch 3000 Loss 1.9398086071014404 in 15.153879\n",
      "saving model with loss 1.9398086071014404\n",
      "Epoch 17 Batch 3500 Loss 2.1102101802825928 in 15.212029\n",
      "Epoch 17 Batch 4000 Loss 1.9854170083999634 in 15.150099\n",
      "saving model with loss 1.9854170083999634\n",
      "Epoch 17 Batch 4500 Loss 1.8963642120361328 in 15.211193\n",
      "Epoch 17 Batch 5000 Loss 2.148232936859131 in 15.150546\n",
      "saving model with loss 2.148232936859131\n",
      "Epoch 17 Batch 5500 Loss 1.9765958786010742 in 15.212782\n",
      "Epoch 17 Batch 6000 Loss 2.062539577484131 in 15.146927\n",
      "saving model with loss 2.062539577484131\n",
      "Epoch 17 Batch 6500 Loss 1.9737555980682373 in 15.203612\n",
      "Epoch 17 Batch 7000 Loss 1.9488270282745361 in 15.175014\n",
      "saving model with loss 1.9488270282745361\n",
      "Epoch 17 Batch 7500 Loss 2.05967378616333 in 15.260260\n",
      "Epoch 17 Batch 8000 Loss 1.9354044198989868 in 15.186954\n",
      "saving model with loss 1.9354044198989868\n",
      "Epoch 17 Batch 8500 Loss 2.043208122253418 in 15.249981\n",
      "Epoch 17 Batch 9000 Loss 2.177727222442627 in 15.184863\n",
      "saving model with loss 2.177727222442627\n",
      "Epoch 17 Batch 9500 Loss 1.9139649868011475 in 15.244925\n",
      "Epoch 17 Batch 10000 Loss 1.947137475013733 in 15.174805\n",
      "saving model with loss 1.947137475013733\n",
      "Epoch 17 Batch 10500 Loss 1.953904390335083 in 15.264201\n",
      "Epoch 17 Batch 11000 Loss 1.93903386592865 in 15.186610\n",
      "saving model with loss 1.93903386592865\n",
      "Epoch 17 Batch 11500 Loss 2.0338644981384277 in 15.261007\n",
      "Epoch 17 Batch 12000 Loss 2.097649097442627 in 15.162827\n",
      "saving model with loss 2.097649097442627\n",
      "Epoch 17 Batch 12500 Loss 1.9333794116973877 in 15.246750\n",
      "Epoch 17 Batch 13000 Loss 2.0807290077209473 in 15.178311\n",
      "saving model with loss 2.0807290077209473\n",
      "Epoch 17 Batch 13500 Loss 1.983452558517456 in 15.238643\n",
      "Epoch 17 Batch 14000 Loss 2.0813441276550293 in 15.166444\n",
      "saving model with loss 2.0813441276550293\n",
      "Epoch 17 Batch 14500 Loss 1.855118989944458 in 15.257158\n",
      "Epoch 17 Batch 15000 Loss 2.230224132537842 in 15.170709\n",
      "saving model with loss 2.230224132537842\n",
      "Epoch 17 Batch 15500 Loss 1.965993881225586 in 15.241504\n",
      "Epoch 17 Batch 16000 Loss 1.905906081199646 in 15.179473\n",
      "saving model with loss 1.905906081199646\n",
      "Epoch 17 Batch 16500 Loss 1.9334805011749268 in 15.256293\n",
      "Epoch 17 Batch 17000 Loss 1.9781837463378906 in 15.178759\n",
      "saving model with loss 1.9781837463378906\n",
      "Epoch 17 Batch 17500 Loss 1.9525401592254639 in 15.240201\n",
      "Epoch 17 Batch 18000 Loss 2.011617660522461 in 15.162660\n",
      "saving model with loss 2.011617660522461\n",
      "Epoch 17 Batch 18500 Loss 1.8868767023086548 in 15.253036\n",
      "Epoch 17 Batch 19000 Loss 2.002105236053467 in 15.189061\n",
      "saving model with loss 2.002105236053467\n",
      "Epoch 17 Batch 19500 Loss 2.0842013359069824 in 15.262545\n",
      "Epoch 17 Batch 20000 Loss 2.0174448490142822 in 15.181696\n",
      "saving model with loss 2.0174448490142822\n",
      "Epoch 17 Batch 20500 Loss 1.897163987159729 in 15.261286\n",
      "Epoch 17 Batch 21000 Loss 1.9154285192489624 in 15.168223\n",
      "saving model with loss 1.9154285192489624\n",
      "Epoch 17 Batch 21500 Loss 2.0078272819519043 in 15.254684\n",
      "Epoch 17 Batch 22000 Loss 2.0318052768707275 in 15.179191\n",
      "saving model with loss 2.0318052768707275\n",
      "Epoch 17 Batch 22500 Loss 2.0738582611083984 in 15.242493\n",
      "Epoch 17 Batch 23000 Loss 2.0992343425750732 in 15.198608\n",
      "saving model with loss 2.0992343425750732\n",
      "Epoch 17 Batch 23500 Loss 1.9989941120147705 in 15.255586\n",
      "Epoch 17 Batch 24000 Loss 2.097383975982666 in 15.172725\n",
      "saving model with loss 2.097383975982666\n",
      "Epoch 17 Batch 24500 Loss 1.8959662914276123 in 15.248685\n",
      "Epoch 17 Batch 25000 Loss 1.950803518295288 in 15.184024\n",
      "saving model with loss 1.950803518295288\n",
      "Epoch 17 Batch 25500 Loss 1.989611029624939 in 15.263051\n",
      "Epoch 17 Batch 26000 Loss 1.9375346899032593 in 15.179346\n",
      "saving model with loss 1.9375346899032593\n",
      "Epoch 17 Batch 26500 Loss 1.9689438343048096 in 15.253333\n",
      "Epoch 17 Batch 27000 Loss 2.043189287185669 in 15.193598\n",
      "saving model with loss 2.043189287185669\n",
      "Epoch 17 Batch 27500 Loss 1.9975063800811768 in 15.266636\n",
      "Epoch 17 Batch 28000 Loss 2.1334290504455566 in 15.165212\n",
      "saving model with loss 2.1334290504455566\n",
      "Epoch 17 Batch 28500 Loss 2.0238845348358154 in 15.235413\n",
      "Epoch 17 Batch 29000 Loss 1.9322255849838257 in 15.173942\n",
      "saving model with loss 1.9322255849838257\n",
      "Epoch 17 Batch 29500 Loss 1.9947001934051514 in 15.240989\n",
      "Epoch 17 Batch 30000 Loss 2.0145554542541504 in 15.185704\n",
      "saving model with loss 2.0145554542541504\n",
      "Epoch 17 Batch 30500 Loss 1.9851114749908447 in 15.255973\n",
      "Epoch 17 Batch 31000 Loss 2.116725444793701 in 15.178881\n",
      "saving model with loss 2.116725444793701\n",
      "Epoch 17 Batch 31500 Loss 2.0042331218719482 in 15.235985\n",
      "Epoch 17 Batch 32000 Loss 2.027660608291626 in 15.184427\n",
      "saving model with loss 2.027660608291626\n",
      "Epoch 17 Batch 32500 Loss 2.003157138824463 in 15.258602\n",
      "Epoch 17 Batch 33000 Loss 2.0490989685058594 in 15.191335\n",
      "saving model with loss 2.0490989685058594\n",
      "Epoch 17 Batch 33500 Loss 2.0413830280303955 in 15.243922\n",
      "Epoch 17 Batch 34000 Loss 2.086909532546997 in 15.182395\n",
      "saving model with loss 2.086909532546997\n",
      "Epoch 17 Batch 34500 Loss 2.0138301849365234 in 15.248397\n",
      "Epoch 17 Batch 35000 Loss 2.0391006469726562 in 15.196709\n",
      "saving model with loss 2.0391006469726562\n",
      "Epoch 17 Batch 35500 Loss 2.0193395614624023 in 15.237184\n",
      "Epoch 17 Batch 36000 Loss 2.040841817855835 in 15.176446\n",
      "saving model with loss 2.040841817855835\n",
      "Epoch 17 Batch 36500 Loss 1.943524718284607 in 15.237669\n",
      "Epoch 17 Batch 37000 Loss 2.116882801055908 in 15.167120\n",
      "saving model with loss 2.116882801055908\n",
      "Epoch 17 Batch 37500 Loss 1.9488584995269775 in 15.252654\n",
      "Epoch 17 Batch 38000 Loss 2.0781497955322266 in 15.187333\n",
      "saving model with loss 2.0781497955322266\n",
      "Epoch 17 Batch 38500 Loss 1.9315392971038818 in 15.257881\n",
      "Epoch 17 Batch 39000 Loss 2.1021080017089844 in 15.180397\n",
      "saving model with loss 2.1021080017089844\n",
      "Epoch 17 Batch 39500 Loss 2.0501320362091064 in 15.265634\n",
      "Epoch 17 Batch 40000 Loss 2.1888108253479004 in 15.178695\n",
      "saving model with loss 2.1888108253479004\n",
      "Epoch 17 Batch 40500 Loss 2.069225311279297 in 15.257364\n",
      "Epoch 17 Batch 41000 Loss 1.9824247360229492 in 15.174983\n",
      "saving model with loss 1.9824247360229492\n",
      "Epoch 17 Batch 41500 Loss 2.1847612857818604 in 15.247917\n",
      "Epoch 17 Batch 42000 Loss 2.129913568496704 in 15.186331\n",
      "saving model with loss 2.129913568496704\n",
      "Epoch 17 Batch 42500 Loss 2.0375213623046875 in 15.252439\n",
      "Epoch 17 Batch 43000 Loss 1.9354970455169678 in 15.187726\n",
      "saving model with loss 1.9354970455169678\n",
      "Epoch 17 Batch 43500 Loss 2.053455352783203 in 15.229287\n",
      "Epoch 17 Batch 44000 Loss 1.948108434677124 in 15.174330\n",
      "saving model with loss 1.948108434677124\n",
      "Epoch 17 Batch 44500 Loss 1.9785540103912354 in 15.257107\n",
      "Epoch 17 Batch 45000 Loss 2.135983943939209 in 15.173230\n",
      "saving model with loss 2.135983943939209\n",
      "Epoch 17 Batch 45500 Loss 2.0990920066833496 in 15.248298\n",
      "Epoch 17 Batch 46000 Loss 2.0482869148254395 in 15.185640\n",
      "saving model with loss 2.0482869148254395\n",
      "Epoch 17 Batch 46500 Loss 2.215538740158081 in 15.248769\n",
      "Epoch 17 Batch 47000 Loss 2.1484851837158203 in 15.172025\n",
      "saving model with loss 2.1484851837158203\n",
      "Epoch 17 Batch 47500 Loss 2.1657874584198 in 15.246685\n",
      "Epoch 17 Batch 48000 Loss 2.072740077972412 in 15.182766\n",
      "saving model with loss 2.072740077972412\n",
      "Epoch 17 Batch 48500 Loss 2.093214273452759 in 15.252182\n",
      "Epoch 17 Batch 49000 Loss 2.077740430831909 in 15.178680\n",
      "saving model with loss 2.077740430831909\n",
      "Epoch 17 Batch 49500 Loss 2.083010196685791 in 15.243751\n",
      "Epoch 17 Batch 50000 Loss 2.0155348777770996 in 15.185956\n",
      "saving model with loss 2.0155348777770996\n",
      "Epoch 17 Batch 50500 Loss 2.0886614322662354 in 15.239224\n",
      "Epoch 17 Batch 51000 Loss 2.0021464824676514 in 15.192938\n",
      "saving model with loss 2.0021464824676514\n",
      "Epoch 17 Batch 51500 Loss 1.992600679397583 in 15.257114\n",
      "Epoch 17 Batch 52000 Loss 1.944411277770996 in 15.188357\n",
      "saving model with loss 1.944411277770996\n",
      "Epoch 17 Batch 52500 Loss 1.8940330743789673 in 15.317635\n",
      "Epoch 17 Batch 53000 Loss 1.949591875076294 in 15.976815\n",
      "saving model with loss 1.949591875076294\n",
      "Epoch 17 Batch 53500 Loss 1.9945484399795532 in 15.943263\n",
      "Epoch 17 Batch 54000 Loss 1.9563766717910767 in 15.622935\n",
      "saving model with loss 1.9563766717910767\n",
      "Epoch 17 Batch 54500 Loss 2.248563528060913 in 16.676008\n",
      "Epoch 17 Batch 55000 Loss 1.9910480976104736 in 16.828483\n",
      "saving model with loss 1.9910480976104736\n",
      "Epoch 17 Batch 55500 Loss 2.09177565574646 in 16.846952\n",
      "Epoch 17 Batch 56000 Loss 2.1295578479766846 in 16.875090\n",
      "saving model with loss 2.1295578479766846\n",
      "Epoch 17 Batch 56500 Loss 2.0100855827331543 in 15.915393\n",
      "Epoch 17 Batch 57000 Loss 2.028815984725952 in 15.299161\n",
      "saving model with loss 2.028815984725952\n",
      "Epoch 17 Batch 57500 Loss 2.005838632583618 in 15.633740\n",
      "Epoch 17 Batch 58000 Loss 1.9222888946533203 in 15.909907\n",
      "saving model with loss 1.9222888946533203\n",
      "Epoch 17 Batch 58500 Loss 2.101381540298462 in 15.257179\n",
      "Epoch 17 Batch 59000 Loss 2.0318331718444824 in 15.253979\n",
      "saving model with loss 2.0318331718444824\n",
      "Epoch 17 Batch 59500 Loss 2.1186118125915527 in 15.278705\n",
      "Epoch 17 Batch 60000 Loss 2.063408136367798 in 15.192547\n",
      "saving model with loss 2.063408136367798\n",
      "Epoch 17 Batch 60500 Loss 1.9539821147918701 in 15.291723\n",
      "Epoch 17 Batch 61000 Loss 2.11849308013916 in 15.213738\n",
      "saving model with loss 2.11849308013916\n",
      "Epoch 17 Batch 61500 Loss 1.9399054050445557 in 15.283058\n",
      "Epoch 17 Batch 62000 Loss 2.0888924598693848 in 15.198539\n",
      "saving model with loss 2.0888924598693848\n",
      "Epoch 17 Batch 62500 Loss 1.8610756397247314 in 15.282587\n",
      "Epoch 17 Batch 63000 Loss 1.9684536457061768 in 15.209272\n",
      "saving model with loss 1.9684536457061768\n",
      "Epoch 17 Batch 63500 Loss 1.9595504999160767 in 15.275622\n",
      "Epoch 17 Batch 64000 Loss 2.137361526489258 in 15.213429\n",
      "saving model with loss 2.137361526489258\n",
      "Epoch 17 Batch 64500 Loss 2.0876529216766357 in 15.261575\n",
      "Epoch 17 Batch 65000 Loss 2.07930326461792 in 15.225002\n",
      "saving model with loss 2.07930326461792\n",
      "Epoch 17 Batch 65500 Loss 2.0621588230133057 in 15.258478\n",
      "Epoch 17 Batch 66000 Loss 1.983705759048462 in 15.259667\n",
      "saving model with loss 1.983705759048462\n",
      "Epoch 17 Batch 66500 Loss 2.039031505584717 in 15.287532\n",
      "Epoch 17 Batch 67000 Loss 2.0643370151519775 in 15.226593\n",
      "saving model with loss 2.0643370151519775\n",
      "Epoch 17 Batch 67500 Loss 1.9640741348266602 in 15.305959\n",
      "Epoch 17 Batch 68000 Loss 2.136557102203369 in 15.214989\n",
      "saving model with loss 2.136557102203369\n",
      "Epoch 17 Batch 68500 Loss 2.1705875396728516 in 15.281385\n",
      "Epoch 17 Batch 69000 Loss 2.035121440887451 in 15.213238\n",
      "saving model with loss 2.035121440887451\n",
      "Epoch 17 Batch 69500 Loss 2.0580105781555176 in 15.304407\n",
      "Epoch 17 Batch 70000 Loss 2.0551161766052246 in 15.211083\n",
      "saving model with loss 2.0551161766052246\n",
      "Epoch 17 Batch 70500 Loss 1.965963363647461 in 15.262715\n",
      "Epoch 18 Loss 1.9510244131088257\n",
      "Time taken for 1 epoch 2158.888437986374\n",
      "number of batches : 70553\n",
      "Epoch 18 Batch 0 Loss 1.9350601434707642 in 1.325157\n",
      "saving model with loss 1.9350601434707642\n",
      "Epoch 18 Batch 500 Loss 1.9221904277801514 in 15.288682\n",
      "Epoch 18 Batch 1000 Loss 2.0287928581237793 in 15.188957\n",
      "saving model with loss 2.0287928581237793\n",
      "Epoch 18 Batch 1500 Loss 2.2144291400909424 in 15.266086\n",
      "Epoch 18 Batch 2000 Loss 2.283630847930908 in 15.187811\n",
      "saving model with loss 2.283630847930908\n",
      "Epoch 18 Batch 2500 Loss 2.0309765338897705 in 15.275639\n",
      "Epoch 18 Batch 3000 Loss 1.9499061107635498 in 15.182513\n",
      "saving model with loss 1.9499061107635498\n",
      "Epoch 18 Batch 3500 Loss 2.0620524883270264 in 15.270650\n",
      "Epoch 18 Batch 4000 Loss 2.121833324432373 in 15.209786\n",
      "saving model with loss 2.121833324432373\n",
      "Epoch 18 Batch 4500 Loss 2.0794460773468018 in 15.242324\n",
      "Epoch 18 Batch 5000 Loss 2.1111512184143066 in 15.194369\n",
      "saving model with loss 2.1111512184143066\n",
      "Epoch 18 Batch 5500 Loss 1.9350121021270752 in 15.266966\n",
      "Epoch 18 Batch 6000 Loss 2.048264980316162 in 15.217687\n",
      "saving model with loss 2.048264980316162\n",
      "Epoch 18 Batch 6500 Loss 1.9241697788238525 in 15.237551\n",
      "Epoch 18 Batch 7000 Loss 1.9716694355010986 in 15.269298\n",
      "saving model with loss 1.9716694355010986\n",
      "Epoch 18 Batch 7500 Loss 1.9763269424438477 in 15.265403\n",
      "Epoch 18 Batch 8000 Loss 1.997138261795044 in 15.180471\n",
      "saving model with loss 1.997138261795044\n",
      "Epoch 18 Batch 8500 Loss 2.1074330806732178 in 15.242497\n",
      "Epoch 18 Batch 9000 Loss 2.235628366470337 in 15.180089\n",
      "saving model with loss 2.235628366470337\n",
      "Epoch 18 Batch 9500 Loss 1.9938554763793945 in 15.245504\n",
      "Epoch 18 Batch 10000 Loss 2.0952324867248535 in 15.150548\n",
      "saving model with loss 2.0952324867248535\n",
      "Epoch 18 Batch 10500 Loss 2.0946602821350098 in 15.243357\n",
      "Epoch 18 Batch 11000 Loss 2.105870246887207 in 15.351032\n",
      "saving model with loss 2.105870246887207\n",
      "Epoch 18 Batch 11500 Loss 2.164793014526367 in 15.282984\n",
      "Epoch 18 Batch 12000 Loss 2.2074408531188965 in 15.186064\n",
      "saving model with loss 2.2074408531188965\n",
      "Epoch 18 Batch 12500 Loss 2.0360705852508545 in 15.240758\n",
      "Epoch 18 Batch 13000 Loss 2.1419711112976074 in 15.180901\n",
      "saving model with loss 2.1419711112976074\n",
      "Epoch 18 Batch 13500 Loss 2.128018856048584 in 15.238506\n",
      "Epoch 18 Batch 14000 Loss 2.0534157752990723 in 15.181903\n",
      "saving model with loss 2.0534157752990723\n",
      "Epoch 18 Batch 14500 Loss 2.07124400138855 in 15.246644\n",
      "Epoch 18 Batch 15000 Loss 2.0811660289764404 in 15.178835\n",
      "saving model with loss 2.0811660289764404\n",
      "Epoch 18 Batch 15500 Loss 2.189427614212036 in 15.242852\n",
      "Epoch 18 Batch 16000 Loss 2.090679168701172 in 15.180326\n",
      "saving model with loss 2.090679168701172\n",
      "Epoch 18 Batch 16500 Loss 2.014253616333008 in 15.240522\n",
      "Epoch 18 Batch 17000 Loss 1.9705508947372437 in 15.159714\n",
      "saving model with loss 1.9705508947372437\n",
      "Epoch 18 Batch 17500 Loss 1.9401054382324219 in 15.206757\n",
      "Epoch 18 Batch 18000 Loss 1.9514153003692627 in 15.184057\n",
      "saving model with loss 1.9514153003692627\n",
      "Epoch 18 Batch 18500 Loss 2.1067891120910645 in 15.241385\n",
      "Epoch 18 Batch 19000 Loss 2.0961997509002686 in 15.180917\n",
      "saving model with loss 2.0961997509002686\n",
      "Epoch 18 Batch 19500 Loss 2.081967830657959 in 15.231758\n",
      "Epoch 18 Batch 20000 Loss 2.092034339904785 in 15.177900\n",
      "saving model with loss 2.092034339904785\n",
      "Epoch 18 Batch 20500 Loss 1.962485909461975 in 15.242589\n",
      "Epoch 18 Batch 21000 Loss 2.034973621368408 in 15.182827\n",
      "saving model with loss 2.034973621368408\n",
      "Epoch 18 Batch 21500 Loss 2.0015342235565186 in 15.242495\n",
      "Epoch 18 Batch 22000 Loss 2.0094571113586426 in 15.153710\n",
      "saving model with loss 2.0094571113586426\n",
      "Epoch 18 Batch 22500 Loss 2.1228039264678955 in 15.224590\n",
      "Epoch 18 Batch 23000 Loss 2.1866695880889893 in 15.166018\n",
      "saving model with loss 2.1866695880889893\n",
      "Epoch 18 Batch 23500 Loss 2.0602495670318604 in 15.244073\n",
      "Epoch 18 Batch 24000 Loss 2.0319724082946777 in 15.182563\n",
      "saving model with loss 2.0319724082946777\n",
      "Epoch 18 Batch 24500 Loss 2.0122485160827637 in 15.236826\n",
      "Epoch 18 Batch 25000 Loss 2.1196682453155518 in 15.182347\n",
      "saving model with loss 2.1196682453155518\n",
      "Epoch 18 Batch 25500 Loss 1.9412864446640015 in 15.243285\n",
      "Epoch 18 Batch 26000 Loss 2.012779474258423 in 15.186081\n",
      "saving model with loss 2.012779474258423\n",
      "Epoch 18 Batch 26500 Loss 2.190885543823242 in 15.242625\n",
      "Epoch 18 Batch 27000 Loss 2.0836338996887207 in 15.160193\n",
      "saving model with loss 2.0836338996887207\n",
      "Epoch 18 Batch 27500 Loss 2.041072130203247 in 15.244346\n",
      "Epoch 18 Batch 28000 Loss 2.0941553115844727 in 15.164879\n",
      "saving model with loss 2.0941553115844727\n",
      "Epoch 18 Batch 28500 Loss 2.15319561958313 in 15.244976\n",
      "Epoch 18 Batch 29000 Loss 2.041626214981079 in 15.172357\n",
      "saving model with loss 2.041626214981079\n",
      "Epoch 18 Batch 29500 Loss 1.9784055948257446 in 15.238578\n",
      "Epoch 18 Batch 30000 Loss 2.016371250152588 in 15.178091\n",
      "saving model with loss 2.016371250152588\n",
      "Epoch 18 Batch 30500 Loss 2.1412882804870605 in 15.213402\n",
      "Epoch 18 Batch 31000 Loss 2.049217462539673 in 15.181746\n",
      "saving model with loss 2.049217462539673\n",
      "Epoch 18 Batch 31500 Loss 2.155667543411255 in 15.246215\n",
      "Epoch 18 Batch 32000 Loss 2.0965733528137207 in 15.165760\n",
      "saving model with loss 2.0965733528137207\n",
      "Epoch 18 Batch 32500 Loss 2.0649988651275635 in 15.239388\n",
      "Epoch 18 Batch 33000 Loss 2.2166905403137207 in 15.183987\n",
      "saving model with loss 2.2166905403137207\n",
      "Epoch 18 Batch 33500 Loss 2.0972900390625 in 15.243324\n",
      "Epoch 18 Batch 34000 Loss 1.9370561838150024 in 15.184092\n",
      "saving model with loss 1.9370561838150024\n",
      "Epoch 18 Batch 34500 Loss 1.9591045379638672 in 15.239743\n",
      "Epoch 18 Batch 35000 Loss 2.2463059425354004 in 15.178663\n",
      "saving model with loss 2.2463059425354004\n",
      "Epoch 18 Batch 35500 Loss 2.0247011184692383 in 15.245697\n",
      "Epoch 18 Batch 36000 Loss 2.109001874923706 in 15.171912\n",
      "saving model with loss 2.109001874923706\n",
      "Epoch 18 Batch 36500 Loss 2.12414813041687 in 15.252861\n",
      "Epoch 18 Batch 37000 Loss 2.0915112495422363 in 15.178133\n",
      "saving model with loss 2.0915112495422363\n",
      "Epoch 18 Batch 37500 Loss 2.1112608909606934 in 15.243519\n",
      "Epoch 18 Batch 38000 Loss 2.1068220138549805 in 15.184515\n",
      "saving model with loss 2.1068220138549805\n",
      "Epoch 18 Batch 38500 Loss 2.1289188861846924 in 15.212873\n",
      "Epoch 18 Batch 39000 Loss 2.152172803878784 in 15.158767\n",
      "saving model with loss 2.152172803878784\n",
      "Epoch 18 Batch 39500 Loss 2.0470688343048096 in 15.220088\n",
      "Epoch 18 Batch 40000 Loss 1.9602962732315063 in 15.179579\n",
      "saving model with loss 1.9602962732315063\n",
      "Epoch 18 Batch 40500 Loss 2.1762630939483643 in 15.245399\n",
      "Epoch 18 Batch 41000 Loss 2.0062012672424316 in 15.178995\n",
      "saving model with loss 2.0062012672424316\n",
      "Epoch 18 Batch 41500 Loss 2.1785404682159424 in 15.244442\n",
      "Epoch 18 Batch 42000 Loss 1.903601050376892 in 15.178813\n",
      "saving model with loss 1.903601050376892\n",
      "Epoch 18 Batch 42500 Loss 1.9187030792236328 in 15.213222\n",
      "Epoch 18 Batch 43000 Loss 1.9467048645019531 in 15.180220\n",
      "saving model with loss 1.9467048645019531\n",
      "Epoch 18 Batch 43500 Loss 1.989919900894165 in 15.253478\n",
      "Epoch 18 Batch 44000 Loss 2.154421329498291 in 15.158640\n",
      "saving model with loss 2.154421329498291\n",
      "Epoch 18 Batch 44500 Loss 2.1398701667785645 in 15.238752\n",
      "Epoch 18 Batch 45000 Loss 2.1750144958496094 in 15.182441\n",
      "saving model with loss 2.1750144958496094\n",
      "Epoch 18 Batch 45500 Loss 2.104995012283325 in 15.245792\n",
      "Epoch 18 Batch 46000 Loss 1.9742085933685303 in 15.187993\n",
      "saving model with loss 1.9742085933685303\n",
      "Epoch 18 Batch 46500 Loss 2.114394426345825 in 15.233528\n",
      "Epoch 18 Batch 47000 Loss 2.0771079063415527 in 15.182110\n",
      "saving model with loss 2.0771079063415527\n",
      "Epoch 18 Batch 47500 Loss 2.1354176998138428 in 15.242554\n",
      "Epoch 18 Batch 48000 Loss 2.1121554374694824 in 15.179187\n",
      "saving model with loss 2.1121554374694824\n",
      "Epoch 18 Batch 48500 Loss 2.0450685024261475 in 15.253593\n",
      "Epoch 18 Batch 49000 Loss 2.1260180473327637 in 15.170249\n",
      "saving model with loss 2.1260180473327637\n",
      "Epoch 18 Batch 49500 Loss 2.3029239177703857 in 15.213643\n",
      "Epoch 18 Batch 50000 Loss 2.146030902862549 in 15.178771\n",
      "saving model with loss 2.146030902862549\n",
      "Epoch 18 Batch 50500 Loss 2.2125625610351562 in 15.244974\n",
      "Epoch 18 Batch 51000 Loss 2.067059278488159 in 15.184112\n",
      "saving model with loss 2.067059278488159\n",
      "Epoch 18 Batch 51500 Loss 2.1359646320343018 in 15.244901\n",
      "Epoch 18 Batch 52000 Loss 2.2247986793518066 in 15.176275\n",
      "saving model with loss 2.2247986793518066\n",
      "Epoch 18 Batch 52500 Loss 2.057952642440796 in 15.240781\n",
      "Epoch 18 Batch 53000 Loss 1.9610316753387451 in 15.152225\n",
      "saving model with loss 1.9610316753387451\n",
      "Epoch 18 Batch 53500 Loss 2.143616199493408 in 15.213542\n",
      "Epoch 18 Batch 54000 Loss 2.110985040664673 in 15.150998\n",
      "saving model with loss 2.110985040664673\n",
      "Epoch 18 Batch 54500 Loss 2.2092738151550293 in 15.242840\n",
      "Epoch 18 Batch 55000 Loss 2.178704261779785 in 15.186774\n",
      "saving model with loss 2.178704261779785\n",
      "Epoch 18 Batch 55500 Loss 2.0771985054016113 in 15.220739\n",
      "Epoch 18 Batch 56000 Loss 2.1154654026031494 in 15.185255\n",
      "saving model with loss 2.1154654026031494\n",
      "Epoch 18 Batch 56500 Loss 2.2214229106903076 in 15.240163\n",
      "Epoch 18 Batch 57000 Loss 2.0196220874786377 in 15.178797\n",
      "saving model with loss 2.0196220874786377\n",
      "Epoch 18 Batch 57500 Loss 2.1691765785217285 in 15.245810\n",
      "Epoch 18 Batch 58000 Loss 2.033193588256836 in 15.181431\n",
      "saving model with loss 2.033193588256836\n",
      "Epoch 18 Batch 58500 Loss 2.272313356399536 in 15.244946\n",
      "Epoch 18 Batch 59000 Loss 2.000476598739624 in 15.178785\n",
      "saving model with loss 2.000476598739624\n",
      "Epoch 18 Batch 59500 Loss 1.9493439197540283 in 15.242398\n",
      "Epoch 18 Batch 60000 Loss 2.1749701499938965 in 15.181523\n",
      "saving model with loss 2.1749701499938965\n",
      "Epoch 18 Batch 60500 Loss 2.2629363536834717 in 15.245579\n",
      "Epoch 18 Batch 61000 Loss 2.2027158737182617 in 15.179922\n",
      "saving model with loss 2.2027158737182617\n",
      "Epoch 18 Batch 61500 Loss 2.091200351715088 in 15.239683\n",
      "Epoch 18 Batch 62000 Loss 1.9615898132324219 in 15.180637\n",
      "saving model with loss 1.9615898132324219\n",
      "Epoch 18 Batch 62500 Loss 2.128993511199951 in 15.247805\n",
      "Epoch 18 Batch 63000 Loss 2.198676586151123 in 15.181343\n",
      "saving model with loss 2.198676586151123\n",
      "Epoch 18 Batch 63500 Loss 2.246901035308838 in 15.208144\n",
      "Epoch 18 Batch 64000 Loss 2.2029271125793457 in 15.177999\n",
      "saving model with loss 2.2029271125793457\n",
      "Epoch 18 Batch 64500 Loss 2.0522961616516113 in 15.233534\n",
      "Epoch 18 Batch 65000 Loss 2.185734987258911 in 15.162801\n",
      "saving model with loss 2.185734987258911\n",
      "Epoch 18 Batch 65500 Loss 2.209139347076416 in 15.248329\n",
      "Epoch 18 Batch 66000 Loss 2.1657893657684326 in 15.176799\n",
      "saving model with loss 2.1657893657684326\n",
      "Epoch 18 Batch 66500 Loss 2.0472140312194824 in 15.241379\n",
      "Epoch 18 Batch 67000 Loss 1.9833214282989502 in 15.164601\n",
      "saving model with loss 1.9833214282989502\n",
      "Epoch 18 Batch 67500 Loss 2.0796799659729004 in 15.251673\n",
      "Epoch 18 Batch 68000 Loss 2.0858755111694336 in 15.164237\n",
      "saving model with loss 2.0858755111694336\n",
      "Epoch 18 Batch 68500 Loss 2.0929176807403564 in 15.240652\n",
      "Epoch 18 Batch 69000 Loss 2.11443829536438 in 15.181182\n",
      "saving model with loss 2.11443829536438\n",
      "Epoch 18 Batch 69500 Loss 2.125406265258789 in 15.239216\n",
      "Epoch 18 Batch 70000 Loss 2.0257601737976074 in 15.187765\n",
      "saving model with loss 2.0257601737976074\n",
      "Epoch 18 Batch 70500 Loss 2.2006783485412598 in 15.240707\n",
      "Epoch 19 Loss 2.1980884075164795\n",
      "Time taken for 1 epoch 2147.872263431549\n",
      "number of batches : 70553\n",
      "Epoch 19 Batch 0 Loss 2.1396801471710205 in 1.176850\n",
      "saving model with loss 2.1396801471710205\n",
      "Epoch 19 Batch 500 Loss 2.012237071990967 in 15.238207\n",
      "Epoch 19 Batch 1000 Loss 2.19657039642334 in 15.160027\n",
      "saving model with loss 2.19657039642334\n",
      "Epoch 19 Batch 1500 Loss 2.006571054458618 in 15.233196\n",
      "Epoch 19 Batch 2000 Loss 2.1727781295776367 in 15.172779\n",
      "saving model with loss 2.1727781295776367\n",
      "Epoch 19 Batch 2500 Loss 2.1605801582336426 in 15.253991\n",
      "Epoch 19 Batch 3000 Loss 2.0060954093933105 in 15.147034\n",
      "saving model with loss 2.0060954093933105\n",
      "Epoch 19 Batch 3500 Loss 2.009982109069824 in 15.242589\n",
      "Epoch 19 Batch 4000 Loss 2.0529558658599854 in 15.183614\n",
      "saving model with loss 2.0529558658599854\n",
      "Epoch 19 Batch 4500 Loss 1.9556633234024048 in 15.245053\n",
      "Epoch 19 Batch 5000 Loss 2.0643982887268066 in 15.182542\n",
      "saving model with loss 2.0643982887268066\n",
      "Epoch 19 Batch 5500 Loss 2.0268852710723877 in 15.238583\n",
      "Epoch 19 Batch 6000 Loss 2.1766133308410645 in 15.182033\n",
      "saving model with loss 2.1766133308410645\n",
      "Epoch 19 Batch 6500 Loss 2.190188407897949 in 15.241155\n",
      "Epoch 19 Batch 7000 Loss 2.1457715034484863 in 15.152296\n",
      "saving model with loss 2.1457715034484863\n",
      "Epoch 19 Batch 7500 Loss 2.119617223739624 in 15.214558\n",
      "Epoch 19 Batch 8000 Loss 2.2324118614196777 in 15.178924\n",
      "saving model with loss 2.2324118614196777\n",
      "Epoch 19 Batch 8500 Loss 2.2078235149383545 in 15.242577\n",
      "Epoch 19 Batch 9000 Loss 2.0291061401367188 in 15.182830\n",
      "saving model with loss 2.0291061401367188\n",
      "Epoch 19 Batch 9500 Loss 2.076624631881714 in 15.244654\n",
      "Epoch 19 Batch 10000 Loss 2.045907735824585 in 15.180246\n",
      "saving model with loss 2.045907735824585\n",
      "Epoch 19 Batch 10500 Loss 2.10465669631958 in 15.240895\n",
      "Epoch 19 Batch 11000 Loss 2.0847887992858887 in 15.174364\n",
      "saving model with loss 2.0847887992858887\n",
      "Epoch 19 Batch 11500 Loss 2.0939877033233643 in 15.251892\n",
      "Epoch 19 Batch 12000 Loss 1.9873154163360596 in 15.179105\n",
      "saving model with loss 1.9873154163360596\n",
      "Epoch 19 Batch 12500 Loss 2.097531318664551 in 15.242083\n",
      "Epoch 19 Batch 13000 Loss 2.1207525730133057 in 15.149869\n",
      "saving model with loss 2.1207525730133057\n",
      "Epoch 19 Batch 13500 Loss 2.2880654335021973 in 15.244534\n",
      "Epoch 19 Batch 14000 Loss 2.0291051864624023 in 15.181952\n",
      "saving model with loss 2.0291051864624023\n",
      "Epoch 19 Batch 14500 Loss 2.0965633392333984 in 15.245999\n",
      "Epoch 19 Batch 15000 Loss 1.962320327758789 in 15.177903\n",
      "saving model with loss 1.962320327758789\n",
      "Epoch 19 Batch 15500 Loss 2.070157289505005 in 15.244701\n",
      "Epoch 19 Batch 16000 Loss 2.0368990898132324 in 15.177343\n",
      "saving model with loss 2.0368990898132324\n",
      "Epoch 19 Batch 16500 Loss 2.1468663215637207 in 15.243597\n",
      "Epoch 19 Batch 17000 Loss 2.1860218048095703 in 15.182616\n",
      "saving model with loss 2.1860218048095703\n",
      "Epoch 19 Batch 17500 Loss 2.105529546737671 in 15.226127\n",
      "Epoch 19 Batch 18000 Loss 2.1135571002960205 in 15.182935\n",
      "saving model with loss 2.1135571002960205\n",
      "Epoch 19 Batch 18500 Loss 2.2108237743377686 in 15.241549\n",
      "Epoch 19 Batch 19000 Loss 2.0649266242980957 in 15.179657\n",
      "saving model with loss 2.0649266242980957\n",
      "Epoch 19 Batch 19500 Loss 2.1779494285583496 in 15.242808\n",
      "Epoch 19 Batch 20000 Loss 2.104574680328369 in 15.155498\n",
      "saving model with loss 2.104574680328369\n",
      "Epoch 19 Batch 20500 Loss 2.1892008781433105 in 15.239046\n",
      "Epoch 19 Batch 21000 Loss 1.9633868932724 in 15.151637\n",
      "saving model with loss 1.9633868932724\n",
      "Epoch 19 Batch 21500 Loss 1.895094871520996 in 15.246069\n",
      "Epoch 19 Batch 22000 Loss 2.1486258506774902 in 15.181866\n",
      "saving model with loss 2.1486258506774902\n",
      "Epoch 19 Batch 22500 Loss 2.1226389408111572 in 15.238887\n",
      "Epoch 19 Batch 23000 Loss 2.0959978103637695 in 15.184119\n",
      "saving model with loss 2.0959978103637695\n",
      "Epoch 19 Batch 23500 Loss 2.0827107429504395 in 15.274724\n",
      "Epoch 19 Batch 24000 Loss 2.0631585121154785 in 15.182260\n",
      "saving model with loss 2.0631585121154785\n",
      "Epoch 19 Batch 24500 Loss 2.183986186981201 in 15.240643\n",
      "Epoch 19 Batch 25000 Loss 2.0804524421691895 in 15.151048\n",
      "saving model with loss 2.0804524421691895\n",
      "Epoch 19 Batch 25500 Loss 2.3002607822418213 in 15.242593\n",
      "Epoch 19 Batch 26000 Loss 2.021714448928833 in 15.181138\n",
      "saving model with loss 2.021714448928833\n",
      "Epoch 19 Batch 26500 Loss 2.1705076694488525 in 15.245910\n",
      "Epoch 19 Batch 27000 Loss 2.131687641143799 in 15.177512\n",
      "saving model with loss 2.131687641143799\n",
      "Epoch 19 Batch 27500 Loss 2.1355690956115723 in 15.241072\n",
      "Epoch 19 Batch 28000 Loss 2.0500149726867676 in 15.153102\n",
      "saving model with loss 2.0500149726867676\n",
      "Epoch 19 Batch 28500 Loss 1.974958062171936 in 15.241446\n",
      "Epoch 19 Batch 29000 Loss 2.0865702629089355 in 15.185447\n",
      "saving model with loss 2.0865702629089355\n",
      "Epoch 19 Batch 29500 Loss 2.047208309173584 in 15.237941\n",
      "Epoch 19 Batch 30000 Loss 2.210801601409912 in 15.151689\n",
      "saving model with loss 2.210801601409912\n",
      "Epoch 19 Batch 30500 Loss 2.10906720161438 in 15.242200\n",
      "Epoch 19 Batch 31000 Loss 2.1244993209838867 in 15.184233\n",
      "saving model with loss 2.1244993209838867\n",
      "Epoch 19 Batch 31500 Loss 2.253413677215576 in 15.243230\n",
      "Epoch 19 Batch 32000 Loss 2.0958690643310547 in 15.150130\n",
      "saving model with loss 2.0958690643310547\n",
      "Epoch 19 Batch 32500 Loss 2.277888536453247 in 15.242444\n",
      "Epoch 19 Batch 33000 Loss 2.033597230911255 in 15.170469\n",
      "saving model with loss 2.033597230911255\n",
      "Epoch 19 Batch 33500 Loss 2.120762586593628 in 15.239202\n",
      "Epoch 19 Batch 34000 Loss 2.0071706771850586 in 15.185105\n",
      "saving model with loss 2.0071706771850586\n",
      "Epoch 19 Batch 34500 Loss 2.20218563079834 in 15.210454\n",
      "Epoch 19 Batch 35000 Loss 2.145986557006836 in 15.146039\n",
      "saving model with loss 2.145986557006836\n",
      "Epoch 19 Batch 35500 Loss 2.137972354888916 in 15.245063\n",
      "Epoch 19 Batch 36000 Loss 2.0895988941192627 in 15.153939\n",
      "saving model with loss 2.0895988941192627\n",
      "Epoch 19 Batch 36500 Loss 2.156217098236084 in 15.242951\n",
      "Epoch 19 Batch 37000 Loss 2.1387059688568115 in 15.180126\n",
      "saving model with loss 2.1387059688568115\n",
      "Epoch 19 Batch 37500 Loss 2.1305928230285645 in 15.240844\n",
      "Epoch 19 Batch 38000 Loss 2.0355191230773926 in 15.183078\n",
      "saving model with loss 2.0355191230773926\n",
      "Epoch 19 Batch 38500 Loss 2.129842758178711 in 15.240561\n",
      "Epoch 19 Batch 39000 Loss 2.1941945552825928 in 15.168015\n",
      "saving model with loss 2.1941945552825928\n",
      "Epoch 19 Batch 39500 Loss 1.8864829540252686 in 15.228541\n",
      "Epoch 19 Batch 40000 Loss 2.0582127571105957 in 15.180153\n",
      "saving model with loss 2.0582127571105957\n",
      "Epoch 19 Batch 40500 Loss 2.126415729522705 in 15.213315\n",
      "Epoch 19 Batch 41000 Loss 2.1229300498962402 in 15.181381\n",
      "saving model with loss 2.1229300498962402\n",
      "Epoch 19 Batch 41500 Loss 2.166203737258911 in 15.240957\n",
      "Epoch 19 Batch 42000 Loss 2.055116891860962 in 15.183234\n",
      "saving model with loss 2.055116891860962\n",
      "Epoch 19 Batch 42500 Loss 2.133511543273926 in 15.240862\n",
      "Epoch 19 Batch 43000 Loss 2.112110137939453 in 15.178018\n",
      "saving model with loss 2.112110137939453\n",
      "Epoch 19 Batch 43500 Loss 1.9348958730697632 in 15.248917\n",
      "Epoch 19 Batch 44000 Loss 2.2099690437316895 in 15.192143\n",
      "saving model with loss 2.2099690437316895\n",
      "Epoch 19 Batch 44500 Loss 2.070308208465576 in 15.353465\n",
      "Epoch 19 Batch 45000 Loss 1.969032645225525 in 15.175293\n",
      "saving model with loss 1.969032645225525\n",
      "Epoch 19 Batch 45500 Loss 2.0984303951263428 in 15.248827\n",
      "Epoch 19 Batch 46000 Loss 2.06158709526062 in 15.157402\n",
      "saving model with loss 2.06158709526062\n",
      "Epoch 19 Batch 46500 Loss 2.023491621017456 in 15.234327\n",
      "Epoch 19 Batch 47000 Loss 2.0630669593811035 in 15.177523\n",
      "saving model with loss 2.0630669593811035\n",
      "Epoch 19 Batch 47500 Loss 2.117921829223633 in 15.248395\n",
      "Epoch 19 Batch 48000 Loss 2.0685932636260986 in 15.181301\n",
      "saving model with loss 2.0685932636260986\n",
      "Epoch 19 Batch 48500 Loss 2.1209893226623535 in 15.240311\n",
      "Epoch 19 Batch 49000 Loss 2.06451153755188 in 15.177616\n",
      "saving model with loss 2.06451153755188\n",
      "Epoch 19 Batch 49500 Loss 2.167569875717163 in 15.245357\n",
      "Epoch 19 Batch 50000 Loss 2.1558055877685547 in 15.179497\n",
      "saving model with loss 2.1558055877685547\n",
      "Epoch 19 Batch 50500 Loss 2.0384597778320312 in 15.217197\n",
      "Epoch 19 Batch 51000 Loss 2.0142769813537598 in 15.148280\n",
      "saving model with loss 2.0142769813537598\n",
      "Epoch 19 Batch 51500 Loss 2.103757858276367 in 15.243695\n",
      "Epoch 19 Batch 52000 Loss 2.108067274093628 in 15.177871\n",
      "saving model with loss 2.108067274093628\n",
      "Epoch 19 Batch 52500 Loss 1.9758532047271729 in 15.216144\n",
      "Epoch 19 Batch 53000 Loss 2.047847270965576 in 15.153358\n",
      "saving model with loss 2.047847270965576\n",
      "Epoch 19 Batch 53500 Loss 2.0861964225769043 in 15.240180\n",
      "Epoch 19 Batch 54000 Loss 2.114229917526245 in 15.178121\n",
      "saving model with loss 2.114229917526245\n",
      "Epoch 19 Batch 54500 Loss 2.114617109298706 in 15.244725\n",
      "Epoch 19 Batch 55000 Loss 2.021139144897461 in 15.181977\n",
      "saving model with loss 2.021139144897461\n",
      "Epoch 19 Batch 55500 Loss 2.1073832511901855 in 15.245231\n",
      "Epoch 19 Batch 56000 Loss 2.0121753215789795 in 15.150094\n",
      "saving model with loss 2.0121753215789795\n",
      "Epoch 19 Batch 56500 Loss 2.118086814880371 in 15.239878\n",
      "Epoch 19 Batch 57000 Loss 2.059598445892334 in 15.182979\n",
      "saving model with loss 2.059598445892334\n",
      "Epoch 19 Batch 57500 Loss 2.092045307159424 in 15.241127\n",
      "Epoch 19 Batch 58000 Loss 2.2221200466156006 in 15.182681\n",
      "saving model with loss 2.2221200466156006\n",
      "Epoch 19 Batch 58500 Loss 2.1444923877716064 in 15.328943\n",
      "Epoch 19 Batch 59000 Loss 2.1941421031951904 in 15.369484\n",
      "saving model with loss 2.1941421031951904\n",
      "Epoch 19 Batch 59500 Loss 2.1653990745544434 in 15.243320\n",
      "Epoch 19 Batch 60000 Loss 2.1257197856903076 in 15.155573\n",
      "saving model with loss 2.1257197856903076\n",
      "Epoch 19 Batch 60500 Loss 2.098508834838867 in 15.233944\n",
      "Epoch 19 Batch 61000 Loss 2.12689471244812 in 15.181364\n",
      "saving model with loss 2.12689471244812\n",
      "Epoch 19 Batch 61500 Loss 2.107787609100342 in 15.247641\n",
      "Epoch 19 Batch 62000 Loss 2.1576216220855713 in 15.185063\n",
      "saving model with loss 2.1576216220855713\n",
      "Epoch 19 Batch 62500 Loss 2.125607967376709 in 15.239311\n",
      "Epoch 19 Batch 63000 Loss 2.256866455078125 in 15.176425\n",
      "saving model with loss 2.256866455078125\n",
      "Epoch 19 Batch 63500 Loss 2.1964168548583984 in 15.245321\n",
      "Epoch 19 Batch 64000 Loss 2.156630754470825 in 15.177978\n",
      "saving model with loss 2.156630754470825\n",
      "Epoch 19 Batch 64500 Loss 2.0201478004455566 in 15.233678\n",
      "Epoch 19 Batch 65000 Loss 2.2162139415740967 in 15.163178\n",
      "saving model with loss 2.2162139415740967\n",
      "Epoch 19 Batch 65500 Loss 2.2001068592071533 in 15.242768\n",
      "Epoch 19 Batch 66000 Loss 2.147516965866089 in 15.181600\n",
      "saving model with loss 2.147516965866089\n",
      "Epoch 19 Batch 66500 Loss 2.156186580657959 in 15.243946\n",
      "Epoch 19 Batch 67000 Loss 2.260206460952759 in 15.183155\n",
      "saving model with loss 2.260206460952759\n",
      "Epoch 19 Batch 67500 Loss 2.1034302711486816 in 15.207699\n",
      "Epoch 19 Batch 68000 Loss 2.196683168411255 in 15.182951\n",
      "saving model with loss 2.196683168411255\n",
      "Epoch 19 Batch 68500 Loss 2.1259498596191406 in 15.241356\n",
      "Epoch 19 Batch 69000 Loss 2.1030685901641846 in 15.183713\n",
      "saving model with loss 2.1030685901641846\n",
      "Epoch 19 Batch 69500 Loss 2.224433660507202 in 15.244999\n",
      "Epoch 19 Batch 70000 Loss 1.9761031866073608 in 15.179132\n",
      "saving model with loss 1.9761031866073608\n",
      "Epoch 19 Batch 70500 Loss 2.0870680809020996 in 15.243127\n",
      "Epoch 20 Loss 2.202040672302246\n",
      "Time taken for 1 epoch 2147.3880281448364\n",
      "number of batches : 70553\n",
      "Epoch 20 Batch 0 Loss 2.0399484634399414 in 1.156469\n",
      "saving model with loss 2.0399484634399414\n",
      "Epoch 20 Batch 500 Loss 2.138728618621826 in 15.234468\n",
      "Epoch 20 Batch 1000 Loss 2.269423723220825 in 15.184610\n",
      "saving model with loss 2.269423723220825\n",
      "Epoch 20 Batch 1500 Loss 2.0403714179992676 in 15.243413\n",
      "Epoch 20 Batch 2000 Loss 2.120483636856079 in 15.179083\n",
      "saving model with loss 2.120483636856079\n",
      "Epoch 20 Batch 2500 Loss 2.2718796730041504 in 15.240333\n",
      "Epoch 20 Batch 3000 Loss 2.1838736534118652 in 15.151339\n",
      "saving model with loss 2.1838736534118652\n",
      "Epoch 20 Batch 3500 Loss 2.143796920776367 in 15.241453\n",
      "Epoch 20 Batch 4000 Loss 2.1403861045837402 in 15.186039\n",
      "saving model with loss 2.1403861045837402\n",
      "Epoch 20 Batch 4500 Loss 2.1021151542663574 in 15.239920\n",
      "Epoch 20 Batch 5000 Loss 2.1232552528381348 in 15.162884\n",
      "saving model with loss 2.1232552528381348\n",
      "Epoch 20 Batch 5500 Loss 2.209808826446533 in 15.227984\n",
      "Epoch 20 Batch 6000 Loss 2.205329179763794 in 15.153030\n",
      "saving model with loss 2.205329179763794\n",
      "Epoch 20 Batch 6500 Loss 2.3379170894622803 in 15.246275\n",
      "Epoch 20 Batch 7000 Loss 2.0951273441314697 in 15.149219\n",
      "saving model with loss 2.0951273441314697\n",
      "Epoch 20 Batch 7500 Loss 2.102180004119873 in 15.242612\n",
      "Epoch 20 Batch 8000 Loss 2.2048277854919434 in 15.181092\n",
      "saving model with loss 2.2048277854919434\n",
      "Epoch 20 Batch 8500 Loss 2.0366063117980957 in 15.242714\n",
      "Epoch 20 Batch 9000 Loss 2.1192433834075928 in 15.182361\n",
      "saving model with loss 2.1192433834075928\n",
      "Epoch 20 Batch 9500 Loss 2.1497278213500977 in 15.238447\n",
      "Epoch 20 Batch 10000 Loss 2.2808799743652344 in 15.182173\n",
      "saving model with loss 2.2808799743652344\n",
      "Epoch 20 Batch 10500 Loss 2.1312897205352783 in 15.695770\n",
      "Epoch 20 Batch 11000 Loss 2.0943119525909424 in 16.652195\n",
      "saving model with loss 2.0943119525909424\n",
      "Epoch 20 Batch 11500 Loss 2.187511682510376 in 15.943124\n",
      "Epoch 20 Batch 12000 Loss 2.2597787380218506 in 16.807776\n",
      "saving model with loss 2.2597787380218506\n",
      "Epoch 20 Batch 12500 Loss 2.11540150642395 in 16.844927\n",
      "Epoch 20 Batch 13000 Loss 2.2569730281829834 in 16.730094\n",
      "saving model with loss 2.2569730281829834\n",
      "Epoch 20 Batch 13500 Loss 2.254605770111084 in 16.464815\n",
      "Epoch 20 Batch 14000 Loss 2.117432117462158 in 15.569562\n",
      "saving model with loss 2.117432117462158\n",
      "Epoch 20 Batch 14500 Loss 2.1336798667907715 in 15.305645\n",
      "Epoch 20 Batch 15000 Loss 2.2950644493103027 in 15.263524\n",
      "saving model with loss 2.2950644493103027\n",
      "Epoch 20 Batch 15500 Loss 2.170818328857422 in 15.762444\n",
      "Epoch 20 Batch 16000 Loss 2.197032928466797 in 15.693395\n",
      "saving model with loss 2.197032928466797\n",
      "Epoch 20 Batch 16500 Loss 2.0545506477355957 in 15.660542\n",
      "Epoch 20 Batch 17000 Loss 2.211932420730591 in 15.450284\n",
      "saving model with loss 2.211932420730591\n",
      "Epoch 20 Batch 17500 Loss 2.1538450717926025 in 15.770413\n",
      "Epoch 20 Batch 18000 Loss 2.0047218799591064 in 15.323760\n",
      "saving model with loss 2.0047218799591064\n",
      "Epoch 20 Batch 18500 Loss 2.2089738845825195 in 15.260252\n",
      "Epoch 20 Batch 19000 Loss 2.2812533378601074 in 15.173437\n",
      "saving model with loss 2.2812533378601074\n",
      "Epoch 20 Batch 19500 Loss 2.2724051475524902 in 15.243679\n",
      "Epoch 20 Batch 20000 Loss 2.2822208404541016 in 15.155054\n",
      "saving model with loss 2.2822208404541016\n",
      "Epoch 20 Batch 20500 Loss 2.0238938331604004 in 15.244416\n",
      "Epoch 20 Batch 21000 Loss 2.2919650077819824 in 15.186153\n",
      "saving model with loss 2.2919650077819824\n",
      "Epoch 20 Batch 21500 Loss 2.0711817741394043 in 15.227040\n",
      "Epoch 20 Batch 22000 Loss 2.1827874183654785 in 15.167133\n",
      "saving model with loss 2.1827874183654785\n",
      "Epoch 20 Batch 22500 Loss 2.2277870178222656 in 15.255272\n",
      "Epoch 20 Batch 23000 Loss 2.206652879714966 in 15.174725\n",
      "saving model with loss 2.206652879714966\n",
      "Epoch 20 Batch 23500 Loss 2.160245180130005 in 15.222394\n",
      "Epoch 20 Batch 24000 Loss 2.0856311321258545 in 15.177931\n",
      "saving model with loss 2.0856311321258545\n",
      "Epoch 20 Batch 24500 Loss 2.1760497093200684 in 15.236524\n",
      "Epoch 20 Batch 25000 Loss 2.3440890312194824 in 15.161659\n",
      "saving model with loss 2.3440890312194824\n",
      "Epoch 20 Batch 25500 Loss 2.2854294776916504 in 15.264367\n",
      "Epoch 20 Batch 26000 Loss 2.166790008544922 in 15.199089\n",
      "saving model with loss 2.166790008544922\n",
      "Epoch 20 Batch 26500 Loss 2.17638897895813 in 15.271053\n",
      "Epoch 20 Batch 27000 Loss 2.0462546348571777 in 15.602176\n",
      "saving model with loss 2.0462546348571777\n",
      "Epoch 20 Batch 27500 Loss 2.0476841926574707 in 15.521303\n",
      "Epoch 20 Batch 28000 Loss 2.2543680667877197 in 15.371773\n",
      "saving model with loss 2.2543680667877197\n",
      "Epoch 20 Batch 28500 Loss 2.2546439170837402 in 15.253451\n",
      "Epoch 20 Batch 29000 Loss 2.379420757293701 in 15.151577\n",
      "saving model with loss 2.379420757293701\n",
      "Epoch 20 Batch 29500 Loss 2.29198956489563 in 15.232672\n",
      "Epoch 20 Batch 30000 Loss 2.3416786193847656 in 15.188363\n",
      "saving model with loss 2.3416786193847656\n",
      "Epoch 20 Batch 30500 Loss 2.2647719383239746 in 15.180619\n",
      "Epoch 20 Batch 31000 Loss 2.190187931060791 in 15.076491\n",
      "saving model with loss 2.190187931060791\n",
      "Epoch 20 Batch 31500 Loss 2.0460221767425537 in 15.126070\n",
      "Epoch 20 Batch 32000 Loss 2.166198492050171 in 15.063723\n",
      "saving model with loss 2.166198492050171\n",
      "Epoch 20 Batch 32500 Loss 2.2682604789733887 in 15.134452\n",
      "Epoch 20 Batch 33000 Loss 2.096907138824463 in 15.078880\n",
      "saving model with loss 2.096907138824463\n",
      "Epoch 20 Batch 33500 Loss 2.13438081741333 in 15.225035\n",
      "Epoch 20 Batch 34000 Loss 2.240161418914795 in 15.076856\n",
      "saving model with loss 2.240161418914795\n",
      "Epoch 20 Batch 34500 Loss 2.03125 in 15.138441\n",
      "Epoch 20 Batch 35000 Loss 2.288444995880127 in 15.060597\n",
      "saving model with loss 2.288444995880127\n",
      "Epoch 20 Batch 35500 Loss 2.321985960006714 in 15.132251\n",
      "Epoch 20 Batch 36000 Loss 2.1288089752197266 in 15.084395\n",
      "saving model with loss 2.1288089752197266\n",
      "Epoch 20 Batch 36500 Loss 2.3001110553741455 in 15.242230\n",
      "Epoch 20 Batch 37000 Loss 2.063573122024536 in 15.080046\n",
      "saving model with loss 2.063573122024536\n",
      "Epoch 20 Batch 37500 Loss 2.242108106613159 in 15.137012\n",
      "Epoch 20 Batch 38000 Loss 2.169768810272217 in 15.084030\n",
      "saving model with loss 2.169768810272217\n",
      "Epoch 20 Batch 38500 Loss 2.226895332336426 in 15.141635\n",
      "Epoch 20 Batch 39000 Loss 2.137763500213623 in 15.115758\n",
      "saving model with loss 2.137763500213623\n",
      "Epoch 20 Batch 39500 Loss 2.18396258354187 in 15.259508\n",
      "Epoch 20 Batch 40000 Loss 2.0983049869537354 in 15.155575\n",
      "saving model with loss 2.0983049869537354\n",
      "Epoch 20 Batch 40500 Loss 2.326749801635742 in 15.123053\n",
      "Epoch 20 Batch 41000 Loss 2.175772190093994 in 15.067319\n",
      "saving model with loss 2.175772190093994\n",
      "Epoch 20 Batch 41500 Loss 2.2436413764953613 in 15.117384\n",
      "Epoch 20 Batch 42000 Loss 2.3277995586395264 in 15.042743\n",
      "saving model with loss 2.3277995586395264\n",
      "Epoch 20 Batch 42500 Loss 2.266000747680664 in 15.212358\n",
      "Epoch 20 Batch 43000 Loss 2.300994873046875 in 15.067076\n",
      "saving model with loss 2.300994873046875\n",
      "Epoch 20 Batch 43500 Loss 2.342254877090454 in 15.140937\n",
      "Epoch 20 Batch 44000 Loss 2.26285457611084 in 15.148515\n",
      "saving model with loss 2.26285457611084\n",
      "Epoch 20 Batch 44500 Loss 2.109712600708008 in 15.138443\n",
      "Epoch 20 Batch 45000 Loss 1.9906227588653564 in 15.066638\n",
      "saving model with loss 1.9906227588653564\n",
      "Epoch 20 Batch 45500 Loss 2.3226730823516846 in 15.147424\n",
      "Epoch 20 Batch 46000 Loss 2.1155269145965576 in 15.054765\n",
      "saving model with loss 2.1155269145965576\n",
      "Epoch 20 Batch 46500 Loss 2.1961424350738525 in 15.139606\n",
      "Epoch 20 Batch 47000 Loss 2.3070147037506104 in 15.089149\n",
      "saving model with loss 2.3070147037506104\n",
      "Epoch 20 Batch 47500 Loss 2.241471290588379 in 15.156621\n",
      "Epoch 20 Batch 48000 Loss 2.2043204307556152 in 15.081844\n",
      "saving model with loss 2.2043204307556152\n",
      "Epoch 20 Batch 48500 Loss 2.067234754562378 in 15.129099\n",
      "Epoch 20 Batch 49000 Loss 2.2571074962615967 in 15.080726\n",
      "saving model with loss 2.2571074962615967\n",
      "Epoch 20 Batch 49500 Loss 2.138772487640381 in 15.151558\n",
      "Epoch 20 Batch 50000 Loss 2.229159355163574 in 15.068268\n",
      "saving model with loss 2.229159355163574\n",
      "Epoch 20 Batch 50500 Loss 2.1419360637664795 in 15.157543\n",
      "Epoch 20 Batch 51000 Loss 2.1487250328063965 in 15.105610\n",
      "saving model with loss 2.1487250328063965\n",
      "Epoch 20 Batch 51500 Loss 2.2145562171936035 in 15.175971\n",
      "Epoch 20 Batch 52000 Loss 2.225907802581787 in 15.104541\n",
      "saving model with loss 2.225907802581787\n",
      "Epoch 20 Batch 52500 Loss 2.07242751121521 in 15.224060\n",
      "Epoch 20 Batch 53000 Loss 2.216644287109375 in 15.077457\n",
      "saving model with loss 2.216644287109375\n",
      "Epoch 20 Batch 53500 Loss 2.129234790802002 in 15.164223\n",
      "Epoch 20 Batch 54000 Loss 2.1037404537200928 in 15.121355\n",
      "saving model with loss 2.1037404537200928\n",
      "Epoch 20 Batch 54500 Loss 2.1515417098999023 in 15.122473\n",
      "Epoch 20 Batch 55000 Loss 2.074291706085205 in 15.036936\n",
      "saving model with loss 2.074291706085205\n",
      "Epoch 20 Batch 55500 Loss 2.0824451446533203 in 15.149715\n",
      "Epoch 20 Batch 56000 Loss 2.2389492988586426 in 15.114885\n",
      "saving model with loss 2.2389492988586426\n",
      "Epoch 20 Batch 56500 Loss 2.1987204551696777 in 15.160384\n",
      "Epoch 20 Batch 57000 Loss 2.2227277755737305 in 15.042315\n",
      "saving model with loss 2.2227277755737305\n",
      "Epoch 20 Batch 57500 Loss 1.9911673069000244 in 15.117375\n",
      "Epoch 20 Batch 58000 Loss 2.469299793243408 in 15.035437\n",
      "saving model with loss 2.469299793243408\n",
      "Epoch 20 Batch 58500 Loss 2.4492154121398926 in 15.101506\n",
      "Epoch 20 Batch 59000 Loss 2.227626323699951 in 15.030336\n",
      "saving model with loss 2.227626323699951\n",
      "Epoch 20 Batch 59500 Loss 2.239147663116455 in 15.114965\n",
      "Epoch 20 Batch 60000 Loss 2.2025227546691895 in 15.042923\n",
      "saving model with loss 2.2025227546691895\n",
      "Epoch 20 Batch 60500 Loss 2.134138822555542 in 15.112466\n",
      "Epoch 20 Batch 61000 Loss 2.276155948638916 in 15.041439\n",
      "saving model with loss 2.276155948638916\n",
      "Epoch 20 Batch 61500 Loss 2.3568031787872314 in 15.102267\n",
      "Epoch 20 Batch 62000 Loss 2.360029458999634 in 15.044175\n",
      "saving model with loss 2.360029458999634\n",
      "Epoch 20 Batch 62500 Loss 2.134925365447998 in 15.223934\n",
      "Epoch 20 Batch 63000 Loss 2.1352105140686035 in 15.127112\n",
      "saving model with loss 2.1352105140686035\n",
      "Epoch 20 Batch 63500 Loss 2.149573564529419 in 15.112123\n",
      "Epoch 20 Batch 64000 Loss 2.1066479682922363 in 15.049110\n",
      "saving model with loss 2.1066479682922363\n",
      "Epoch 20 Batch 64500 Loss 2.2703890800476074 in 15.088985\n",
      "Epoch 20 Batch 65000 Loss 2.2178916931152344 in 15.042343\n",
      "saving model with loss 2.2178916931152344\n",
      "Epoch 20 Batch 65500 Loss 2.248751401901245 in 15.197856\n",
      "Epoch 20 Batch 66000 Loss 2.2067220211029053 in 15.115030\n",
      "saving model with loss 2.2067220211029053\n",
      "Epoch 20 Batch 66500 Loss 2.1284308433532715 in 15.165887\n",
      "Epoch 20 Batch 67000 Loss 2.1615347862243652 in 15.036524\n",
      "saving model with loss 2.1615347862243652\n",
      "Epoch 20 Batch 67500 Loss 2.2734718322753906 in 15.106684\n",
      "Epoch 20 Batch 68000 Loss 2.230391263961792 in 15.035967\n",
      "saving model with loss 2.230391263961792\n",
      "Epoch 20 Batch 68500 Loss 2.260892868041992 in 15.125188\n",
      "Epoch 20 Batch 69000 Loss 2.2951481342315674 in 15.052154\n",
      "saving model with loss 2.2951481342315674\n",
      "Epoch 20 Batch 69500 Loss 2.357117176055908 in 15.095458\n",
      "Epoch 20 Batch 70000 Loss 2.175642490386963 in 15.022699\n",
      "saving model with loss 2.175642490386963\n",
      "Epoch 20 Batch 70500 Loss 2.2182693481445312 in 15.118021\n",
      "Epoch 21 Loss 2.2398204803466797\n",
      "Time taken for 1 epoch 2151.8205621242523\n",
      "number of batches : 70553\n",
      "Epoch 21 Batch 0 Loss 2.046809673309326 in 1.181840\n",
      "saving model with loss 2.046809673309326\n",
      "Epoch 21 Batch 500 Loss 2.160681962966919 in 15.109527\n",
      "Epoch 21 Batch 1000 Loss 2.155806303024292 in 15.711612\n",
      "saving model with loss 2.155806303024292\n",
      "Epoch 21 Batch 1500 Loss 2.123274326324463 in 15.755339\n",
      "Epoch 21 Batch 2000 Loss 2.1704864501953125 in 16.618318\n",
      "saving model with loss 2.1704864501953125\n",
      "Epoch 21 Batch 2500 Loss 2.316859722137451 in 16.624318\n",
      "Epoch 21 Batch 3000 Loss 2.179882764816284 in 16.559281\n",
      "saving model with loss 2.179882764816284\n",
      "Epoch 21 Batch 3500 Loss 2.2218222618103027 in 16.266788\n",
      "Epoch 21 Batch 4000 Loss 2.284369945526123 in 15.423239\n",
      "saving model with loss 2.284369945526123\n",
      "Epoch 21 Batch 4500 Loss 2.19435715675354 in 15.136566\n",
      "Epoch 21 Batch 5000 Loss 2.2240359783172607 in 15.381963\n",
      "saving model with loss 2.2240359783172607\n",
      "Epoch 21 Batch 5500 Loss 2.0866751670837402 in 16.458997\n",
      "Epoch 21 Batch 6000 Loss 2.116640090942383 in 16.290748\n",
      "saving model with loss 2.116640090942383\n",
      "Epoch 21 Batch 6500 Loss 2.19378662109375 in 16.098072\n",
      "Epoch 21 Batch 7000 Loss 2.2372734546661377 in 16.647612\n",
      "saving model with loss 2.2372734546661377\n",
      "Epoch 21 Batch 7500 Loss 2.3572185039520264 in 15.634512\n",
      "Epoch 21 Batch 8000 Loss 2.135039806365967 in 15.379040\n",
      "saving model with loss 2.135039806365967\n",
      "Epoch 21 Batch 8500 Loss 2.292224407196045 in 15.755427\n",
      "Epoch 21 Batch 9000 Loss 2.1390979290008545 in 15.406523\n",
      "saving model with loss 2.1390979290008545\n",
      "Epoch 21 Batch 9500 Loss 2.060462713241577 in 15.777186\n",
      "Epoch 21 Batch 10000 Loss 2.2369039058685303 in 15.406803\n",
      "saving model with loss 2.2369039058685303\n",
      "Epoch 21 Batch 10500 Loss 2.227006435394287 in 15.341083\n",
      "Epoch 21 Batch 11000 Loss 2.1988492012023926 in 15.396292\n",
      "saving model with loss 2.1988492012023926\n",
      "Epoch 21 Batch 11500 Loss 2.192004680633545 in 15.413993\n",
      "Epoch 21 Batch 12000 Loss 2.0694432258605957 in 15.856942\n",
      "saving model with loss 2.0694432258605957\n",
      "Epoch 21 Batch 12500 Loss 2.196836471557617 in 16.107545\n",
      "Epoch 21 Batch 13000 Loss 2.2604706287384033 in 15.252607\n",
      "saving model with loss 2.2604706287384033\n",
      "Epoch 21 Batch 13500 Loss 2.1218514442443848 in 15.156236\n",
      "Epoch 21 Batch 14000 Loss 2.2070658206939697 in 15.080329\n",
      "saving model with loss 2.2070658206939697\n",
      "Epoch 21 Batch 14500 Loss 2.187793731689453 in 15.136665\n",
      "Epoch 21 Batch 15000 Loss 2.396949291229248 in 15.091181\n",
      "saving model with loss 2.396949291229248\n",
      "Epoch 21 Batch 15500 Loss 2.303452968597412 in 15.165080\n",
      "Epoch 21 Batch 16000 Loss 2.183896541595459 in 15.089503\n",
      "saving model with loss 2.183896541595459\n",
      "Epoch 21 Batch 16500 Loss 2.3439886569976807 in 15.155129\n",
      "Epoch 21 Batch 17000 Loss 2.210165023803711 in 15.085255\n",
      "saving model with loss 2.210165023803711\n",
      "Epoch 21 Batch 17500 Loss 2.2548718452453613 in 15.152676\n",
      "Epoch 21 Batch 18000 Loss 2.100806951522827 in 15.100810\n",
      "saving model with loss 2.100806951522827\n",
      "Epoch 21 Batch 18500 Loss 2.1371960639953613 in 15.168092\n",
      "Epoch 21 Batch 19000 Loss 2.236663341522217 in 15.086748\n",
      "saving model with loss 2.236663341522217\n",
      "Epoch 21 Batch 19500 Loss 2.146610736846924 in 15.154663\n",
      "Epoch 21 Batch 20000 Loss 2.156066417694092 in 15.078904\n",
      "saving model with loss 2.156066417694092\n",
      "Epoch 21 Batch 20500 Loss 2.0471079349517822 in 15.145858\n",
      "Epoch 21 Batch 21000 Loss 2.3233115673065186 in 15.100325\n",
      "saving model with loss 2.3233115673065186\n",
      "Epoch 21 Batch 21500 Loss 2.14340877532959 in 15.152903\n",
      "Epoch 21 Batch 22000 Loss 2.210066556930542 in 15.093094\n",
      "saving model with loss 2.210066556930542\n",
      "Epoch 21 Batch 22500 Loss 2.237976312637329 in 15.160910\n",
      "Epoch 21 Batch 23000 Loss 2.2976720333099365 in 15.086167\n",
      "saving model with loss 2.2976720333099365\n",
      "Epoch 21 Batch 23500 Loss 2.0391690731048584 in 15.169815\n",
      "Epoch 21 Batch 24000 Loss 2.226860761642456 in 15.077398\n",
      "saving model with loss 2.226860761642456\n",
      "Epoch 21 Batch 24500 Loss 2.201040267944336 in 15.151778\n",
      "Epoch 21 Batch 25000 Loss 2.0945346355438232 in 15.090215\n",
      "saving model with loss 2.0945346355438232\n",
      "Epoch 21 Batch 25500 Loss 2.270479679107666 in 15.145558\n",
      "Epoch 21 Batch 26000 Loss 2.2246601581573486 in 15.082539\n",
      "saving model with loss 2.2246601581573486\n",
      "Epoch 21 Batch 26500 Loss 2.389446973800659 in 15.159360\n",
      "Epoch 21 Batch 27000 Loss 2.332292079925537 in 15.086664\n",
      "saving model with loss 2.332292079925537\n",
      "Epoch 21 Batch 27500 Loss 2.304356575012207 in 15.149832\n",
      "Epoch 21 Batch 28000 Loss 2.2395737171173096 in 15.090121\n",
      "saving model with loss 2.2395737171173096\n",
      "Epoch 21 Batch 28500 Loss 2.2139925956726074 in 15.145292\n",
      "Epoch 21 Batch 29000 Loss 2.2008719444274902 in 15.069112\n",
      "saving model with loss 2.2008719444274902\n",
      "Epoch 21 Batch 29500 Loss 2.206298589706421 in 15.149840\n",
      "Epoch 21 Batch 30000 Loss 2.2515013217926025 in 15.077544\n",
      "saving model with loss 2.2515013217926025\n",
      "Epoch 21 Batch 30500 Loss 2.12062668800354 in 15.140676\n",
      "Epoch 21 Batch 31000 Loss 2.2776036262512207 in 15.065150\n",
      "saving model with loss 2.2776036262512207\n",
      "Epoch 21 Batch 31500 Loss 2.073970317840576 in 15.152429\n",
      "Epoch 21 Batch 32000 Loss 2.2838501930236816 in 15.077156\n",
      "saving model with loss 2.2838501930236816\n",
      "Epoch 21 Batch 32500 Loss 2.2617688179016113 in 15.150925\n",
      "Epoch 21 Batch 33000 Loss 2.5179638862609863 in 15.070657\n",
      "saving model with loss 2.5179638862609863\n",
      "Epoch 21 Batch 33500 Loss 2.3540282249450684 in 15.159516\n",
      "Epoch 21 Batch 34000 Loss 2.3264503479003906 in 15.092124\n",
      "saving model with loss 2.3264503479003906\n",
      "Epoch 21 Batch 34500 Loss 2.2213244438171387 in 15.146886\n",
      "Epoch 21 Batch 35000 Loss 2.251737117767334 in 15.096942\n",
      "saving model with loss 2.251737117767334\n",
      "Epoch 21 Batch 35500 Loss 2.0654397010803223 in 15.317374\n",
      "Epoch 21 Batch 36000 Loss 2.145474672317505 in 15.092397\n",
      "saving model with loss 2.145474672317505\n",
      "Epoch 21 Batch 36500 Loss 2.2001640796661377 in 15.138508\n",
      "Epoch 21 Batch 37000 Loss 2.2224724292755127 in 15.062914\n",
      "saving model with loss 2.2224724292755127\n",
      "Epoch 21 Batch 37500 Loss 2.2440788745880127 in 15.151247\n",
      "Epoch 21 Batch 38000 Loss 2.3736374378204346 in 15.082095\n",
      "saving model with loss 2.3736374378204346\n",
      "Epoch 21 Batch 38500 Loss 2.1850502490997314 in 15.157946\n",
      "Epoch 21 Batch 39000 Loss 2.1031219959259033 in 15.083440\n",
      "saving model with loss 2.1031219959259033\n",
      "Epoch 21 Batch 39500 Loss 2.2772462368011475 in 15.154245\n",
      "Epoch 21 Batch 40000 Loss 2.349738121032715 in 15.087501\n",
      "saving model with loss 2.349738121032715\n",
      "Epoch 21 Batch 40500 Loss 2.1610958576202393 in 15.158797\n",
      "Epoch 21 Batch 41000 Loss 2.086036205291748 in 15.082137\n",
      "saving model with loss 2.086036205291748\n",
      "Epoch 21 Batch 41500 Loss 2.0472190380096436 in 15.153475\n",
      "Epoch 21 Batch 42000 Loss 2.164888381958008 in 15.091219\n",
      "saving model with loss 2.164888381958008\n",
      "Epoch 21 Batch 42500 Loss 2.2221240997314453 in 15.158353\n",
      "Epoch 21 Batch 43000 Loss 2.212205171585083 in 15.091307\n",
      "saving model with loss 2.212205171585083\n",
      "Epoch 21 Batch 43500 Loss 2.205627202987671 in 15.150720\n",
      "Epoch 21 Batch 44000 Loss 2.2190134525299072 in 15.075848\n",
      "saving model with loss 2.2190134525299072\n",
      "Epoch 21 Batch 44500 Loss 2.347311019897461 in 15.152494\n",
      "Epoch 21 Batch 45000 Loss 2.123563289642334 in 15.086667\n",
      "saving model with loss 2.123563289642334\n",
      "Epoch 21 Batch 45500 Loss 2.2829787731170654 in 15.160167\n",
      "Epoch 21 Batch 46000 Loss 2.219564914703369 in 15.084857\n",
      "saving model with loss 2.219564914703369\n",
      "Epoch 21 Batch 46500 Loss 2.3186144828796387 in 15.151039\n",
      "Epoch 21 Batch 47000 Loss 2.17824649810791 in 15.065643\n",
      "saving model with loss 2.17824649810791\n",
      "Epoch 21 Batch 47500 Loss 2.2520885467529297 in 15.154144\n",
      "Epoch 21 Batch 48000 Loss 2.163799524307251 in 15.082554\n",
      "saving model with loss 2.163799524307251\n",
      "Epoch 21 Batch 48500 Loss 2.1834793090820312 in 15.160678\n",
      "Epoch 21 Batch 49000 Loss 2.222275733947754 in 15.087990\n",
      "saving model with loss 2.222275733947754\n",
      "Epoch 21 Batch 49500 Loss 2.1603026390075684 in 15.154915\n",
      "Epoch 21 Batch 50000 Loss 2.1107659339904785 in 15.095728\n",
      "saving model with loss 2.1107659339904785\n",
      "Epoch 21 Batch 50500 Loss 2.2945432662963867 in 15.131661\n",
      "Epoch 21 Batch 51000 Loss 2.1397464275360107 in 15.077484\n",
      "saving model with loss 2.1397464275360107\n",
      "Epoch 21 Batch 51500 Loss 2.2674083709716797 in 15.147177\n",
      "Epoch 21 Batch 52000 Loss 2.3380074501037598 in 15.088771\n",
      "saving model with loss 2.3380074501037598\n",
      "Epoch 21 Batch 52500 Loss 2.0533459186553955 in 15.154160\n",
      "Epoch 21 Batch 53000 Loss 2.2007815837860107 in 15.080697\n",
      "saving model with loss 2.2007815837860107\n",
      "Epoch 21 Batch 53500 Loss 2.145503044128418 in 15.147665\n",
      "Epoch 21 Batch 54000 Loss 2.1840224266052246 in 15.071133\n",
      "saving model with loss 2.1840224266052246\n",
      "Epoch 21 Batch 54500 Loss 2.0824830532073975 in 15.126281\n",
      "Epoch 21 Batch 55000 Loss 2.213207244873047 in 15.085220\n",
      "saving model with loss 2.213207244873047\n",
      "Epoch 21 Batch 55500 Loss 2.140392303466797 in 15.145360\n",
      "Epoch 21 Batch 56000 Loss 2.0961456298828125 in 15.080921\n",
      "saving model with loss 2.0961456298828125\n",
      "Epoch 21 Batch 56500 Loss 2.2934441566467285 in 15.138267\n",
      "Epoch 21 Batch 57000 Loss 2.318978786468506 in 15.123538\n",
      "saving model with loss 2.318978786468506\n",
      "Epoch 21 Batch 57500 Loss 2.1677043437957764 in 15.149408\n",
      "Epoch 21 Batch 58000 Loss 2.161989212036133 in 15.083487\n",
      "saving model with loss 2.161989212036133\n",
      "Epoch 21 Batch 58500 Loss 2.267625093460083 in 15.131833\n",
      "Epoch 21 Batch 59000 Loss 2.1970279216766357 in 15.062989\n",
      "saving model with loss 2.1970279216766357\n",
      "Epoch 21 Batch 59500 Loss 2.1682586669921875 in 15.139134\n",
      "Epoch 21 Batch 60000 Loss 2.0976734161376953 in 15.075677\n",
      "saving model with loss 2.0976734161376953\n",
      "Epoch 21 Batch 60500 Loss 2.2908666133880615 in 15.147400\n",
      "Epoch 21 Batch 61000 Loss 2.064208745956421 in 15.074674\n",
      "saving model with loss 2.064208745956421\n",
      "Epoch 21 Batch 61500 Loss 2.2657291889190674 in 15.539158\n",
      "Epoch 21 Batch 62000 Loss 2.339181661605835 in 15.084969\n",
      "saving model with loss 2.339181661605835\n",
      "Epoch 21 Batch 62500 Loss 2.304576873779297 in 15.143130\n",
      "Epoch 21 Batch 63000 Loss 2.4526970386505127 in 15.088502\n",
      "saving model with loss 2.4526970386505127\n",
      "Epoch 21 Batch 63500 Loss 2.2350590229034424 in 15.157016\n",
      "Epoch 21 Batch 64000 Loss 2.3547043800354004 in 15.111771\n",
      "saving model with loss 2.3547043800354004\n",
      "Epoch 21 Batch 64500 Loss 2.175215721130371 in 15.144322\n",
      "Epoch 21 Batch 65000 Loss 2.2336418628692627 in 15.224614\n",
      "saving model with loss 2.2336418628692627\n",
      "Epoch 21 Batch 65500 Loss 2.1177258491516113 in 15.124254\n",
      "Epoch 21 Batch 66000 Loss 2.2048048973083496 in 15.066445\n",
      "saving model with loss 2.2048048973083496\n",
      "Epoch 21 Batch 66500 Loss 2.2939505577087402 in 15.153200\n",
      "Epoch 21 Batch 67000 Loss 2.276224374771118 in 15.079823\n",
      "saving model with loss 2.276224374771118\n",
      "Epoch 21 Batch 67500 Loss 2.276538848876953 in 15.141368\n",
      "Epoch 21 Batch 68000 Loss 2.0330796241760254 in 15.067491\n",
      "saving model with loss 2.0330796241760254\n",
      "Epoch 21 Batch 68500 Loss 2.1656901836395264 in 15.138912\n",
      "Epoch 21 Batch 69000 Loss 2.171782970428467 in 15.092689\n",
      "saving model with loss 2.171782970428467\n",
      "Epoch 21 Batch 69500 Loss 2.5304884910583496 in 15.149229\n",
      "Epoch 21 Batch 70000 Loss 2.2935214042663574 in 15.068871\n",
      "saving model with loss 2.2935214042663574\n",
      "Epoch 21 Batch 70500 Loss 2.167407751083374 in 15.143285\n",
      "Epoch 22 Loss 2.178600311279297\n",
      "Time taken for 1 epoch 2152.774924993515\n",
      "number of batches : 70553\n",
      "Epoch 22 Batch 0 Loss 2.285107135772705 in 1.166936\n",
      "saving model with loss 2.285107135772705\n",
      "Epoch 22 Batch 500 Loss 2.19966459274292 in 15.142237\n",
      "Epoch 22 Batch 1000 Loss 2.196424961090088 in 15.053539\n",
      "saving model with loss 2.196424961090088\n",
      "Epoch 22 Batch 1500 Loss 2.2422661781311035 in 15.132902\n",
      "Epoch 22 Batch 2000 Loss 2.286884069442749 in 15.080419\n",
      "saving model with loss 2.286884069442749\n",
      "Epoch 22 Batch 2500 Loss 2.3496718406677246 in 15.147416\n",
      "Epoch 22 Batch 3000 Loss 2.2941763401031494 in 15.064681\n",
      "saving model with loss 2.2941763401031494\n",
      "Epoch 22 Batch 3500 Loss 2.3088507652282715 in 15.145248\n",
      "Epoch 22 Batch 4000 Loss 2.209254264831543 in 15.065722\n",
      "saving model with loss 2.209254264831543\n",
      "Epoch 22 Batch 4500 Loss 2.1346094608306885 in 15.153664\n",
      "Epoch 22 Batch 5000 Loss 2.1840689182281494 in 15.119065\n",
      "saving model with loss 2.1840689182281494\n",
      "Epoch 22 Batch 5500 Loss 2.1031227111816406 in 15.113795\n",
      "Epoch 22 Batch 6000 Loss 2.325591564178467 in 15.052236\n",
      "saving model with loss 2.325591564178467\n",
      "Epoch 22 Batch 6500 Loss 2.2391531467437744 in 15.116142\n",
      "Epoch 22 Batch 7000 Loss 2.2123937606811523 in 15.047913\n",
      "saving model with loss 2.2123937606811523\n",
      "Epoch 22 Batch 7500 Loss 2.3165359497070312 in 15.110844\n",
      "Epoch 22 Batch 8000 Loss 2.203594446182251 in 15.042526\n",
      "saving model with loss 2.203594446182251\n",
      "Epoch 22 Batch 8500 Loss 2.318852424621582 in 15.119832\n",
      "Epoch 22 Batch 9000 Loss 2.1225414276123047 in 15.039800\n",
      "saving model with loss 2.1225414276123047\n",
      "Epoch 22 Batch 9500 Loss 2.1311779022216797 in 15.118843\n",
      "Epoch 22 Batch 10000 Loss 2.170320510864258 in 15.034363\n",
      "saving model with loss 2.170320510864258\n",
      "Epoch 22 Batch 10500 Loss 2.4771158695220947 in 15.112378\n",
      "Epoch 22 Batch 11000 Loss 2.429187297821045 in 15.040209\n",
      "saving model with loss 2.429187297821045\n",
      "Epoch 22 Batch 11500 Loss 2.305093288421631 in 15.082320\n",
      "Epoch 22 Batch 12000 Loss 2.1530864238739014 in 15.026816\n",
      "saving model with loss 2.1530864238739014\n",
      "Epoch 22 Batch 12500 Loss 2.3202481269836426 in 15.074735\n",
      "Epoch 22 Batch 13000 Loss 2.4083831310272217 in 15.050716\n",
      "saving model with loss 2.4083831310272217\n",
      "Epoch 22 Batch 13500 Loss 2.265212297439575 in 15.097639\n",
      "Epoch 22 Batch 14000 Loss 2.22622013092041 in 15.032076\n",
      "saving model with loss 2.22622013092041\n",
      "Epoch 22 Batch 14500 Loss 2.151933193206787 in 15.107632\n",
      "Epoch 22 Batch 15000 Loss 2.3029563426971436 in 15.033001\n",
      "saving model with loss 2.3029563426971436\n",
      "Epoch 22 Batch 15500 Loss 2.2243900299072266 in 15.105510\n",
      "Epoch 22 Batch 16000 Loss 2.0480074882507324 in 15.030292\n",
      "saving model with loss 2.0480074882507324\n",
      "Epoch 22 Batch 16500 Loss 2.2124786376953125 in 15.114231\n",
      "Epoch 22 Batch 17000 Loss 2.0787367820739746 in 15.035256\n",
      "saving model with loss 2.0787367820739746\n",
      "Epoch 22 Batch 17500 Loss 2.2581627368927 in 15.107921\n",
      "Epoch 22 Batch 18000 Loss 2.4493489265441895 in 15.024900\n",
      "saving model with loss 2.4493489265441895\n",
      "Epoch 22 Batch 18500 Loss 2.3605642318725586 in 15.105164\n",
      "Epoch 22 Batch 19000 Loss 2.2155590057373047 in 15.034782\n",
      "saving model with loss 2.2155590057373047\n",
      "Epoch 22 Batch 19500 Loss 2.3568806648254395 in 15.106127\n",
      "Epoch 22 Batch 20000 Loss 2.1451706886291504 in 15.016916\n",
      "saving model with loss 2.1451706886291504\n",
      "Epoch 22 Batch 20500 Loss 2.148585796356201 in 15.108931\n",
      "Epoch 22 Batch 21000 Loss 2.1654818058013916 in 15.048759\n",
      "saving model with loss 2.1654818058013916\n",
      "Epoch 22 Batch 21500 Loss 2.238285541534424 in 15.118734\n",
      "Epoch 22 Batch 22000 Loss 2.2946765422821045 in 15.056195\n",
      "saving model with loss 2.2946765422821045\n",
      "Epoch 22 Batch 22500 Loss 2.217520236968994 in 15.096420\n",
      "Epoch 22 Batch 23000 Loss 2.251527786254883 in 15.039611\n",
      "saving model with loss 2.251527786254883\n",
      "Epoch 22 Batch 23500 Loss 2.215146064758301 in 15.106678\n",
      "Epoch 22 Batch 24000 Loss 2.238532543182373 in 15.042879\n",
      "saving model with loss 2.238532543182373\n",
      "Epoch 22 Batch 24500 Loss 2.1572105884552 in 15.126673\n",
      "Epoch 22 Batch 25000 Loss 2.195803165435791 in 15.450751\n",
      "saving model with loss 2.195803165435791\n",
      "Epoch 22 Batch 25500 Loss 2.249228000640869 in 15.142486\n",
      "Epoch 22 Batch 26000 Loss 2.218651533126831 in 15.081607\n",
      "saving model with loss 2.218651533126831\n",
      "Epoch 22 Batch 26500 Loss 2.163869619369507 in 15.132479\n",
      "Epoch 22 Batch 27000 Loss 2.2575671672821045 in 15.067241\n",
      "saving model with loss 2.2575671672821045\n",
      "Epoch 22 Batch 27500 Loss 2.250769853591919 in 15.147132\n",
      "Epoch 22 Batch 28000 Loss 2.3292107582092285 in 15.074045\n",
      "saving model with loss 2.3292107582092285\n",
      "Epoch 22 Batch 28500 Loss 2.161198139190674 in 15.152800\n",
      "Epoch 22 Batch 29000 Loss 2.275266170501709 in 15.085899\n",
      "saving model with loss 2.275266170501709\n",
      "Epoch 22 Batch 29500 Loss 2.225132703781128 in 15.140298\n",
      "Epoch 22 Batch 30000 Loss 2.1164135932922363 in 15.071965\n",
      "saving model with loss 2.1164135932922363\n",
      "Epoch 22 Batch 30500 Loss 2.1805920600891113 in 15.147986\n",
      "Epoch 22 Batch 31000 Loss 2.0574233531951904 in 15.072463\n",
      "saving model with loss 2.0574233531951904\n",
      "Epoch 22 Batch 31500 Loss 2.1357474327087402 in 15.131335\n",
      "Epoch 22 Batch 32000 Loss 2.275383710861206 in 15.075277\n",
      "saving model with loss 2.275383710861206\n",
      "Epoch 22 Batch 32500 Loss 2.2912514209747314 in 15.132446\n",
      "Epoch 22 Batch 33000 Loss 2.2073166370391846 in 15.067281\n",
      "saving model with loss 2.2073166370391846\n",
      "Epoch 22 Batch 33500 Loss 2.276564121246338 in 15.160136\n",
      "Epoch 22 Batch 34000 Loss 2.210829496383667 in 15.076047\n",
      "saving model with loss 2.210829496383667\n",
      "Epoch 22 Batch 34500 Loss 2.146078109741211 in 15.154093\n",
      "Epoch 22 Batch 35000 Loss 2.249544858932495 in 15.069449\n",
      "saving model with loss 2.249544858932495\n",
      "Epoch 22 Batch 35500 Loss 2.260406017303467 in 15.148030\n",
      "Epoch 22 Batch 36000 Loss 2.1879773139953613 in 15.061953\n",
      "saving model with loss 2.1879773139953613\n",
      "Epoch 22 Batch 36500 Loss 2.1806793212890625 in 15.156686\n",
      "Epoch 22 Batch 37000 Loss 2.1476926803588867 in 15.076255\n",
      "saving model with loss 2.1476926803588867\n",
      "Epoch 22 Batch 37500 Loss 2.2629168033599854 in 15.149518\n",
      "Epoch 22 Batch 38000 Loss 2.2067363262176514 in 15.084911\n",
      "saving model with loss 2.2067363262176514\n",
      "Epoch 22 Batch 38500 Loss 2.151676654815674 in 15.149500\n",
      "Epoch 22 Batch 39000 Loss 2.369535446166992 in 15.082757\n",
      "saving model with loss 2.369535446166992\n",
      "Epoch 22 Batch 39500 Loss 2.2984814643859863 in 15.149920\n",
      "Epoch 22 Batch 40000 Loss 2.040752410888672 in 15.076794\n",
      "saving model with loss 2.040752410888672\n",
      "Epoch 22 Batch 40500 Loss 2.1421828269958496 in 15.155194\n",
      "Epoch 22 Batch 41000 Loss 2.2290000915527344 in 15.075248\n",
      "saving model with loss 2.2290000915527344\n",
      "Epoch 22 Batch 41500 Loss 2.1285908222198486 in 15.141707\n",
      "Epoch 22 Batch 42000 Loss 2.2781927585601807 in 15.170339\n",
      "saving model with loss 2.2781927585601807\n",
      "Epoch 22 Batch 42500 Loss 2.134885787963867 in 15.155855\n",
      "Epoch 22 Batch 43000 Loss 2.2903003692626953 in 15.078460\n",
      "saving model with loss 2.2903003692626953\n",
      "Epoch 22 Batch 43500 Loss 2.207484722137451 in 15.133131\n",
      "Epoch 22 Batch 44000 Loss 2.198012590408325 in 15.078531\n",
      "saving model with loss 2.198012590408325\n",
      "Epoch 22 Batch 44500 Loss 2.486746072769165 in 15.156914\n",
      "Epoch 22 Batch 45000 Loss 2.3068013191223145 in 15.083356\n",
      "saving model with loss 2.3068013191223145\n",
      "Epoch 22 Batch 45500 Loss 2.155792474746704 in 15.136127\n",
      "Epoch 22 Batch 46000 Loss 2.149428129196167 in 15.060646\n",
      "saving model with loss 2.149428129196167\n",
      "Epoch 22 Batch 46500 Loss 2.302966594696045 in 15.174905\n",
      "Epoch 22 Batch 47000 Loss 2.2425971031188965 in 15.080178\n",
      "saving model with loss 2.2425971031188965\n",
      "Epoch 22 Batch 47500 Loss 2.1967825889587402 in 15.132421\n",
      "Epoch 22 Batch 48000 Loss 2.3611936569213867 in 15.062145\n",
      "saving model with loss 2.3611936569213867\n",
      "Epoch 22 Batch 48500 Loss 2.1693530082702637 in 15.129371\n",
      "Epoch 22 Batch 49000 Loss 2.1773736476898193 in 15.078510\n",
      "saving model with loss 2.1773736476898193\n",
      "Epoch 22 Batch 49500 Loss 2.1132805347442627 in 15.130383\n",
      "Epoch 22 Batch 50000 Loss 2.308014392852783 in 15.067122\n",
      "saving model with loss 2.308014392852783\n",
      "Epoch 22 Batch 50500 Loss 2.37982177734375 in 15.138675\n",
      "Epoch 22 Batch 51000 Loss 2.215701103210449 in 15.074983\n",
      "saving model with loss 2.215701103210449\n",
      "Epoch 22 Batch 51500 Loss 2.314936876296997 in 15.154516\n",
      "Epoch 22 Batch 52000 Loss 2.1954259872436523 in 15.103995\n",
      "saving model with loss 2.1954259872436523\n",
      "Epoch 22 Batch 52500 Loss 2.2523210048675537 in 15.143130\n",
      "Epoch 22 Batch 53000 Loss 2.3032517433166504 in 15.074951\n",
      "saving model with loss 2.3032517433166504\n",
      "Epoch 22 Batch 53500 Loss 2.3125290870666504 in 15.145776\n",
      "Epoch 22 Batch 54000 Loss 2.2069029808044434 in 15.095128\n",
      "saving model with loss 2.2069029808044434\n",
      "Epoch 22 Batch 54500 Loss 2.1527466773986816 in 15.158888\n",
      "Epoch 22 Batch 55000 Loss 2.1397268772125244 in 15.079921\n",
      "saving model with loss 2.1397268772125244\n",
      "Epoch 22 Batch 55500 Loss 2.248685598373413 in 15.140653\n",
      "Epoch 22 Batch 56000 Loss 2.3718008995056152 in 15.094504\n",
      "saving model with loss 2.3718008995056152\n",
      "Epoch 22 Batch 56500 Loss 2.0893375873565674 in 15.153565\n",
      "Epoch 22 Batch 57000 Loss 2.200467586517334 in 15.065542\n",
      "saving model with loss 2.200467586517334\n",
      "Epoch 22 Batch 57500 Loss 2.232203483581543 in 15.143630\n",
      "Epoch 22 Batch 58000 Loss 2.2526047229766846 in 15.056885\n",
      "saving model with loss 2.2526047229766846\n",
      "Epoch 22 Batch 58500 Loss 2.2658467292785645 in 15.167033\n",
      "Epoch 22 Batch 59000 Loss 2.230341911315918 in 15.028552\n",
      "saving model with loss 2.230341911315918\n",
      "Epoch 22 Batch 59500 Loss 2.0913190841674805 in 15.103688\n",
      "Epoch 22 Batch 60000 Loss 2.319476842880249 in 15.048413\n",
      "saving model with loss 2.319476842880249\n",
      "Epoch 22 Batch 60500 Loss 2.239758253097534 in 15.110598\n",
      "Epoch 22 Batch 61000 Loss 2.249549388885498 in 15.042119\n",
      "saving model with loss 2.249549388885498\n",
      "Epoch 22 Batch 61500 Loss 2.1847736835479736 in 15.112878\n",
      "Epoch 22 Batch 62000 Loss 2.384075403213501 in 15.030478\n",
      "saving model with loss 2.384075403213501\n",
      "Epoch 22 Batch 62500 Loss 2.3090157508850098 in 15.101401\n",
      "Epoch 22 Batch 63000 Loss 2.2214019298553467 in 15.042346\n",
      "saving model with loss 2.2214019298553467\n",
      "Epoch 22 Batch 63500 Loss 2.1599371433258057 in 15.101139\n",
      "Epoch 22 Batch 64000 Loss 2.313931941986084 in 15.032913\n",
      "saving model with loss 2.313931941986084\n",
      "Epoch 22 Batch 64500 Loss 2.2043631076812744 in 15.099876\n",
      "Epoch 22 Batch 65000 Loss 2.2995972633361816 in 15.036178\n",
      "saving model with loss 2.2995972633361816\n",
      "Epoch 22 Batch 65500 Loss 2.2522966861724854 in 15.103719\n",
      "Epoch 22 Batch 66000 Loss 2.2640252113342285 in 15.033382\n",
      "saving model with loss 2.2640252113342285\n",
      "Epoch 22 Batch 66500 Loss 2.273977518081665 in 15.134291\n",
      "Epoch 22 Batch 67000 Loss 2.401210308074951 in 15.046867\n",
      "saving model with loss 2.401210308074951\n",
      "Epoch 22 Batch 67500 Loss 2.2211499214172363 in 15.113514\n",
      "Epoch 22 Batch 68000 Loss 2.1055290699005127 in 15.042457\n",
      "saving model with loss 2.1055290699005127\n",
      "Epoch 22 Batch 68500 Loss 2.2475576400756836 in 15.110772\n",
      "Epoch 22 Batch 69000 Loss 2.1601502895355225 in 15.033259\n",
      "saving model with loss 2.1601502895355225\n",
      "Epoch 22 Batch 69500 Loss 2.1456944942474365 in 15.121796\n",
      "Epoch 22 Batch 70000 Loss 2.3495755195617676 in 15.039691\n",
      "saving model with loss 2.3495755195617676\n",
      "Epoch 22 Batch 70500 Loss 2.2544407844543457 in 15.110058\n",
      "Epoch 23 Loss 2.3288722038269043\n",
      "Time taken for 1 epoch 2131.606181383133\n",
      "number of batches : 70553\n",
      "Epoch 23 Batch 0 Loss 2.093294382095337 in 1.326480\n",
      "saving model with loss 2.093294382095337\n",
      "Epoch 23 Batch 500 Loss 2.374972105026245 in 15.135842\n",
      "Epoch 23 Batch 1000 Loss 2.1471848487854004 in 15.032726\n",
      "saving model with loss 2.1471848487854004\n",
      "Epoch 23 Batch 1500 Loss 2.1553704738616943 in 15.087674\n",
      "Epoch 23 Batch 2000 Loss 2.2629306316375732 in 15.036900\n",
      "saving model with loss 2.2629306316375732\n",
      "Epoch 23 Batch 2500 Loss 2.194789409637451 in 15.114659\n",
      "Epoch 23 Batch 3000 Loss 2.4795522689819336 in 15.018691\n",
      "saving model with loss 2.4795522689819336\n",
      "Epoch 23 Batch 3500 Loss 2.106196880340576 in 15.103762\n",
      "Epoch 23 Batch 4000 Loss 2.309237241744995 in 15.045716\n",
      "saving model with loss 2.309237241744995\n",
      "Epoch 23 Batch 4500 Loss 2.1128501892089844 in 15.102359\n",
      "Epoch 23 Batch 5000 Loss 2.4619433879852295 in 15.026079\n",
      "saving model with loss 2.4619433879852295\n",
      "Epoch 23 Batch 5500 Loss 2.217153310775757 in 15.104927\n",
      "Epoch 23 Batch 6000 Loss 2.120124340057373 in 15.032213\n",
      "saving model with loss 2.120124340057373\n",
      "Epoch 23 Batch 6500 Loss 2.1130993366241455 in 15.092509\n",
      "Epoch 23 Batch 7000 Loss 2.3116538524627686 in 15.028607\n",
      "saving model with loss 2.3116538524627686\n",
      "Epoch 23 Batch 7500 Loss 2.172844648361206 in 15.102581\n",
      "Epoch 23 Batch 8000 Loss 2.300100803375244 in 15.036698\n",
      "saving model with loss 2.300100803375244\n",
      "Epoch 23 Batch 8500 Loss 2.3504247665405273 in 15.093933\n",
      "Epoch 23 Batch 9000 Loss 2.200026750564575 in 15.047416\n",
      "saving model with loss 2.200026750564575\n",
      "Epoch 23 Batch 9500 Loss 2.167032241821289 in 15.114851\n",
      "Epoch 23 Batch 10000 Loss 2.1050968170166016 in 15.023219\n",
      "saving model with loss 2.1050968170166016\n",
      "Epoch 23 Batch 10500 Loss 2.2342047691345215 in 15.104896\n",
      "Epoch 23 Batch 11000 Loss 2.1082193851470947 in 15.048706\n",
      "saving model with loss 2.1082193851470947\n",
      "Epoch 23 Batch 11500 Loss 2.125771999359131 in 15.108481\n",
      "Epoch 23 Batch 12000 Loss 2.1908559799194336 in 15.032639\n",
      "saving model with loss 2.1908559799194336\n",
      "Epoch 23 Batch 12500 Loss 2.288393020629883 in 15.103234\n",
      "Epoch 23 Batch 13000 Loss 2.2516748905181885 in 15.046429\n",
      "saving model with loss 2.2516748905181885\n",
      "Epoch 23 Batch 13500 Loss 2.1772334575653076 in 15.305951\n",
      "Epoch 23 Batch 14000 Loss 2.226407051086426 in 15.045206\n",
      "saving model with loss 2.226407051086426\n",
      "Epoch 23 Batch 14500 Loss 2.0811922550201416 in 15.116637\n",
      "Epoch 23 Batch 15000 Loss 2.151400327682495 in 15.037014\n",
      "saving model with loss 2.151400327682495\n",
      "Epoch 23 Batch 15500 Loss 2.131368637084961 in 15.113699\n",
      "Epoch 23 Batch 16000 Loss 2.290959596633911 in 15.030232\n",
      "saving model with loss 2.290959596633911\n",
      "Epoch 23 Batch 16500 Loss 2.2231154441833496 in 15.105720\n",
      "Epoch 23 Batch 17000 Loss 2.302049160003662 in 15.045227\n",
      "saving model with loss 2.302049160003662\n",
      "Epoch 23 Batch 17500 Loss 2.326312303543091 in 15.090384\n",
      "Epoch 23 Batch 18000 Loss 2.1892337799072266 in 15.033372\n",
      "saving model with loss 2.1892337799072266\n",
      "Epoch 23 Batch 18500 Loss 2.1284351348876953 in 15.102872\n",
      "Epoch 23 Batch 19000 Loss 2.2798149585723877 in 15.029962\n",
      "saving model with loss 2.2798149585723877\n",
      "Epoch 23 Batch 19500 Loss 2.2948787212371826 in 15.084939\n",
      "Epoch 23 Batch 20000 Loss 2.1182548999786377 in 15.031511\n",
      "saving model with loss 2.1182548999786377\n",
      "Epoch 23 Batch 20500 Loss 2.231252670288086 in 15.098371\n",
      "Epoch 23 Batch 21000 Loss 2.3871231079101562 in 15.027558\n",
      "saving model with loss 2.3871231079101562\n",
      "Epoch 23 Batch 21500 Loss 2.2954673767089844 in 15.102615\n",
      "Epoch 23 Batch 22000 Loss 2.24402117729187 in 15.040132\n",
      "saving model with loss 2.24402117729187\n",
      "Epoch 23 Batch 22500 Loss 2.2816977500915527 in 15.105368\n",
      "Epoch 23 Batch 23000 Loss 2.0936224460601807 in 15.034060\n",
      "saving model with loss 2.0936224460601807\n",
      "Epoch 23 Batch 23500 Loss 2.3173134326934814 in 15.122630\n",
      "Epoch 23 Batch 24000 Loss 2.1940178871154785 in 15.039696\n",
      "saving model with loss 2.1940178871154785\n",
      "Epoch 23 Batch 24500 Loss 2.067066192626953 in 15.091924\n",
      "Epoch 23 Batch 25000 Loss 2.189545154571533 in 15.033209\n",
      "saving model with loss 2.189545154571533\n",
      "Epoch 23 Batch 25500 Loss 2.185439348220825 in 15.091248\n",
      "Epoch 23 Batch 26000 Loss 2.181865692138672 in 15.031041\n",
      "saving model with loss 2.181865692138672\n",
      "Epoch 23 Batch 26500 Loss 2.2740464210510254 in 15.104844\n",
      "Epoch 23 Batch 27000 Loss 2.2210299968719482 in 15.046180\n",
      "saving model with loss 2.2210299968719482\n",
      "Epoch 23 Batch 27500 Loss 2.146667957305908 in 15.105889\n",
      "Epoch 23 Batch 28000 Loss 2.2618420124053955 in 15.036855\n",
      "saving model with loss 2.2618420124053955\n",
      "Epoch 23 Batch 28500 Loss 2.392127752304077 in 15.122823\n",
      "Epoch 23 Batch 29000 Loss 2.3249897956848145 in 15.042546\n",
      "saving model with loss 2.3249897956848145\n",
      "Epoch 23 Batch 29500 Loss 2.060420513153076 in 15.099492\n",
      "Epoch 23 Batch 30000 Loss 2.3508830070495605 in 15.029546\n",
      "saving model with loss 2.3508830070495605\n",
      "Epoch 23 Batch 30500 Loss 2.189462184906006 in 15.102809\n",
      "Epoch 23 Batch 31000 Loss 2.3528637886047363 in 15.035621\n",
      "saving model with loss 2.3528637886047363\n",
      "Epoch 23 Batch 31500 Loss 2.186239719390869 in 15.099039\n",
      "Epoch 23 Batch 32000 Loss 2.218038558959961 in 15.050765\n",
      "saving model with loss 2.218038558959961\n",
      "Epoch 23 Batch 32500 Loss 2.0787479877471924 in 15.113540\n",
      "Epoch 23 Batch 33000 Loss 2.226868152618408 in 15.049576\n",
      "saving model with loss 2.226868152618408\n",
      "Epoch 23 Batch 33500 Loss 2.3546574115753174 in 15.104492\n",
      "Epoch 23 Batch 34000 Loss 2.325942277908325 in 15.043050\n",
      "saving model with loss 2.325942277908325\n",
      "Epoch 23 Batch 34500 Loss 2.153374195098877 in 15.111689\n",
      "Epoch 23 Batch 35000 Loss 2.2389414310455322 in 15.044329\n",
      "saving model with loss 2.2389414310455322\n",
      "Epoch 23 Batch 35500 Loss 2.2554776668548584 in 15.110992\n",
      "Epoch 23 Batch 36000 Loss 2.181279420852661 in 15.024422\n",
      "saving model with loss 2.181279420852661\n",
      "Epoch 23 Batch 36500 Loss 2.2191925048828125 in 15.097300\n",
      "Epoch 23 Batch 37000 Loss 2.3414928913116455 in 15.063785\n",
      "saving model with loss 2.3414928913116455\n",
      "Epoch 23 Batch 37500 Loss 2.2474796772003174 in 15.117903\n",
      "Epoch 23 Batch 38000 Loss 2.0641932487487793 in 15.036292\n",
      "saving model with loss 2.0641932487487793\n",
      "Epoch 23 Batch 38500 Loss 2.1375064849853516 in 15.090820\n",
      "Epoch 23 Batch 39000 Loss 2.22328519821167 in 15.035458\n",
      "saving model with loss 2.22328519821167\n",
      "Epoch 23 Batch 39500 Loss 2.137359142303467 in 15.109540\n",
      "Epoch 23 Batch 40000 Loss 2.2647910118103027 in 15.449048\n",
      "saving model with loss 2.2647910118103027\n",
      "Epoch 23 Batch 40500 Loss 2.250345468521118 in 15.142124\n",
      "Epoch 23 Batch 41000 Loss 2.3089828491210938 in 15.090760\n",
      "saving model with loss 2.3089828491210938\n",
      "Epoch 23 Batch 41500 Loss 2.3255412578582764 in 15.140967\n",
      "Epoch 23 Batch 42000 Loss 2.2423737049102783 in 15.079263\n",
      "saving model with loss 2.2423737049102783\n",
      "Epoch 23 Batch 42500 Loss 2.173891544342041 in 15.141993\n",
      "Epoch 23 Batch 43000 Loss 2.2427589893341064 in 15.061182\n",
      "saving model with loss 2.2427589893341064\n",
      "Epoch 23 Batch 43500 Loss 2.190648317337036 in 15.134448\n",
      "Epoch 23 Batch 44000 Loss 2.4077892303466797 in 15.072648\n",
      "saving model with loss 2.4077892303466797\n",
      "Epoch 23 Batch 44500 Loss 2.2906742095947266 in 15.134710\n",
      "Epoch 23 Batch 45000 Loss 2.3108417987823486 in 15.078046\n",
      "saving model with loss 2.3108417987823486\n",
      "Epoch 23 Batch 45500 Loss 2.3125858306884766 in 15.147690\n",
      "Epoch 23 Batch 46000 Loss 2.1520230770111084 in 15.070442\n",
      "saving model with loss 2.1520230770111084\n",
      "Epoch 23 Batch 46500 Loss 2.167793035507202 in 15.149190\n",
      "Epoch 23 Batch 47000 Loss 2.3632075786590576 in 15.070619\n",
      "saving model with loss 2.3632075786590576\n",
      "Epoch 23 Batch 47500 Loss 2.3760952949523926 in 15.125159\n",
      "Epoch 23 Batch 48000 Loss 2.212254047393799 in 15.068548\n",
      "saving model with loss 2.212254047393799\n",
      "Epoch 23 Batch 48500 Loss 2.1294994354248047 in 15.140292\n",
      "Epoch 23 Batch 49000 Loss 2.133361339569092 in 15.068011\n",
      "saving model with loss 2.133361339569092\n",
      "Epoch 23 Batch 49500 Loss 2.2227237224578857 in 15.144285\n",
      "Epoch 23 Batch 50000 Loss 2.483921527862549 in 15.059008\n",
      "saving model with loss 2.483921527862549\n",
      "Epoch 23 Batch 50500 Loss 2.4427618980407715 in 15.157727\n",
      "Epoch 23 Batch 51000 Loss 2.2055561542510986 in 15.080413\n",
      "saving model with loss 2.2055561542510986\n",
      "Epoch 23 Batch 51500 Loss 2.2374279499053955 in 15.140849\n",
      "Epoch 23 Batch 52000 Loss 2.2083258628845215 in 15.072069\n",
      "saving model with loss 2.2083258628845215\n",
      "Epoch 23 Batch 52500 Loss 2.406895637512207 in 15.133208\n",
      "Epoch 23 Batch 53000 Loss 2.1002345085144043 in 15.060695\n",
      "saving model with loss 2.1002345085144043\n",
      "Epoch 23 Batch 53500 Loss 2.341034412384033 in 15.139283\n",
      "Epoch 23 Batch 54000 Loss 2.1334385871887207 in 15.078872\n",
      "saving model with loss 2.1334385871887207\n",
      "Epoch 23 Batch 54500 Loss 2.110950231552124 in 15.135424\n",
      "Epoch 23 Batch 55000 Loss 2.2110183238983154 in 15.100632\n",
      "saving model with loss 2.2110183238983154\n",
      "Epoch 23 Batch 55500 Loss 2.2991344928741455 in 15.149218\n",
      "Epoch 23 Batch 56000 Loss 2.3098912239074707 in 15.071521\n",
      "saving model with loss 2.3098912239074707\n",
      "Epoch 23 Batch 56500 Loss 2.261348247528076 in 15.140916\n",
      "Epoch 23 Batch 57000 Loss 2.292203664779663 in 15.055569\n",
      "saving model with loss 2.292203664779663\n",
      "Epoch 23 Batch 57500 Loss 2.2301888465881348 in 15.149888\n",
      "Epoch 23 Batch 58000 Loss 2.234391450881958 in 15.070992\n",
      "saving model with loss 2.234391450881958\n",
      "Epoch 23 Batch 58500 Loss 2.395807981491089 in 15.141421\n",
      "Epoch 23 Batch 59000 Loss 2.041172504425049 in 15.057204\n",
      "saving model with loss 2.041172504425049\n",
      "Epoch 23 Batch 59500 Loss 2.245718240737915 in 15.137461\n",
      "Epoch 23 Batch 60000 Loss 2.2253174781799316 in 15.069385\n",
      "saving model with loss 2.2253174781799316\n",
      "Epoch 23 Batch 60500 Loss 2.232907295227051 in 15.135389\n",
      "Epoch 23 Batch 61000 Loss 2.1154122352600098 in 15.069077\n",
      "saving model with loss 2.1154122352600098\n",
      "Epoch 23 Batch 61500 Loss 2.172450065612793 in 15.149119\n",
      "Epoch 23 Batch 62000 Loss 2.3042032718658447 in 15.063973\n",
      "saving model with loss 2.3042032718658447\n",
      "Epoch 23 Batch 62500 Loss 2.1934432983398438 in 15.146208\n",
      "Epoch 23 Batch 63000 Loss 2.4240965843200684 in 15.070947\n",
      "saving model with loss 2.4240965843200684\n",
      "Epoch 23 Batch 63500 Loss 2.4863877296447754 in 15.148290\n",
      "Epoch 23 Batch 64000 Loss 2.160808563232422 in 15.059814\n",
      "saving model with loss 2.160808563232422\n",
      "Epoch 23 Batch 64500 Loss 2.128427267074585 in 15.141993\n",
      "Epoch 23 Batch 65000 Loss 2.186058759689331 in 15.077017\n",
      "saving model with loss 2.186058759689331\n",
      "Epoch 23 Batch 65500 Loss 2.196394681930542 in 15.154893\n",
      "Epoch 23 Batch 66000 Loss 2.170200824737549 in 15.091368\n",
      "saving model with loss 2.170200824737549\n",
      "Epoch 23 Batch 66500 Loss 2.196136951446533 in 15.139714\n",
      "Epoch 23 Batch 67000 Loss 2.1713359355926514 in 15.072373\n",
      "saving model with loss 2.1713359355926514\n",
      "Epoch 23 Batch 67500 Loss 2.1721608638763428 in 15.142776\n",
      "Epoch 23 Batch 68000 Loss 2.263510227203369 in 15.068328\n",
      "saving model with loss 2.263510227203369\n",
      "Epoch 23 Batch 68500 Loss 2.254718780517578 in 15.144870\n",
      "Epoch 23 Batch 69000 Loss 2.400383710861206 in 15.066139\n",
      "saving model with loss 2.400383710861206\n",
      "Epoch 23 Batch 69500 Loss 2.4030344486236572 in 15.138399\n",
      "Epoch 23 Batch 70000 Loss 2.2002968788146973 in 15.130911\n",
      "saving model with loss 2.2002968788146973\n",
      "Epoch 23 Batch 70500 Loss 2.2165918350219727 in 15.156891\n",
      "Epoch 24 Loss 2.375441551208496\n",
      "Time taken for 1 epoch 2130.8865027427673\n",
      "number of batches : 70553\n",
      "Epoch 24 Batch 0 Loss 2.1399760246276855 in 1.174898\n",
      "saving model with loss 2.1399760246276855\n",
      "Epoch 24 Batch 500 Loss 2.336388349533081 in 15.147330\n",
      "Epoch 24 Batch 1000 Loss 2.326046943664551 in 15.074112\n",
      "saving model with loss 2.326046943664551\n",
      "Epoch 24 Batch 1500 Loss 2.275191307067871 in 15.158208\n",
      "Epoch 24 Batch 2000 Loss 2.24816632270813 in 15.059150\n",
      "saving model with loss 2.24816632270813\n",
      "Epoch 24 Batch 2500 Loss 2.2462306022644043 in 15.126375\n",
      "Epoch 24 Batch 3000 Loss 2.313542604446411 in 15.192893\n",
      "saving model with loss 2.313542604446411\n",
      "Epoch 24 Batch 3500 Loss 2.362349510192871 in 15.363205\n",
      "Epoch 24 Batch 4000 Loss 2.2883009910583496 in 15.061675\n",
      "saving model with loss 2.2883009910583496\n",
      "Epoch 24 Batch 4500 Loss 2.2705819606781006 in 15.137830\n",
      "Epoch 24 Batch 5000 Loss 2.1557960510253906 in 15.072369\n",
      "saving model with loss 2.1557960510253906\n",
      "Epoch 24 Batch 5500 Loss 2.376028060913086 in 15.159680\n",
      "Epoch 24 Batch 6000 Loss 2.23537015914917 in 15.063359\n",
      "saving model with loss 2.23537015914917\n",
      "Epoch 24 Batch 6500 Loss 2.293336868286133 in 15.130964\n",
      "Epoch 24 Batch 7000 Loss 2.247814893722534 in 15.072548\n",
      "saving model with loss 2.247814893722534\n",
      "Epoch 24 Batch 7500 Loss 2.197521448135376 in 15.132741\n",
      "Epoch 24 Batch 8000 Loss 2.2867679595947266 in 15.068427\n",
      "saving model with loss 2.2867679595947266\n",
      "Epoch 24 Batch 8500 Loss 2.3087377548217773 in 15.132811\n",
      "Epoch 24 Batch 9000 Loss 2.364304304122925 in 15.072404\n",
      "saving model with loss 2.364304304122925\n",
      "Epoch 24 Batch 9500 Loss 2.174773693084717 in 15.136198\n",
      "Epoch 24 Batch 10000 Loss 2.296989917755127 in 15.071785\n",
      "saving model with loss 2.296989917755127\n",
      "Epoch 24 Batch 10500 Loss 2.138852596282959 in 15.124649\n",
      "Epoch 24 Batch 11000 Loss 2.2729763984680176 in 15.070277\n",
      "saving model with loss 2.2729763984680176\n",
      "Epoch 24 Batch 11500 Loss 2.0780715942382812 in 15.147854\n",
      "Epoch 24 Batch 12000 Loss 2.3034565448760986 in 15.064399\n",
      "saving model with loss 2.3034565448760986\n",
      "Epoch 24 Batch 12500 Loss 2.248973846435547 in 15.134290\n",
      "Epoch 24 Batch 13000 Loss 2.3200786113739014 in 15.073243\n",
      "saving model with loss 2.3200786113739014\n",
      "Epoch 24 Batch 13500 Loss 2.0817551612854004 in 15.175869\n",
      "Epoch 24 Batch 14000 Loss 2.216738224029541 in 15.076531\n",
      "saving model with loss 2.216738224029541\n",
      "Epoch 24 Batch 14500 Loss 2.3206048011779785 in 15.137436\n",
      "Epoch 24 Batch 15000 Loss 2.3783671855926514 in 15.067197\n",
      "saving model with loss 2.3783671855926514\n",
      "Epoch 24 Batch 15500 Loss 2.367922306060791 in 15.140060\n",
      "Epoch 24 Batch 16000 Loss 2.305607318878174 in 15.067545\n",
      "saving model with loss 2.305607318878174\n",
      "Epoch 24 Batch 16500 Loss 2.2804818153381348 in 15.129687\n",
      "Epoch 24 Batch 17000 Loss 2.25394606590271 in 15.082865\n",
      "saving model with loss 2.25394606590271\n",
      "Epoch 24 Batch 17500 Loss 2.347687244415283 in 15.150555\n",
      "Epoch 24 Batch 18000 Loss 2.0688114166259766 in 15.062703\n",
      "saving model with loss 2.0688114166259766\n",
      "Epoch 24 Batch 18500 Loss 2.231733560562134 in 15.144138\n",
      "Epoch 24 Batch 19000 Loss 2.4548096656799316 in 15.062155\n",
      "saving model with loss 2.4548096656799316\n",
      "Epoch 24 Batch 19500 Loss 2.381382465362549 in 15.127606\n",
      "Epoch 24 Batch 20000 Loss 2.302849292755127 in 15.066077\n",
      "saving model with loss 2.302849292755127\n",
      "Epoch 24 Batch 20500 Loss 2.258171558380127 in 15.145802\n",
      "Epoch 24 Batch 21000 Loss 2.293544292449951 in 15.064575\n",
      "saving model with loss 2.293544292449951\n",
      "Epoch 24 Batch 21500 Loss 2.149460554122925 in 15.140411\n",
      "Epoch 24 Batch 22000 Loss 2.297478675842285 in 15.072402\n",
      "saving model with loss 2.297478675842285\n",
      "Epoch 24 Batch 22500 Loss 2.0696139335632324 in 15.144024\n",
      "Epoch 24 Batch 23000 Loss 2.2756340503692627 in 15.062015\n",
      "saving model with loss 2.2756340503692627\n",
      "Epoch 24 Batch 23500 Loss 2.2026267051696777 in 15.146554\n",
      "Epoch 24 Batch 24000 Loss 2.2484560012817383 in 15.067530\n",
      "saving model with loss 2.2484560012817383\n",
      "Epoch 24 Batch 24500 Loss 2.1132984161376953 in 15.144319\n",
      "Epoch 24 Batch 25000 Loss 2.376875162124634 in 15.077918\n",
      "saving model with loss 2.376875162124634\n",
      "Epoch 24 Batch 25500 Loss 2.2611653804779053 in 15.177984\n",
      "Epoch 24 Batch 26000 Loss 2.209160327911377 in 15.117196\n",
      "saving model with loss 2.209160327911377\n",
      "Epoch 24 Batch 26500 Loss 2.3330271244049072 in 15.130323\n",
      "Epoch 24 Batch 27000 Loss 2.052604913711548 in 15.079852\n",
      "saving model with loss 2.052604913711548\n",
      "Epoch 24 Batch 27500 Loss 2.3174517154693604 in 15.134476\n",
      "Epoch 24 Batch 28000 Loss 2.3756537437438965 in 15.085142\n",
      "saving model with loss 2.3756537437438965\n",
      "Epoch 24 Batch 28500 Loss 2.0739645957946777 in 15.144092\n",
      "Epoch 24 Batch 29000 Loss 2.306758403778076 in 15.068813\n",
      "saving model with loss 2.306758403778076\n",
      "Epoch 24 Batch 29500 Loss 2.0750365257263184 in 15.170652\n",
      "Epoch 24 Batch 30000 Loss 2.281442403793335 in 15.070813\n",
      "saving model with loss 2.281442403793335\n",
      "Epoch 24 Batch 30500 Loss 2.0675320625305176 in 15.152000\n",
      "Epoch 24 Batch 31000 Loss 2.1465110778808594 in 15.066585\n",
      "saving model with loss 2.1465110778808594\n",
      "Epoch 24 Batch 31500 Loss 2.43784761428833 in 15.137834\n",
      "Epoch 24 Batch 32000 Loss 2.3022093772888184 in 15.089778\n",
      "saving model with loss 2.3022093772888184\n",
      "Epoch 24 Batch 32500 Loss 2.152055263519287 in 15.130233\n",
      "Epoch 24 Batch 33000 Loss 2.330667018890381 in 15.074144\n",
      "saving model with loss 2.330667018890381\n",
      "Epoch 24 Batch 33500 Loss 2.273937463760376 in 15.132839\n",
      "Epoch 24 Batch 34000 Loss 2.333386182785034 in 15.069957\n",
      "saving model with loss 2.333386182785034\n",
      "Epoch 24 Batch 34500 Loss 2.1113383769989014 in 15.150197\n",
      "Epoch 24 Batch 35000 Loss 2.3267035484313965 in 15.069822\n",
      "saving model with loss 2.3267035484313965\n",
      "Epoch 24 Batch 35500 Loss 2.2969272136688232 in 15.152825\n",
      "Epoch 24 Batch 36000 Loss 2.281259536743164 in 15.066807\n",
      "saving model with loss 2.281259536743164\n",
      "Epoch 24 Batch 36500 Loss 2.147934675216675 in 15.149398\n",
      "Epoch 24 Batch 37000 Loss 2.431673526763916 in 15.085628\n",
      "saving model with loss 2.431673526763916\n",
      "Epoch 24 Batch 37500 Loss 2.127737522125244 in 15.137129\n",
      "Epoch 24 Batch 38000 Loss 2.333775758743286 in 15.068669\n",
      "saving model with loss 2.333775758743286\n",
      "Epoch 24 Batch 38500 Loss 2.195552349090576 in 15.151127\n",
      "Epoch 24 Batch 39000 Loss 2.3840315341949463 in 15.099823\n",
      "saving model with loss 2.3840315341949463\n",
      "Epoch 24 Batch 39500 Loss 2.069434404373169 in 15.150843\n",
      "Epoch 24 Batch 40000 Loss 2.2925727367401123 in 15.073552\n",
      "saving model with loss 2.2925727367401123\n",
      "Epoch 24 Batch 40500 Loss 2.3989615440368652 in 15.149251\n",
      "Epoch 24 Batch 41000 Loss 2.214468479156494 in 15.071914\n",
      "saving model with loss 2.214468479156494\n",
      "Epoch 24 Batch 41500 Loss 2.3903212547302246 in 15.131041\n",
      "Epoch 24 Batch 42000 Loss 2.30749249458313 in 15.078640\n",
      "saving model with loss 2.30749249458313\n",
      "Epoch 24 Batch 42500 Loss 2.2521157264709473 in 15.144642\n",
      "Epoch 24 Batch 43000 Loss 2.184098720550537 in 15.086677\n",
      "saving model with loss 2.184098720550537\n",
      "Epoch 24 Batch 43500 Loss 2.215390682220459 in 15.153735\n",
      "Epoch 24 Batch 44000 Loss 2.152233600616455 in 15.079664\n",
      "saving model with loss 2.152233600616455\n",
      "Epoch 24 Batch 44500 Loss 2.234711170196533 in 15.138927\n",
      "Epoch 24 Batch 45000 Loss 2.375739574432373 in 15.065395\n",
      "saving model with loss 2.375739574432373\n",
      "Epoch 24 Batch 45500 Loss 2.1868350505828857 in 15.145951\n",
      "Epoch 24 Batch 46000 Loss 2.454144239425659 in 15.073339\n",
      "saving model with loss 2.454144239425659\n",
      "Epoch 24 Batch 46500 Loss 2.0924787521362305 in 15.140277\n",
      "Epoch 24 Batch 47000 Loss 2.317394971847534 in 15.077539\n",
      "saving model with loss 2.317394971847534\n",
      "Epoch 24 Batch 47500 Loss 2.1196670532226562 in 15.135048\n",
      "Epoch 24 Batch 48000 Loss 2.2124133110046387 in 15.096682\n",
      "saving model with loss 2.2124133110046387\n",
      "Epoch 24 Batch 48500 Loss 2.1840922832489014 in 15.146040\n",
      "Epoch 24 Batch 49000 Loss 2.304306745529175 in 15.063008\n",
      "saving model with loss 2.304306745529175\n",
      "Epoch 24 Batch 49500 Loss 2.2483415603637695 in 15.119610\n",
      "Epoch 24 Batch 50000 Loss 2.313922882080078 in 15.075818\n",
      "saving model with loss 2.313922882080078\n",
      "Epoch 24 Batch 50500 Loss 2.192997455596924 in 15.213160\n",
      "Epoch 24 Batch 51000 Loss 2.2678627967834473 in 15.043593\n",
      "saving model with loss 2.2678627967834473\n",
      "Epoch 24 Batch 51500 Loss 2.171956777572632 in 15.106077\n",
      "Epoch 24 Batch 52000 Loss 2.2659988403320312 in 15.049273\n",
      "saving model with loss 2.2659988403320312\n",
      "Epoch 24 Batch 52500 Loss 2.202350616455078 in 15.118014\n",
      "Epoch 24 Batch 53000 Loss 2.238381862640381 in 15.039372\n",
      "saving model with loss 2.238381862640381\n",
      "Epoch 24 Batch 53500 Loss 2.0976269245147705 in 15.108013\n",
      "Epoch 24 Batch 54000 Loss 2.244955062866211 in 15.045868\n",
      "saving model with loss 2.244955062866211\n",
      "Epoch 24 Batch 54500 Loss 2.3401777744293213 in 15.100050\n",
      "Epoch 24 Batch 55000 Loss 2.196164846420288 in 15.051400\n",
      "saving model with loss 2.196164846420288\n",
      "Epoch 24 Batch 55500 Loss 2.33341646194458 in 15.949514\n",
      "Epoch 24 Batch 56000 Loss 2.3921358585357666 in 15.523420\n",
      "saving model with loss 2.3921358585357666\n",
      "Epoch 24 Batch 56500 Loss 2.199465274810791 in 15.887368\n",
      "Epoch 24 Batch 57000 Loss 2.392707586288452 in 15.338924\n",
      "saving model with loss 2.392707586288452\n",
      "Epoch 24 Batch 57500 Loss 2.276125192642212 in 15.367597\n",
      "Epoch 24 Batch 58000 Loss 2.207711935043335 in 15.480557\n",
      "saving model with loss 2.207711935043335\n",
      "Epoch 24 Batch 58500 Loss 2.105686664581299 in 15.302878\n",
      "Epoch 24 Batch 59000 Loss 2.070720672607422 in 12.089039\n",
      "saving model with loss 2.070720672607422\n",
      "Epoch 24 Batch 59500 Loss 2.2085652351379395 in 15.164041\n",
      "Epoch 24 Batch 60000 Loss 2.280844211578369 in 15.678339\n",
      "saving model with loss 2.280844211578369\n",
      "Epoch 24 Batch 60500 Loss 2.1861798763275146 in 15.387012\n",
      "Epoch 24 Batch 61000 Loss 2.209089756011963 in 15.183833\n",
      "saving model with loss 2.209089756011963\n",
      "Epoch 24 Batch 61500 Loss 2.2313458919525146 in 15.209409\n",
      "Epoch 24 Batch 62000 Loss 2.272552013397217 in 15.688329\n",
      "saving model with loss 2.272552013397217\n",
      "Epoch 24 Batch 62500 Loss 2.28761625289917 in 15.837495\n",
      "Epoch 24 Batch 63000 Loss 2.173208236694336 in 15.627352\n",
      "saving model with loss 2.173208236694336\n",
      "Epoch 24 Batch 63500 Loss 2.07474946975708 in 15.798413\n",
      "Epoch 24 Batch 64000 Loss 2.211655616760254 in 16.311111\n",
      "saving model with loss 2.211655616760254\n",
      "Epoch 24 Batch 64500 Loss 2.1093993186950684 in 15.734316\n",
      "Epoch 24 Batch 65000 Loss 2.113687515258789 in 15.491095\n",
      "saving model with loss 2.113687515258789\n",
      "Epoch 24 Batch 65500 Loss 2.2532448768615723 in 15.532881\n",
      "Epoch 24 Batch 66000 Loss 2.143929958343506 in 15.712968\n",
      "saving model with loss 2.143929958343506\n",
      "Epoch 24 Batch 66500 Loss 2.107783794403076 in 16.695565\n",
      "Epoch 24 Batch 67000 Loss 2.2293925285339355 in 16.702213\n",
      "saving model with loss 2.2293925285339355\n",
      "Epoch 24 Batch 67500 Loss 2.149296998977661 in 16.478547\n",
      "Epoch 24 Batch 68000 Loss 2.157464027404785 in 15.468669\n",
      "saving model with loss 2.157464027404785\n",
      "Epoch 24 Batch 68500 Loss 2.1593830585479736 in 15.511626\n",
      "Epoch 24 Batch 69000 Loss 2.3036835193634033 in 16.071056\n",
      "saving model with loss 2.3036835193634033\n",
      "Epoch 24 Batch 69500 Loss 2.3109207153320312 in 15.438449\n",
      "Epoch 24 Batch 70000 Loss 2.3528919219970703 in 15.703376\n",
      "saving model with loss 2.3528919219970703\n",
      "Epoch 24 Batch 70500 Loss 2.248979091644287 in 15.470034\n",
      "Epoch 25 Loss 2.2136549949645996\n",
      "Time taken for 1 epoch 2147.7177143096924\n"
     ]
    }
   ],
   "source": [
    "model.train('.\\\\vocabulary.voc', tokenized_files, 25)\n",
    "#start 17:54"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# model.prepare_predictions('.\\\\vocabulary', '.\\\\checkpoints\\\\model.h5')\n",
    "# model.get_prediction('import numpy', 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}