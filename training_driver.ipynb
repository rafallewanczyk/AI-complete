{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import src.utils as utils\n",
    "import numpy as np\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# import src.generate_vocabulary as gv\n",
    "#\n",
    "# with open('.\\\\data\\\\my_train.txt') as f:\n",
    "#     train_dirs = f.readlines()\n",
    "# train_dirs = [re.sub(r'/', r'\\\\', ('.\\\\data\\\\'+d)[:-1]) for d in train_dirs ]\n",
    "#\n",
    "# with open('.\\\\data\\\\my_eval.txt') as f:\n",
    "#     eval_dirs = f.readlines()\n",
    "# eval_dirs = [re.sub(r'/', r'\\\\', ('.\\\\data\\\\'+d)[:-1]) for d in eval_dirs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# v = gv.generate_vocabulary(train_dirs + eval_dirs, ['py'], 20000, name='.\\\\vocabulary')\n",
    "# print(f'generated {len(v)} words')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# v"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# tokenized_files = utils.tokenize_data(train_dirs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# with open('.\\\\tokenized_files', 'wb') as f:\n",
    "#     pickle.dump(tokenized_files, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9167\n"
     ]
    },
    {
     "data": {
      "text/plain": "13233740"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\\\tokenized_files', 'rb') as f:\n",
    "    tokenized_files = pickle.load(f)\n",
    "\n",
    "print(len(tokenized_files))\n",
    "sum([len(x[1]) for x in tokenized_files])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches : 102704\n",
      "Epoch 0 Batch 0 Loss 9.903509140014648 in 2.030741\n",
      "saving model with loss 9.903509140014648\n",
      "Epoch 0 Batch 500 Loss 3.621448516845703 in 25.514809\n",
      "Epoch 0 Batch 1000 Loss 3.1629748344421387 in 26.490258\n",
      "saving model with loss 3.1629748344421387\n",
      "Epoch 0 Batch 1500 Loss 2.8412880897521973 in 25.994091\n",
      "Epoch 0 Batch 2000 Loss 2.9527289867401123 in 27.397132\n",
      "saving model with loss 2.9527289867401123\n",
      "Epoch 0 Batch 2500 Loss 2.646019220352173 in 27.254199\n",
      "Epoch 0 Batch 3000 Loss 2.479398488998413 in 28.629321\n",
      "saving model with loss 2.479398488998413\n",
      "Epoch 0 Batch 3500 Loss 2.5095725059509277 in 28.353095\n",
      "Epoch 0 Batch 4000 Loss 2.4283244609832764 in 28.180382\n",
      "saving model with loss 2.4283244609832764\n",
      "Epoch 0 Batch 4500 Loss 2.5605087280273438 in 27.799984\n",
      "Epoch 0 Batch 5000 Loss 2.4885685443878174 in 27.012017\n",
      "saving model with loss 2.4885685443878174\n",
      "Epoch 0 Batch 5500 Loss 2.3995165824890137 in 26.748384\n",
      "Epoch 0 Batch 6000 Loss 2.419912815093994 in 26.603703\n",
      "saving model with loss 2.419912815093994\n",
      "Epoch 0 Batch 6500 Loss 2.359870433807373 in 26.300020\n",
      "Epoch 0 Batch 7000 Loss 2.2692654132843018 in 26.176065\n",
      "saving model with loss 2.2692654132843018\n",
      "Epoch 0 Batch 7500 Loss 2.354400634765625 in 26.287804\n",
      "Epoch 0 Batch 8000 Loss 2.4695534706115723 in 25.732825\n",
      "saving model with loss 2.4695534706115723\n",
      "Epoch 0 Batch 8500 Loss 2.2559638023376465 in 26.838003\n",
      "Epoch 0 Batch 9000 Loss 2.4254212379455566 in 26.853784\n",
      "saving model with loss 2.4254212379455566\n",
      "Epoch 0 Batch 9500 Loss 2.3076276779174805 in 27.372808\n",
      "Epoch 0 Batch 10000 Loss 2.4341115951538086 in 27.422786\n",
      "saving model with loss 2.4341115951538086\n",
      "Epoch 0 Batch 10500 Loss 2.3824987411499023 in 26.372379\n",
      "Epoch 0 Batch 11000 Loss 2.2455239295959473 in 28.001086\n",
      "saving model with loss 2.2455239295959473\n",
      "Epoch 0 Batch 11500 Loss 2.2540206909179688 in 28.980081\n",
      "Epoch 0 Batch 12000 Loss 2.199242115020752 in 29.201738\n",
      "saving model with loss 2.199242115020752\n",
      "Epoch 0 Batch 12500 Loss 2.2496683597564697 in 27.896372\n",
      "Epoch 0 Batch 13000 Loss 2.3240106105804443 in 27.140009\n",
      "saving model with loss 2.3240106105804443\n",
      "Epoch 0 Batch 13500 Loss 2.2523458003997803 in 26.949860\n",
      "Epoch 0 Batch 14000 Loss 2.2798447608947754 in 26.998211\n",
      "saving model with loss 2.2798447608947754\n",
      "Epoch 0 Batch 14500 Loss 2.276519298553467 in 27.912926\n",
      "Epoch 0 Batch 15000 Loss 2.251955986022949 in 26.847739\n",
      "saving model with loss 2.251955986022949\n",
      "Epoch 0 Batch 15500 Loss 2.024616003036499 in 27.022860\n",
      "Epoch 0 Batch 16000 Loss 2.3483643531799316 in 26.887242\n",
      "saving model with loss 2.3483643531799316\n",
      "Epoch 0 Batch 16500 Loss 2.1479344367980957 in 27.030105\n",
      "Epoch 0 Batch 17000 Loss 2.2304329872131348 in 26.743478\n",
      "saving model with loss 2.2304329872131348\n",
      "Epoch 0 Batch 17500 Loss 2.2213528156280518 in 28.543354\n",
      "Epoch 0 Batch 18000 Loss 2.129661798477173 in 29.801820\n",
      "saving model with loss 2.129661798477173\n",
      "Epoch 0 Batch 18500 Loss 1.981574296951294 in 30.934978\n",
      "Epoch 0 Batch 19000 Loss 2.2050652503967285 in 30.236311\n",
      "saving model with loss 2.2050652503967285\n",
      "Epoch 0 Batch 19500 Loss 2.1747922897338867 in 29.773849\n",
      "Epoch 0 Batch 20000 Loss 2.0445590019226074 in 28.829289\n",
      "saving model with loss 2.0445590019226074\n",
      "Epoch 0 Batch 20500 Loss 2.286555767059326 in 27.756041\n",
      "Epoch 0 Batch 21000 Loss 2.1131529808044434 in 27.498569\n",
      "saving model with loss 2.1131529808044434\n",
      "Epoch 0 Batch 21500 Loss 2.115844249725342 in 27.606193\n",
      "Epoch 0 Batch 22000 Loss 2.375704288482666 in 27.502335\n",
      "saving model with loss 2.375704288482666\n",
      "Epoch 0 Batch 22500 Loss 2.1057257652282715 in 28.950006\n",
      "Epoch 0 Batch 23000 Loss 2.20381236076355 in 29.475842\n",
      "saving model with loss 2.20381236076355\n",
      "Epoch 0 Batch 23500 Loss 2.121574640274048 in 30.377088\n",
      "Epoch 0 Batch 24000 Loss 1.955474853515625 in 29.466300\n",
      "saving model with loss 1.955474853515625\n",
      "Epoch 0 Batch 24500 Loss 2.1638340950012207 in 29.924993\n",
      "Epoch 0 Batch 25000 Loss 2.0513205528259277 in 30.797689\n",
      "saving model with loss 2.0513205528259277\n",
      "Epoch 0 Batch 25500 Loss 2.108315944671631 in 29.377485\n",
      "Epoch 0 Batch 26000 Loss 2.2856600284576416 in 29.287724\n",
      "saving model with loss 2.2856600284576416\n",
      "Epoch 0 Batch 26500 Loss 2.1782779693603516 in 29.452284\n",
      "Epoch 0 Batch 27000 Loss 2.085847854614258 in 29.905074\n",
      "saving model with loss 2.085847854614258\n",
      "Epoch 0 Batch 27500 Loss 2.0525012016296387 in 27.814662\n",
      "Epoch 0 Batch 28000 Loss 2.1350345611572266 in 28.021621\n",
      "saving model with loss 2.1350345611572266\n",
      "Epoch 0 Batch 28500 Loss 2.2109732627868652 in 29.692991\n",
      "Epoch 0 Batch 29000 Loss 2.1833975315093994 in 29.094779\n",
      "saving model with loss 2.1833975315093994\n",
      "Epoch 0 Batch 29500 Loss 2.16894268989563 in 27.841958\n",
      "Epoch 0 Batch 30000 Loss 2.1081411838531494 in 27.681490\n",
      "saving model with loss 2.1081411838531494\n",
      "Epoch 0 Batch 30500 Loss 1.9698177576065063 in 30.024407\n",
      "Epoch 0 Batch 31000 Loss 2.162285089492798 in 33.252464\n",
      "saving model with loss 2.162285089492798\n",
      "Epoch 0 Batch 31500 Loss 2.100968599319458 in 31.925827\n",
      "Epoch 0 Batch 32000 Loss 1.9841725826263428 in 33.883134\n",
      "saving model with loss 1.9841725826263428\n",
      "Epoch 0 Batch 32500 Loss 2.257579803466797 in 33.110292\n",
      "Epoch 0 Batch 33000 Loss 2.066389322280884 in 33.109033\n",
      "saving model with loss 2.066389322280884\n",
      "Epoch 0 Batch 33500 Loss 2.1661665439605713 in 34.204885\n",
      "Epoch 0 Batch 34000 Loss 2.12506103515625 in 34.116531\n",
      "saving model with loss 2.12506103515625\n",
      "Epoch 0 Batch 34500 Loss 2.186209201812744 in 35.676538\n",
      "Epoch 0 Batch 35000 Loss 1.9442205429077148 in 35.142020\n",
      "saving model with loss 1.9442205429077148\n",
      "Epoch 0 Batch 35500 Loss 2.105785846710205 in 33.902514\n",
      "Epoch 0 Batch 36000 Loss 2.0621204376220703 in 30.430197\n",
      "saving model with loss 2.0621204376220703\n",
      "Epoch 0 Batch 36500 Loss 2.1898486614227295 in 32.821866\n",
      "Epoch 0 Batch 37000 Loss 1.8859764337539673 in 34.158440\n",
      "saving model with loss 1.8859764337539673\n",
      "Epoch 0 Batch 37500 Loss 2.1355576515197754 in 34.159496\n",
      "Epoch 0 Batch 38000 Loss 2.163062810897827 in 34.163874\n",
      "saving model with loss 2.163062810897827\n",
      "Epoch 0 Batch 38500 Loss 1.8217811584472656 in 35.580779\n",
      "Epoch 0 Batch 39000 Loss 2.0750794410705566 in 34.885741\n",
      "saving model with loss 2.0750794410705566\n",
      "Epoch 0 Batch 39500 Loss 2.142122268676758 in 36.460746\n",
      "Epoch 0 Batch 40000 Loss 2.165630340576172 in 37.171386\n",
      "saving model with loss 2.165630340576172\n",
      "Epoch 0 Batch 40500 Loss 1.9458818435668945 in 39.702020\n",
      "Epoch 0 Batch 41000 Loss 2.1670169830322266 in 40.282227\n",
      "saving model with loss 2.1670169830322266\n",
      "Epoch 0 Batch 41500 Loss 1.981035590171814 in 37.990807\n",
      "Epoch 0 Batch 42000 Loss 2.0515260696411133 in 37.809310\n",
      "saving model with loss 2.0515260696411133\n",
      "Epoch 0 Batch 42500 Loss 1.8071022033691406 in 35.866535\n",
      "Epoch 0 Batch 43000 Loss 2.066394805908203 in 35.209745\n",
      "saving model with loss 2.066394805908203\n",
      "Epoch 0 Batch 43500 Loss 2.0766048431396484 in 35.635789\n",
      "Epoch 0 Batch 44000 Loss 2.078295946121216 in 33.131988\n",
      "saving model with loss 2.078295946121216\n",
      "Epoch 0 Batch 44500 Loss 2.181135416030884 in 31.237898\n",
      "Epoch 0 Batch 45000 Loss 2.023632287979126 in 29.406053\n",
      "saving model with loss 2.023632287979126\n",
      "Epoch 0 Batch 45500 Loss 2.1107242107391357 in 29.995324\n",
      "Epoch 0 Batch 46000 Loss 1.9576241970062256 in 31.159476\n",
      "saving model with loss 1.9576241970062256\n",
      "Epoch 0 Batch 46500 Loss 1.9255555868148804 in 28.889128\n",
      "Epoch 0 Batch 47000 Loss 1.9717975854873657 in 28.393166\n",
      "saving model with loss 1.9717975854873657\n",
      "Epoch 0 Batch 47500 Loss 2.036263942718506 in 27.733972\n",
      "Epoch 0 Batch 48000 Loss 1.9645891189575195 in 27.660086\n",
      "saving model with loss 1.9645891189575195\n",
      "Epoch 0 Batch 48500 Loss 2.224097728729248 in 27.853899\n",
      "Epoch 0 Batch 49000 Loss 1.9979356527328491 in 27.089862\n",
      "saving model with loss 1.9979356527328491\n",
      "Epoch 0 Batch 49500 Loss 2.0193064212799072 in 29.287134\n",
      "Epoch 0 Batch 50000 Loss 2.0727038383483887 in 32.916035\n",
      "saving model with loss 2.0727038383483887\n",
      "Epoch 0 Batch 50500 Loss 1.9265995025634766 in 33.381632\n",
      "Epoch 0 Batch 51000 Loss 2.086944818496704 in 29.912888\n",
      "saving model with loss 2.086944818496704\n",
      "Epoch 0 Batch 51500 Loss 1.8953012228012085 in 32.900049\n",
      "Epoch 0 Batch 52000 Loss 1.938296914100647 in 35.626444\n",
      "saving model with loss 1.938296914100647\n",
      "Epoch 0 Batch 52500 Loss 2.0025668144226074 in 35.683292\n",
      "Epoch 0 Batch 53000 Loss 1.9669755697250366 in 34.779362\n",
      "saving model with loss 1.9669755697250366\n",
      "Epoch 0 Batch 53500 Loss 2.0232176780700684 in 36.462937\n",
      "Epoch 0 Batch 54000 Loss 2.0221071243286133 in 33.021157\n",
      "saving model with loss 2.0221071243286133\n",
      "Epoch 0 Batch 54500 Loss 2.1593947410583496 in 35.225725\n",
      "Epoch 0 Batch 55000 Loss 1.9399394989013672 in 34.575083\n",
      "saving model with loss 1.9399394989013672\n",
      "Epoch 0 Batch 55500 Loss 1.8827550411224365 in 34.830857\n",
      "Epoch 0 Batch 56000 Loss 1.9696853160858154 in 34.611944\n",
      "saving model with loss 1.9696853160858154\n",
      "Epoch 0 Batch 56500 Loss 2.0113768577575684 in 34.884001\n",
      "Epoch 0 Batch 57000 Loss 1.9809681177139282 in 29.934597\n",
      "saving model with loss 1.9809681177139282\n",
      "Epoch 0 Batch 57500 Loss 2.092287302017212 in 36.490294\n",
      "Epoch 0 Batch 58000 Loss 1.9481532573699951 in 37.127131\n",
      "saving model with loss 1.9481532573699951\n",
      "Epoch 0 Batch 58500 Loss 1.9930543899536133 in 31.144373\n",
      "Epoch 0 Batch 59000 Loss 1.9167060852050781 in 30.019454\n",
      "saving model with loss 1.9167060852050781\n",
      "Epoch 0 Batch 59500 Loss 2.070120096206665 in 29.053156\n",
      "Epoch 0 Batch 60000 Loss 1.8229405879974365 in 28.811386\n",
      "saving model with loss 1.8229405879974365\n",
      "Epoch 0 Batch 60500 Loss 2.156853437423706 in 29.168579\n",
      "Epoch 0 Batch 61000 Loss 1.9720933437347412 in 28.567477\n",
      "saving model with loss 1.9720933437347412\n",
      "Epoch 0 Batch 61500 Loss 2.1638846397399902 in 28.062444\n",
      "Epoch 0 Batch 62000 Loss 2.0227890014648438 in 28.456675\n",
      "saving model with loss 2.0227890014648438\n",
      "Epoch 0 Batch 62500 Loss 2.093531370162964 in 31.318776\n",
      "Epoch 0 Batch 63000 Loss 1.903865098953247 in 34.284861\n",
      "saving model with loss 1.903865098953247\n",
      "Epoch 0 Batch 63500 Loss 2.038856029510498 in 31.701578\n",
      "Epoch 0 Batch 64000 Loss 1.9273868799209595 in 30.088532\n",
      "saving model with loss 1.9273868799209595\n",
      "Epoch 0 Batch 64500 Loss 1.9949147701263428 in 34.171521\n",
      "Epoch 0 Batch 65000 Loss 1.9604896306991577 in 30.514925\n",
      "saving model with loss 1.9604896306991577\n",
      "Epoch 0 Batch 65500 Loss 1.9940950870513916 in 32.240903\n",
      "Epoch 0 Batch 66000 Loss 1.9048341512680054 in 32.000103\n",
      "saving model with loss 1.9048341512680054\n",
      "Epoch 0 Batch 66500 Loss 1.936958909034729 in 32.638386\n",
      "Epoch 0 Batch 67000 Loss 1.9187084436416626 in 34.593399\n",
      "saving model with loss 1.9187084436416626\n",
      "Epoch 0 Batch 67500 Loss 2.013162851333618 in 34.275645\n",
      "Epoch 0 Batch 68000 Loss 2.037802219390869 in 29.241480\n",
      "saving model with loss 2.037802219390869\n",
      "Epoch 0 Batch 68500 Loss 2.053675889968872 in 28.675479\n",
      "Epoch 0 Batch 69000 Loss 1.7465250492095947 in 28.277343\n",
      "saving model with loss 1.7465250492095947\n",
      "Epoch 0 Batch 69500 Loss 1.973785161972046 in 31.865018\n",
      "Epoch 0 Batch 70000 Loss 1.9220085144042969 in 28.348387\n",
      "saving model with loss 1.9220085144042969\n",
      "Epoch 0 Batch 70500 Loss 2.00642991065979 in 28.484103\n",
      "Epoch 0 Batch 71000 Loss 2.0306572914123535 in 29.244561\n",
      "saving model with loss 2.0306572914123535\n",
      "Epoch 0 Batch 71500 Loss 2.1366162300109863 in 32.581270\n",
      "Epoch 0 Batch 72000 Loss 1.9108946323394775 in 30.489370\n",
      "saving model with loss 1.9108946323394775\n",
      "Epoch 0 Batch 72500 Loss 1.9445221424102783 in 30.821453\n",
      "Epoch 0 Batch 73000 Loss 2.045673131942749 in 29.963810\n",
      "saving model with loss 2.045673131942749\n",
      "Epoch 0 Batch 73500 Loss 1.9174144268035889 in 31.404596\n",
      "Epoch 0 Batch 74000 Loss 1.9863914251327515 in 36.457511\n",
      "saving model with loss 1.9863914251327515\n",
      "Epoch 0 Batch 74500 Loss 1.8378604650497437 in 37.507975\n",
      "Epoch 0 Batch 75000 Loss 1.8899129629135132 in 35.827453\n",
      "saving model with loss 1.8899129629135132\n",
      "Epoch 0 Batch 75500 Loss 1.7942845821380615 in 33.067543\n",
      "Epoch 0 Batch 76000 Loss 2.0706703662872314 in 35.411145\n",
      "saving model with loss 2.0706703662872314\n",
      "Epoch 0 Batch 76500 Loss 2.0672554969787598 in 33.420080\n",
      "Epoch 0 Batch 77000 Loss 2.1088032722473145 in 31.347843\n",
      "saving model with loss 2.1088032722473145\n",
      "Epoch 0 Batch 77500 Loss 1.9387775659561157 in 33.229541\n",
      "Epoch 0 Batch 78000 Loss 2.035125732421875 in 30.222444\n",
      "saving model with loss 2.035125732421875\n",
      "Epoch 0 Batch 78500 Loss 1.8880951404571533 in 27.631812\n",
      "Epoch 0 Batch 79000 Loss 1.98422110080719 in 27.483120\n",
      "saving model with loss 1.98422110080719\n",
      "Epoch 0 Batch 79500 Loss 1.8579607009887695 in 27.940672\n",
      "Epoch 0 Batch 80000 Loss 1.9207311868667603 in 27.634830\n",
      "saving model with loss 1.9207311868667603\n",
      "Epoch 0 Batch 80500 Loss 2.113386869430542 in 27.764066\n",
      "Epoch 0 Batch 81000 Loss 1.990051507949829 in 27.875006\n",
      "saving model with loss 1.990051507949829\n",
      "Epoch 0 Batch 81500 Loss 1.9069116115570068 in 29.374492\n",
      "Epoch 0 Batch 82000 Loss 1.9978126287460327 in 27.194063\n",
      "saving model with loss 1.9978126287460327\n",
      "Epoch 0 Batch 82500 Loss 1.911318063735962 in 25.705692\n",
      "Epoch 0 Batch 83000 Loss 1.982561469078064 in 28.306423\n",
      "saving model with loss 1.982561469078064\n",
      "Epoch 0 Batch 83500 Loss 2.0012753009796143 in 30.614178\n",
      "Epoch 0 Batch 84000 Loss 2.11369252204895 in 29.393472\n",
      "saving model with loss 2.11369252204895\n",
      "Epoch 0 Batch 84500 Loss 1.9721734523773193 in 29.795344\n",
      "Epoch 0 Batch 85000 Loss 1.8727785348892212 in 29.351592\n",
      "saving model with loss 1.8727785348892212\n",
      "Epoch 0 Batch 85500 Loss 1.8160736560821533 in 30.389778\n",
      "Epoch 0 Batch 86000 Loss 1.908532738685608 in 30.034266\n",
      "saving model with loss 1.908532738685608\n",
      "Epoch 0 Batch 86500 Loss 1.8959792852401733 in 31.710250\n",
      "Epoch 0 Batch 87000 Loss 1.9697110652923584 in 27.330675\n",
      "saving model with loss 1.9697110652923584\n",
      "Epoch 0 Batch 87500 Loss 1.8746602535247803 in 28.907641\n",
      "Epoch 0 Batch 88000 Loss 1.9485785961151123 in 30.708625\n",
      "saving model with loss 1.9485785961151123\n",
      "Epoch 0 Batch 88500 Loss 2.0047354698181152 in 29.473397\n",
      "Epoch 0 Batch 89000 Loss 1.7976796627044678 in 28.249904\n",
      "saving model with loss 1.7976796627044678\n",
      "Epoch 0 Batch 89500 Loss 1.8990846872329712 in 26.316685\n",
      "Epoch 0 Batch 90000 Loss 1.8451519012451172 in 26.349567\n",
      "saving model with loss 1.8451519012451172\n",
      "Epoch 0 Batch 90500 Loss 1.7991176843643188 in 27.323432\n",
      "Epoch 0 Batch 91000 Loss 1.9123748540878296 in 28.365191\n",
      "saving model with loss 1.9123748540878296\n",
      "Epoch 0 Batch 91500 Loss 1.9351775646209717 in 29.670069\n",
      "Epoch 0 Batch 92000 Loss 1.9091732501983643 in 27.440757\n",
      "saving model with loss 1.9091732501983643\n",
      "Epoch 0 Batch 92500 Loss 1.9691162109375 in 27.219308\n",
      "Epoch 0 Batch 93000 Loss 2.006348133087158 in 28.498610\n",
      "saving model with loss 2.006348133087158\n",
      "Epoch 0 Batch 93500 Loss 2.0204620361328125 in 28.128073\n",
      "Epoch 0 Batch 94000 Loss 1.9354631900787354 in 27.662533\n",
      "saving model with loss 1.9354631900787354\n",
      "Epoch 0 Batch 94500 Loss 1.8413536548614502 in 25.940987\n",
      "Epoch 0 Batch 95000 Loss 1.841221809387207 in 25.810695\n",
      "saving model with loss 1.841221809387207\n",
      "Epoch 0 Batch 95500 Loss 2.052670955657959 in 26.314956\n",
      "Epoch 0 Batch 96000 Loss 1.916622519493103 in 25.927026\n",
      "saving model with loss 1.916622519493103\n",
      "Epoch 0 Batch 96500 Loss 1.8478294610977173 in 26.944232\n",
      "Epoch 0 Batch 97000 Loss 1.9008474349975586 in 26.927013\n",
      "saving model with loss 1.9008474349975586\n",
      "Epoch 0 Batch 97500 Loss 1.9091460704803467 in 26.728627\n",
      "Epoch 0 Batch 98000 Loss 1.8495715856552124 in 26.902203\n",
      "saving model with loss 1.8495715856552124\n",
      "Epoch 0 Batch 98500 Loss 2.0041308403015137 in 27.184376\n",
      "Epoch 0 Batch 99000 Loss 1.9227790832519531 in 28.235507\n",
      "saving model with loss 1.9227790832519531\n",
      "Epoch 0 Batch 99500 Loss 1.8780635595321655 in 28.263379\n",
      "Epoch 0 Batch 100000 Loss 1.7945730686187744 in 28.291180\n",
      "saving model with loss 1.7945730686187744\n",
      "Epoch 0 Batch 100500 Loss 1.817572832107544 in 28.271662\n",
      "Epoch 0 Batch 101000 Loss 1.843026876449585 in 27.534895\n",
      "saving model with loss 1.843026876449585\n",
      "Epoch 0 Batch 101500 Loss 2.2084555625915527 in 27.585275\n",
      "Epoch 0 Batch 102000 Loss 1.9095678329467773 in 27.165426\n",
      "saving model with loss 1.9095678329467773\n",
      "Epoch 0 Batch 102500 Loss 1.7985906600952148 in 27.734876\n",
      "Epoch 1 Loss 2.0533699989318848\n",
      "Time taken for 1 epoch 6195.3471484184265\n",
      "number of batches : 102704\n",
      "Epoch 1 Batch 0 Loss 2.279893398284912 in 2.441473\n",
      "saving model with loss 2.279893398284912\n",
      "Epoch 1 Batch 500 Loss 2.1380765438079834 in 26.761867\n",
      "Epoch 1 Batch 1000 Loss 2.0912299156188965 in 29.601169\n",
      "saving model with loss 2.0912299156188965\n",
      "Epoch 1 Batch 1500 Loss 1.9796146154403687 in 29.276814\n",
      "Epoch 1 Batch 2000 Loss 1.976305603981018 in 27.976205\n",
      "saving model with loss 1.976305603981018\n",
      "Epoch 1 Batch 2500 Loss 2.061692953109741 in 28.592645\n",
      "Epoch 1 Batch 3000 Loss 1.979796051979065 in 27.577325\n",
      "saving model with loss 1.979796051979065\n",
      "Epoch 1 Batch 3500 Loss 1.8810781240463257 in 28.016122\n",
      "Epoch 1 Batch 4000 Loss 1.8032722473144531 in 29.010437\n",
      "saving model with loss 1.8032722473144531\n",
      "Epoch 1 Batch 4500 Loss 1.8849976062774658 in 28.558685\n",
      "Epoch 1 Batch 5000 Loss 1.921446442604065 in 29.443605\n",
      "saving model with loss 1.921446442604065\n",
      "Epoch 1 Batch 5500 Loss 2.0805587768554688 in 27.746588\n",
      "Epoch 1 Batch 6000 Loss 1.9154882431030273 in 28.523844\n",
      "saving model with loss 1.9154882431030273\n",
      "Epoch 1 Batch 6500 Loss 1.76792311668396 in 29.648759\n",
      "Epoch 1 Batch 7000 Loss 1.8210662603378296 in 29.689931\n",
      "saving model with loss 1.8210662603378296\n",
      "Epoch 1 Batch 7500 Loss 1.7754865884780884 in 27.282084\n",
      "Epoch 1 Batch 8000 Loss 1.9254223108291626 in 26.772478\n",
      "saving model with loss 1.9254223108291626\n",
      "Epoch 1 Batch 8500 Loss 1.949496865272522 in 26.681821\n",
      "Epoch 1 Batch 9000 Loss 1.946987509727478 in 26.432231\n",
      "saving model with loss 1.946987509727478\n",
      "Epoch 1 Batch 9500 Loss 1.9926183223724365 in 27.157458\n",
      "Epoch 1 Batch 10000 Loss 1.957240343093872 in 26.088637\n",
      "saving model with loss 1.957240343093872\n",
      "Epoch 1 Batch 10500 Loss 2.0469114780426025 in 26.043555\n",
      "Epoch 1 Batch 11000 Loss 1.9715726375579834 in 25.963663\n",
      "saving model with loss 1.9715726375579834\n",
      "Epoch 1 Batch 11500 Loss 2.0139992237091064 in 26.896459\n",
      "Epoch 1 Batch 12000 Loss 2.1154580116271973 in 27.720667\n",
      "saving model with loss 2.1154580116271973\n",
      "Epoch 1 Batch 12500 Loss 1.9931890964508057 in 27.751433\n",
      "Epoch 1 Batch 13000 Loss 1.8579742908477783 in 27.625164\n",
      "saving model with loss 1.8579742908477783\n",
      "Epoch 1 Batch 13500 Loss 1.8854929208755493 in 27.379318\n",
      "Epoch 1 Batch 14000 Loss 1.8304659128189087 in 27.015796\n",
      "saving model with loss 1.8304659128189087\n",
      "Epoch 1 Batch 14500 Loss 2.090792179107666 in 28.051030\n",
      "Epoch 1 Batch 15000 Loss 1.917925238609314 in 28.418018\n",
      "saving model with loss 1.917925238609314\n",
      "Epoch 1 Batch 15500 Loss 1.8084716796875 in 28.702316\n",
      "Epoch 1 Batch 16000 Loss 1.993778944015503 in 28.307318\n",
      "saving model with loss 1.993778944015503\n",
      "Epoch 1 Batch 16500 Loss 2.0314202308654785 in 29.354546\n",
      "Epoch 1 Batch 17000 Loss 1.9134724140167236 in 28.149704\n",
      "saving model with loss 1.9134724140167236\n",
      "Epoch 1 Batch 17500 Loss 1.8324718475341797 in 29.117712\n",
      "Epoch 1 Batch 18000 Loss 2.022059917449951 in 27.540532\n",
      "saving model with loss 2.022059917449951\n",
      "Epoch 1 Batch 18500 Loss 1.866909384727478 in 27.542864\n",
      "Epoch 1 Batch 19000 Loss 2.0018367767333984 in 27.314548\n",
      "saving model with loss 2.0018367767333984\n",
      "Epoch 1 Batch 19500 Loss 2.036078929901123 in 27.503842\n",
      "Epoch 1 Batch 20000 Loss 2.0795652866363525 in 27.444560\n",
      "saving model with loss 2.0795652866363525\n",
      "Epoch 1 Batch 20500 Loss 2.1064324378967285 in 29.213480\n",
      "Epoch 1 Batch 21000 Loss 1.968813180923462 in 29.447746\n",
      "saving model with loss 1.968813180923462\n",
      "Epoch 1 Batch 21500 Loss 2.097543716430664 in 28.795071\n",
      "Epoch 1 Batch 22000 Loss 2.0745317935943604 in 28.559474\n",
      "saving model with loss 2.0745317935943604\n",
      "Epoch 1 Batch 22500 Loss 1.9149430990219116 in 28.844915\n",
      "Epoch 1 Batch 23000 Loss 1.8896042108535767 in 28.088923\n",
      "saving model with loss 1.8896042108535767\n",
      "Epoch 1 Batch 23500 Loss 2.0770411491394043 in 29.785119\n",
      "Epoch 1 Batch 24000 Loss 2.0326335430145264 in 29.101177\n",
      "saving model with loss 2.0326335430145264\n",
      "Epoch 1 Batch 24500 Loss 1.7994811534881592 in 29.428391\n",
      "Epoch 1 Batch 25000 Loss 1.9281336069107056 in 28.771447\n",
      "saving model with loss 1.9281336069107056\n",
      "Epoch 1 Batch 25500 Loss 1.9946365356445312 in 28.234062\n",
      "Epoch 1 Batch 26000 Loss 1.8660942316055298 in 27.926678\n",
      "saving model with loss 1.8660942316055298\n",
      "Epoch 1 Batch 26500 Loss 1.9911912679672241 in 26.899109\n",
      "Epoch 1 Batch 27000 Loss 2.0138254165649414 in 26.855226\n",
      "saving model with loss 2.0138254165649414\n",
      "Epoch 1 Batch 27500 Loss 1.780930519104004 in 27.312011\n",
      "Epoch 1 Batch 28000 Loss 1.9162498712539673 in 28.360804\n",
      "saving model with loss 1.9162498712539673\n",
      "Epoch 1 Batch 28500 Loss 1.9234081506729126 in 28.482661\n",
      "Epoch 1 Batch 29000 Loss 1.9884923696517944 in 28.513272\n",
      "saving model with loss 1.9884923696517944\n",
      "Epoch 1 Batch 29500 Loss 1.9948492050170898 in 28.247616\n",
      "Epoch 1 Batch 30000 Loss 1.9899088144302368 in 28.420412\n",
      "saving model with loss 1.9899088144302368\n",
      "Epoch 1 Batch 30500 Loss 1.7800709009170532 in 28.551684\n",
      "Epoch 1 Batch 31000 Loss 2.065744400024414 in 28.037490\n",
      "saving model with loss 2.065744400024414\n",
      "Epoch 1 Batch 31500 Loss 1.9396727085113525 in 27.701468\n",
      "Epoch 1 Batch 32000 Loss 1.9414398670196533 in 28.346802\n",
      "saving model with loss 1.9414398670196533\n",
      "Epoch 1 Batch 32500 Loss 1.9402482509613037 in 28.573117\n",
      "Epoch 1 Batch 33000 Loss 1.8717286586761475 in 29.282821\n",
      "saving model with loss 1.8717286586761475\n",
      "Epoch 1 Batch 33500 Loss 2.068019390106201 in 28.184968\n",
      "Epoch 1 Batch 34000 Loss 1.9634100198745728 in 28.791027\n",
      "saving model with loss 1.9634100198745728\n",
      "Epoch 1 Batch 34500 Loss 1.9347301721572876 in 29.245442\n",
      "Epoch 1 Batch 35000 Loss 1.9590132236480713 in 29.069381\n",
      "saving model with loss 1.9590132236480713\n",
      "Epoch 1 Batch 35500 Loss 1.871739149093628 in 26.878519\n",
      "Epoch 1 Batch 36000 Loss 1.9354374408721924 in 26.317689\n",
      "saving model with loss 1.9354374408721924\n",
      "Epoch 1 Batch 36500 Loss 1.9603290557861328 in 26.339633\n",
      "Epoch 1 Batch 37000 Loss 1.8256772756576538 in 26.355561\n",
      "saving model with loss 1.8256772756576538\n",
      "Epoch 1 Batch 37500 Loss 1.961422324180603 in 26.352570\n",
      "Epoch 1 Batch 38000 Loss 2.0172476768493652 in 26.227875\n",
      "saving model with loss 2.0172476768493652\n",
      "Epoch 1 Batch 38500 Loss 2.0083069801330566 in 26.689700\n",
      "Epoch 1 Batch 39000 Loss 1.8600623607635498 in 26.926033\n",
      "saving model with loss 1.8600623607635498\n",
      "Epoch 1 Batch 39500 Loss 2.1170825958251953 in 27.638761\n",
      "Epoch 1 Batch 40000 Loss 1.9872920513153076 in 27.541001\n",
      "saving model with loss 1.9872920513153076\n",
      "Epoch 1 Batch 40500 Loss 1.968867301940918 in 27.203098\n",
      "Epoch 1 Batch 41000 Loss 1.9901618957519531 in 26.792432\n",
      "saving model with loss 1.9901618957519531\n",
      "Epoch 1 Batch 41500 Loss 2.0412209033966064 in 26.587940\n",
      "Epoch 1 Batch 42000 Loss 1.9562135934829712 in 26.631863\n",
      "saving model with loss 1.9562135934829712\n",
      "Epoch 1 Batch 42500 Loss 1.870679497718811 in 28.364225\n",
      "Epoch 1 Batch 43000 Loss 1.8788331747055054 in 28.230578\n",
      "saving model with loss 1.8788331747055054\n",
      "Epoch 1 Batch 43500 Loss 1.8135074377059937 in 27.641603\n",
      "Epoch 1 Batch 44000 Loss 2.044708013534546 in 27.957279\n",
      "saving model with loss 2.044708013534546\n",
      "Epoch 1 Batch 44500 Loss 1.8808667659759521 in 27.779784\n",
      "Epoch 1 Batch 45000 Loss 2.017883777618408 in 27.095562\n",
      "saving model with loss 2.017883777618408\n",
      "Epoch 1 Batch 45500 Loss 1.7861363887786865 in 27.201379\n",
      "Epoch 1 Batch 46000 Loss 1.9376485347747803 in 27.173390\n",
      "saving model with loss 1.9376485347747803\n",
      "Epoch 1 Batch 46500 Loss 1.983020544052124 in 27.543061\n",
      "Epoch 1 Batch 47000 Loss 1.8886579275131226 in 27.918384\n",
      "saving model with loss 1.8886579275131226\n",
      "Epoch 1 Batch 47500 Loss 1.800169587135315 in 27.605728\n",
      "Epoch 1 Batch 48000 Loss 1.9068748950958252 in 27.366858\n",
      "saving model with loss 1.9068748950958252\n",
      "Epoch 1 Batch 48500 Loss 1.9079557657241821 in 29.302685\n",
      "Epoch 1 Batch 49000 Loss 2.0746021270751953 in 28.415205\n",
      "saving model with loss 2.0746021270751953\n",
      "Epoch 1 Batch 49500 Loss 1.8824818134307861 in 29.185029\n",
      "Epoch 1 Batch 50000 Loss 1.793379783630371 in 28.149766\n",
      "saving model with loss 1.793379783630371\n",
      "Epoch 1 Batch 50500 Loss 1.9360332489013672 in 27.726160\n",
      "Epoch 1 Batch 51000 Loss 2.010387659072876 in 27.208283\n",
      "saving model with loss 2.010387659072876\n",
      "Epoch 1 Batch 51500 Loss 1.864122986793518 in 27.499664\n",
      "Epoch 1 Batch 52000 Loss 1.8159576654434204 in 27.628773\n",
      "saving model with loss 1.8159576654434204\n",
      "Epoch 1 Batch 52500 Loss 1.88783860206604 in 27.245815\n",
      "Epoch 1 Batch 53000 Loss 1.9967060089111328 in 27.971704\n",
      "saving model with loss 1.9967060089111328\n",
      "Epoch 1 Batch 53500 Loss 1.8892940282821655 in 27.601239\n",
      "Epoch 1 Batch 54000 Loss 1.915034532546997 in 27.323001\n",
      "saving model with loss 1.915034532546997\n",
      "Epoch 1 Batch 54500 Loss 1.7673534154891968 in 28.190093\n",
      "Epoch 1 Batch 55000 Loss 1.8127466440200806 in 27.809188\n",
      "saving model with loss 1.8127466440200806\n",
      "Epoch 1 Batch 55500 Loss 1.8990358114242554 in 27.986304\n",
      "Epoch 1 Batch 56000 Loss 1.8240993022918701 in 27.876196\n",
      "saving model with loss 1.8240993022918701\n",
      "Epoch 1 Batch 56500 Loss 1.8369629383087158 in 28.170895\n",
      "Epoch 1 Batch 57000 Loss 1.8997585773468018 in 28.477372\n",
      "saving model with loss 1.8997585773468018\n",
      "Epoch 1 Batch 57500 Loss 1.897560477256775 in 27.454262\n",
      "Epoch 1 Batch 58000 Loss 1.7367384433746338 in 27.470216\n",
      "saving model with loss 1.7367384433746338\n",
      "Epoch 1 Batch 58500 Loss 1.8058502674102783 in 27.350899\n",
      "Epoch 1 Batch 59000 Loss 2.0682873725891113 in 27.066661\n",
      "saving model with loss 2.0682873725891113\n",
      "Epoch 1 Batch 59500 Loss 1.9084444046020508 in 27.249173\n",
      "Epoch 1 Batch 60000 Loss 1.9754459857940674 in 27.390813\n",
      "saving model with loss 1.9754459857940674\n",
      "Epoch 1 Batch 60500 Loss 1.8123337030410767 in 28.206646\n",
      "Epoch 1 Batch 61000 Loss 1.8766450881958008 in 28.624735\n",
      "saving model with loss 1.8766450881958008\n",
      "Epoch 1 Batch 61500 Loss 1.809960126876831 in 27.954288\n",
      "Epoch 1 Batch 62000 Loss 1.907240629196167 in 28.103539\n",
      "saving model with loss 1.907240629196167\n",
      "Epoch 1 Batch 62500 Loss 1.9537131786346436 in 28.334345\n",
      "Epoch 1 Batch 63000 Loss 1.9694026708602905 in 27.154430\n",
      "saving model with loss 1.9694026708602905\n",
      "Epoch 1 Batch 63500 Loss 2.0445454120635986 in 28.215170\n",
      "Epoch 1 Batch 64000 Loss 1.9802440404891968 in 27.650102\n",
      "saving model with loss 1.9802440404891968\n",
      "Epoch 1 Batch 64500 Loss 1.9149160385131836 in 28.656412\n",
      "Epoch 1 Batch 65000 Loss 1.9042489528656006 in 27.378826\n",
      "saving model with loss 1.9042489528656006\n",
      "Epoch 1 Batch 65500 Loss 1.9230868816375732 in 27.827628\n",
      "Epoch 1 Batch 66000 Loss 1.9025440216064453 in 27.469584\n",
      "saving model with loss 1.9025440216064453\n",
      "Epoch 1 Batch 66500 Loss 1.7911663055419922 in 27.574304\n",
      "Epoch 1 Batch 67000 Loss 1.903420090675354 in 27.418749\n",
      "saving model with loss 1.903420090675354\n",
      "Epoch 1 Batch 67500 Loss 1.9308226108551025 in 28.187663\n",
      "Epoch 1 Batch 68000 Loss 1.9594764709472656 in 27.120517\n",
      "saving model with loss 1.9594764709472656\n",
      "Epoch 1 Batch 68500 Loss 2.070437431335449 in 28.867847\n",
      "Epoch 1 Batch 69000 Loss 1.8542741537094116 in 27.103561\n",
      "saving model with loss 1.8542741537094116\n",
      "Epoch 1 Batch 69500 Loss 1.8173255920410156 in 27.395812\n",
      "Epoch 1 Batch 70000 Loss 1.9051481485366821 in 27.802807\n",
      "saving model with loss 1.9051481485366821\n",
      "Epoch 1 Batch 70500 Loss 1.9430348873138428 in 27.471197\n",
      "Epoch 1 Batch 71000 Loss 1.940269112586975 in 27.140492\n",
      "saving model with loss 1.940269112586975\n",
      "Epoch 1 Batch 71500 Loss 1.7691965103149414 in 26.771485\n",
      "Epoch 1 Batch 72000 Loss 1.8767902851104736 in 26.891130\n",
      "saving model with loss 1.8767902851104736\n",
      "Epoch 1 Batch 72500 Loss 1.8430992364883423 in 29.004216\n",
      "Epoch 1 Batch 73000 Loss 2.0463359355926514 in 28.429465\n",
      "saving model with loss 2.0463359355926514\n",
      "Epoch 1 Batch 73500 Loss 1.8905870914459229 in 27.384432\n",
      "Epoch 1 Batch 74000 Loss 1.8574193716049194 in 26.704321\n",
      "saving model with loss 1.8574193716049194\n",
      "Epoch 1 Batch 74500 Loss 1.97860848903656 in 28.175596\n",
      "Epoch 1 Batch 75000 Loss 1.8865779638290405 in 27.722906\n",
      "saving model with loss 1.8865779638290405\n",
      "Epoch 1 Batch 75500 Loss 1.9141912460327148 in 26.958949\n",
      "Epoch 1 Batch 76000 Loss 1.8316898345947266 in 26.870458\n",
      "saving model with loss 1.8316898345947266\n",
      "Epoch 1 Batch 76500 Loss 1.7755365371704102 in 27.057686\n",
      "Epoch 1 Batch 77000 Loss 1.8817720413208008 in 28.093915\n",
      "saving model with loss 1.8817720413208008\n",
      "Epoch 1 Batch 77500 Loss 2.080395221710205 in 27.629173\n",
      "Epoch 1 Batch 78000 Loss 1.9214814901351929 in 27.689058\n",
      "saving model with loss 1.9214814901351929\n",
      "Epoch 1 Batch 78500 Loss 1.9148308038711548 in 27.910406\n",
      "Epoch 1 Batch 79000 Loss 1.889025092124939 in 27.444158\n",
      "saving model with loss 1.889025092124939\n",
      "Epoch 1 Batch 79500 Loss 1.9592244625091553 in 27.077080\n",
      "Epoch 1 Batch 80000 Loss 2.026732921600342 in 26.798480\n",
      "saving model with loss 2.026732921600342\n",
      "Epoch 1 Batch 80500 Loss 1.9330618381500244 in 28.988730\n",
      "Epoch 1 Batch 81000 Loss 2.0205254554748535 in 28.657107\n",
      "saving model with loss 2.0205254554748535\n",
      "Epoch 1 Batch 81500 Loss 2.0342345237731934 in 28.110315\n",
      "Epoch 1 Batch 82000 Loss 1.843380331993103 in 27.589550\n",
      "saving model with loss 1.843380331993103\n",
      "Epoch 1 Batch 82500 Loss 1.7877280712127686 in 27.764898\n",
      "Epoch 1 Batch 83000 Loss 1.9040911197662354 in 27.047981\n",
      "saving model with loss 1.9040911197662354\n",
      "Epoch 1 Batch 83500 Loss 1.9008289575576782 in 27.052743\n",
      "Epoch 1 Batch 84000 Loss 1.9579136371612549 in 26.592487\n",
      "saving model with loss 1.9579136371612549\n",
      "Epoch 1 Batch 84500 Loss 1.7951685190200806 in 26.674364\n",
      "Epoch 1 Batch 85000 Loss 1.8949025869369507 in 26.592525\n",
      "saving model with loss 1.8949025869369507\n",
      "Epoch 1 Batch 85500 Loss 1.9825503826141357 in 26.646047\n",
      "Epoch 1 Batch 86000 Loss 1.8682979345321655 in 27.125129\n",
      "saving model with loss 1.8682979345321655\n",
      "Epoch 1 Batch 86500 Loss 1.9787918329238892 in 27.278871\n",
      "Epoch 1 Batch 87000 Loss 1.9953680038452148 in 26.594052\n",
      "saving model with loss 1.9953680038452148\n",
      "Epoch 1 Batch 87500 Loss 1.9204753637313843 in 26.899067\n",
      "Epoch 1 Batch 88000 Loss 1.9753059148788452 in 27.053981\n",
      "saving model with loss 1.9753059148788452\n",
      "Epoch 1 Batch 88500 Loss 1.9919488430023193 in 26.980303\n",
      "Epoch 1 Batch 89000 Loss 1.8088003396987915 in 26.991200\n",
      "saving model with loss 1.8088003396987915\n",
      "Epoch 1 Batch 89500 Loss 1.8463904857635498 in 27.187013\n",
      "Epoch 1 Batch 90000 Loss 1.9971014261245728 in 27.099011\n",
      "saving model with loss 1.9971014261245728\n",
      "Epoch 1 Batch 90500 Loss 2.1372358798980713 in 26.849214\n",
      "Epoch 1 Batch 91000 Loss 1.9104862213134766 in 26.974780\n",
      "saving model with loss 1.9104862213134766\n",
      "Epoch 1 Batch 91500 Loss 1.8429893255233765 in 26.745519\n",
      "Epoch 1 Batch 92000 Loss 1.885636568069458 in 26.198581\n",
      "saving model with loss 1.885636568069458\n",
      "Epoch 1 Batch 92500 Loss 1.9123090505599976 in 26.049935\n",
      "Epoch 1 Batch 93000 Loss 1.9576694965362549 in 26.147119\n",
      "saving model with loss 1.9576694965362549\n",
      "Epoch 1 Batch 93500 Loss 1.9818958044052124 in 26.177039\n",
      "Epoch 1 Batch 94000 Loss 1.8207738399505615 in 26.044917\n",
      "saving model with loss 1.8207738399505615\n",
      "Epoch 1 Batch 94500 Loss 1.907350778579712 in 26.111243\n",
      "Epoch 1 Batch 95000 Loss 1.9633184671401978 in 26.344591\n",
      "saving model with loss 1.9633184671401978\n",
      "Epoch 1 Batch 95500 Loss 1.9839575290679932 in 26.106228\n",
      "Epoch 1 Batch 96000 Loss 1.9986892938613892 in 25.807056\n",
      "saving model with loss 1.9986892938613892\n",
      "Epoch 1 Batch 96500 Loss 1.860937476158142 in 25.841905\n",
      "Epoch 1 Batch 97000 Loss 1.938489556312561 in 26.049411\n",
      "saving model with loss 1.938489556312561\n",
      "Epoch 1 Batch 97500 Loss 1.861448049545288 in 26.011510\n",
      "Epoch 1 Batch 98000 Loss 1.8561680316925049 in 25.869831\n",
      "saving model with loss 1.8561680316925049\n",
      "Epoch 1 Batch 98500 Loss 1.8994067907333374 in 26.308686\n",
      "Epoch 1 Batch 99000 Loss 1.9975627660751343 in 26.626970\n",
      "saving model with loss 1.9975627660751343\n",
      "Epoch 1 Batch 99500 Loss 1.8022587299346924 in 26.662452\n",
      "Epoch 1 Batch 100000 Loss 1.9226338863372803 in 26.745862\n",
      "saving model with loss 1.9226338863372803\n",
      "Epoch 1 Batch 100500 Loss 2.005546808242798 in 26.153608\n",
      "Epoch 1 Batch 101000 Loss 1.862633466720581 in 26.597913\n",
      "saving model with loss 1.862633466720581\n",
      "Epoch 1 Batch 101500 Loss 1.983445167541504 in 27.247368\n",
      "Epoch 1 Batch 102000 Loss 2.0735719203948975 in 26.392491\n",
      "saving model with loss 2.0735719203948975\n",
      "Epoch 1 Batch 102500 Loss 1.7286298274993896 in 27.748809\n",
      "Epoch 2 Loss 1.8545860052108765\n",
      "Time taken for 1 epoch 5668.303062438965\n",
      "number of batches : 102704\n",
      "Epoch 2 Batch 0 Loss 2.4914958477020264 in 1.965746\n",
      "saving model with loss 2.4914958477020264\n",
      "Epoch 2 Batch 500 Loss 1.9426958560943604 in 27.795713\n",
      "Epoch 2 Batch 1000 Loss 2.1186137199401855 in 29.331644\n",
      "saving model with loss 2.1186137199401855\n",
      "Epoch 2 Batch 1500 Loss 2.088813304901123 in 28.197637\n",
      "Epoch 2 Batch 2000 Loss 1.8901954889297485 in 27.770813\n",
      "saving model with loss 1.8901954889297485\n",
      "Epoch 2 Batch 2500 Loss 1.8699290752410889 in 27.762801\n",
      "Epoch 2 Batch 3000 Loss 1.7381423711776733 in 27.148973\n",
      "saving model with loss 1.7381423711776733\n",
      "Epoch 2 Batch 3500 Loss 1.9262521266937256 in 26.613994\n",
      "Epoch 2 Batch 4000 Loss 1.8950347900390625 in 26.754571\n",
      "saving model with loss 1.8950347900390625\n",
      "Epoch 2 Batch 4500 Loss 1.9431564807891846 in 27.134665\n",
      "Epoch 2 Batch 5000 Loss 2.0108916759490967 in 27.786910\n",
      "saving model with loss 2.0108916759490967\n",
      "Epoch 2 Batch 5500 Loss 1.9480823278427124 in 28.539752\n",
      "Epoch 2 Batch 6000 Loss 2.070063591003418 in 30.162358\n",
      "saving model with loss 2.070063591003418\n",
      "Epoch 2 Batch 6500 Loss 1.8607285022735596 in 27.526432\n",
      "Epoch 2 Batch 7000 Loss 2.06062912940979 in 27.308015\n",
      "saving model with loss 2.06062912940979\n",
      "Epoch 2 Batch 7500 Loss 1.978034257888794 in 27.244187\n",
      "Epoch 2 Batch 8000 Loss 1.9498964548110962 in 27.570315\n",
      "saving model with loss 1.9498964548110962\n",
      "Epoch 2 Batch 8500 Loss 1.8771120309829712 in 28.263462\n",
      "Epoch 2 Batch 9000 Loss 1.9321377277374268 in 27.481552\n",
      "saving model with loss 1.9321377277374268\n",
      "Epoch 2 Batch 9500 Loss 1.959621787071228 in 27.880485\n",
      "Epoch 2 Batch 10000 Loss 1.9094817638397217 in 27.583278\n",
      "saving model with loss 1.9094817638397217\n",
      "Epoch 2 Batch 10500 Loss 1.9271252155303955 in 27.155424\n",
      "Epoch 2 Batch 11000 Loss 1.8665136098861694 in 28.074966\n",
      "saving model with loss 1.8665136098861694\n",
      "Epoch 2 Batch 11500 Loss 1.8167330026626587 in 27.285109\n",
      "Epoch 2 Batch 12000 Loss 1.8371788263320923 in 27.522439\n",
      "saving model with loss 1.8371788263320923\n",
      "Epoch 2 Batch 12500 Loss 1.8960025310516357 in 27.304995\n",
      "Epoch 2 Batch 13000 Loss 1.8402286767959595 in 26.854229\n",
      "saving model with loss 1.8402286767959595\n",
      "Epoch 2 Batch 13500 Loss 1.8998714685440063 in 27.767954\n",
      "Epoch 2 Batch 14000 Loss 1.877288818359375 in 28.465752\n",
      "saving model with loss 1.877288818359375\n",
      "Epoch 2 Batch 14500 Loss 2.005599021911621 in 27.153472\n",
      "Epoch 2 Batch 15000 Loss 1.7437747716903687 in 26.960311\n",
      "saving model with loss 1.7437747716903687\n",
      "Epoch 2 Batch 15500 Loss 1.9828689098358154 in 27.012504\n",
      "Epoch 2 Batch 16000 Loss 1.8063008785247803 in 26.660888\n",
      "saving model with loss 1.8063008785247803\n",
      "Epoch 2 Batch 16500 Loss 1.8839906454086304 in 27.423205\n",
      "Epoch 2 Batch 17000 Loss 1.8526191711425781 in 26.708885\n",
      "saving model with loss 1.8526191711425781\n",
      "Epoch 2 Batch 17500 Loss 1.795372724533081 in 25.817029\n",
      "Epoch 2 Batch 18000 Loss 1.7135041952133179 in 25.895869\n",
      "saving model with loss 1.7135041952133179\n",
      "Epoch 2 Batch 18500 Loss 1.7463483810424805 in 26.932049\n",
      "Epoch 2 Batch 19000 Loss 1.9674854278564453 in 26.724546\n",
      "saving model with loss 1.9674854278564453\n",
      "Epoch 2 Batch 19500 Loss 1.9165958166122437 in 26.480265\n",
      "Epoch 2 Batch 20000 Loss 1.8039722442626953 in 26.600940\n",
      "saving model with loss 1.8039722442626953\n",
      "Epoch 2 Batch 20500 Loss 1.9888811111450195 in 27.011828\n",
      "Epoch 2 Batch 21000 Loss 1.8173099756240845 in 25.813698\n",
      "saving model with loss 1.8173099756240845\n",
      "Epoch 2 Batch 21500 Loss 1.970116376876831 in 25.434005\n",
      "Epoch 2 Batch 22000 Loss 1.734693169593811 in 25.568300\n",
      "saving model with loss 1.734693169593811\n",
      "Epoch 2 Batch 22500 Loss 1.9373372793197632 in 25.883111\n",
      "Epoch 2 Batch 23000 Loss 1.8314082622528076 in 26.135460\n",
      "saving model with loss 1.8314082622528076\n",
      "Epoch 2 Batch 23500 Loss 1.9699327945709229 in 26.427555\n",
      "Epoch 2 Batch 24000 Loss 1.8136259317398071 in 26.053201\n",
      "saving model with loss 1.8136259317398071\n",
      "Epoch 2 Batch 24500 Loss 1.9328533411026 in 26.436721\n",
      "Epoch 2 Batch 25000 Loss 1.8998935222625732 in 26.488790\n",
      "saving model with loss 1.8998935222625732\n",
      "Epoch 2 Batch 25500 Loss 1.938887596130371 in 26.362314\n",
      "Epoch 2 Batch 26000 Loss 1.8445640802383423 in 27.358939\n",
      "saving model with loss 1.8445640802383423\n",
      "Epoch 2 Batch 26500 Loss 1.953796625137329 in 26.484601\n",
      "Epoch 2 Batch 27000 Loss 1.9305789470672607 in 25.842199\n",
      "saving model with loss 1.9305789470672607\n",
      "Epoch 2 Batch 27500 Loss 2.030073642730713 in 25.902840\n",
      "Epoch 2 Batch 28000 Loss 1.9069904088974 in 25.809826\n",
      "saving model with loss 1.9069904088974\n",
      "Epoch 2 Batch 28500 Loss 1.9710209369659424 in 25.915463\n",
      "Epoch 2 Batch 29000 Loss 1.8626468181610107 in 25.854530\n",
      "saving model with loss 1.8626468181610107\n",
      "Epoch 2 Batch 29500 Loss 1.8562339544296265 in 26.157641\n",
      "Epoch 2 Batch 30000 Loss 1.9094337224960327 in 25.912643\n",
      "saving model with loss 1.9094337224960327\n",
      "Epoch 2 Batch 30500 Loss 1.8338806629180908 in 25.984025\n",
      "Epoch 2 Batch 31000 Loss 1.8458772897720337 in 26.539149\n",
      "saving model with loss 1.8458772897720337\n",
      "Epoch 2 Batch 31500 Loss 1.9920856952667236 in 26.814318\n",
      "Epoch 2 Batch 32000 Loss 1.9594796895980835 in 26.559604\n",
      "saving model with loss 1.9594796895980835\n",
      "Epoch 2 Batch 32500 Loss 1.917557954788208 in 26.458217\n",
      "Epoch 2 Batch 33000 Loss 1.9873311519622803 in 26.723017\n",
      "saving model with loss 1.9873311519622803\n",
      "Epoch 2 Batch 33500 Loss 1.9574865102767944 in 26.486142\n",
      "Epoch 2 Batch 34000 Loss 1.735107421875 in 26.292140\n",
      "saving model with loss 1.735107421875\n",
      "Epoch 2 Batch 34500 Loss 1.8342241048812866 in 26.444706\n",
      "Epoch 2 Batch 35000 Loss 1.9217844009399414 in 26.854772\n",
      "saving model with loss 1.9217844009399414\n",
      "Epoch 2 Batch 35500 Loss 1.8673328161239624 in 26.831414\n",
      "Epoch 2 Batch 36000 Loss 1.8128904104232788 in 26.578856\n",
      "saving model with loss 1.8128904104232788\n",
      "Epoch 2 Batch 36500 Loss 1.9022146463394165 in 27.145600\n",
      "Epoch 2 Batch 37000 Loss 1.9491844177246094 in 25.371915\n",
      "saving model with loss 1.9491844177246094\n",
      "Epoch 2 Batch 37500 Loss 1.844696283340454 in 25.188005\n",
      "Epoch 2 Batch 38000 Loss 1.90383780002594 in 25.554848\n",
      "saving model with loss 1.90383780002594\n",
      "Epoch 2 Batch 38500 Loss 1.9282867908477783 in 25.645674\n",
      "Epoch 2 Batch 39000 Loss 2.1187081336975098 in 25.515304\n",
      "saving model with loss 2.1187081336975098\n",
      "Epoch 2 Batch 39500 Loss 1.8887901306152344 in 27.077697\n",
      "Epoch 2 Batch 40000 Loss 1.9351717233657837 in 26.058508\n",
      "saving model with loss 1.9351717233657837\n",
      "Epoch 2 Batch 40500 Loss 2.037100315093994 in 25.204449\n",
      "Epoch 2 Batch 41000 Loss 2.013279914855957 in 25.420057\n",
      "saving model with loss 2.013279914855957\n",
      "Epoch 2 Batch 41500 Loss 1.9223905801773071 in 25.401329\n",
      "Epoch 2 Batch 42000 Loss 2.0448288917541504 in 24.683608\n",
      "saving model with loss 2.0448288917541504\n",
      "Epoch 2 Batch 42500 Loss 1.822532296180725 in 26.711201\n",
      "Epoch 2 Batch 43000 Loss 1.873883605003357 in 27.279327\n",
      "saving model with loss 1.873883605003357\n",
      "Epoch 2 Batch 43500 Loss 2.0533902645111084 in 26.100449\n",
      "Epoch 2 Batch 44000 Loss 1.9972789287567139 in 24.977661\n",
      "saving model with loss 1.9972789287567139\n",
      "Epoch 2 Batch 44500 Loss 1.936461091041565 in 25.143623\n",
      "Epoch 2 Batch 45000 Loss 1.970107078552246 in 25.193784\n",
      "saving model with loss 1.970107078552246\n",
      "Epoch 2 Batch 45500 Loss 1.936762809753418 in 25.402405\n",
      "Epoch 2 Batch 46000 Loss 1.9387156963348389 in 25.316833\n",
      "saving model with loss 1.9387156963348389\n",
      "Epoch 2 Batch 46500 Loss 1.758426308631897 in 25.763732\n",
      "Epoch 2 Batch 47000 Loss 1.8697935342788696 in 25.645529\n",
      "saving model with loss 1.8697935342788696\n",
      "Epoch 2 Batch 47500 Loss 1.9460909366607666 in 25.818729\n",
      "Epoch 2 Batch 48000 Loss 1.8066818714141846 in 26.297496\n",
      "saving model with loss 1.8066818714141846\n",
      "Epoch 2 Batch 48500 Loss 1.898943305015564 in 26.131356\n",
      "Epoch 2 Batch 49000 Loss 1.9205433130264282 in 25.862476\n",
      "saving model with loss 1.9205433130264282\n",
      "Epoch 2 Batch 49500 Loss 1.9635570049285889 in 26.172868\n",
      "Epoch 2 Batch 50000 Loss 1.9968522787094116 in 26.115668\n",
      "saving model with loss 1.9968522787094116\n",
      "Epoch 2 Batch 50500 Loss 1.916556715965271 in 26.326622\n",
      "Epoch 2 Batch 51000 Loss 1.8600757122039795 in 26.666839\n",
      "saving model with loss 1.8600757122039795\n",
      "Epoch 2 Batch 51500 Loss 1.93447744846344 in 27.146316\n",
      "Epoch 2 Batch 52000 Loss 1.7688190937042236 in 26.956731\n",
      "saving model with loss 1.7688190937042236\n",
      "Epoch 2 Batch 52500 Loss 1.9877088069915771 in 27.029966\n",
      "Epoch 2 Batch 53000 Loss 2.00211763381958 in 26.201054\n",
      "saving model with loss 2.00211763381958\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-27-cf36d8cd8dd0>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mwin_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mModel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrnn_units\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwin_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcheckpoint_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'.\\\\checkpoints\\\\model.h5'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'.\\\\vocabulary'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtokenized_files\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m20\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\_praca_inzynierska\\ai-complete\\src\\model.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, vocab_path, tokenized_files, epochs)\u001B[0m\n\u001B[0;32m     53\u001B[0m             \u001B[0mbatch_start\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mbatch_n\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnext_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokenized_files\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 55\u001B[1;33m                 \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     56\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mbatch_n\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;36m500\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m                     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'.\\\\checkpoints\\\\losses.txt'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'a'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\_praca_inzynierska\\ai-complete\\src\\model.py\u001B[0m in \u001B[0;36mtrain_step\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    121\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreduce_mean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    122\u001B[0m             \u001B[0mgrads\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 123\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_gradients\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrads\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    124\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001B[0m in \u001B[0;36mapply_gradients\u001B[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001B[0m\n\u001B[0;32m    547\u001B[0m           \u001B[0margs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrads_and_vars\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    548\u001B[0m           kwargs={\n\u001B[1;32m--> 549\u001B[1;33m               \u001B[1;34m\"name\"\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    550\u001B[0m           })\n\u001B[0;32m    551\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001B[0m in \u001B[0;36mmerge_call\u001B[1;34m(self, merge_fn, args, kwargs)\u001B[0m\n\u001B[0;32m   2713\u001B[0m     merge_fn = autograph.tf_convert(\n\u001B[0;32m   2714\u001B[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001B[1;32m-> 2715\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_merge_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmerge_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2716\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2717\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_merge_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmerge_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001B[0m in \u001B[0;36m_merge_call\u001B[1;34m(self, merge_fn, args, kwargs)\u001B[0m\n\u001B[0;32m   2720\u001B[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001B[0;32m   2721\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2722\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mmerge_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_strategy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2723\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2724\u001B[0m       \u001B[0m_pop_per_thread_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    273\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    274\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mag_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mControlStatusCtx\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstatus\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mag_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mStatus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mUNSPECIFIED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 275\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    276\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    277\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfunction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mismethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001B[0m in \u001B[0;36m_distributed_apply\u001B[1;34m(self, distribution, grads_and_vars, name, apply_state)\u001B[0m\n\u001B[0;32m    631\u001B[0m                               \"update_\" + var.op.name, skip_on_eager=True):\n\u001B[0;32m    632\u001B[0m             update_ops.extend(distribution.extended.update(\n\u001B[1;32m--> 633\u001B[1;33m                 var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001B[0m\u001B[0;32m    634\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    635\u001B[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001B[0m in \u001B[0;36mupdate\u001B[1;34m(self, var, fn, args, kwargs, group)\u001B[0m\n\u001B[0;32m   2298\u001B[0m         fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001B[0;32m   2299\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_container_strategy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2300\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvar\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgroup\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2301\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2302\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_update\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvar\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgroup\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001B[0m in \u001B[0;36m_update\u001B[1;34m(self, var, fn, args, kwargs, group)\u001B[0m\n\u001B[0;32m   2953\u001B[0m     \u001B[1;31m# The implementations of _update() and _update_non_slot() are identical\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2954\u001B[0m     \u001B[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2955\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_update_non_slot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvar\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mvar\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgroup\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2956\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2957\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_update_non_slot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcolocate_with\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshould_group\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001B[0m in \u001B[0;36m_update_non_slot\u001B[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001B[0m\n\u001B[0;32m   2959\u001B[0m     \u001B[1;31m# once that value is used for something.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2960\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mUpdateContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcolocate_with\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2961\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2962\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mshould_group\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2963\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    273\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    274\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mag_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mControlStatusCtx\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstatus\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mag_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mStatus\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mUNSPECIFIED\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 275\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    276\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    277\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfunction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mismethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001B[0m in \u001B[0;36mapply_grad_to_update_var\u001B[1;34m(var, grad)\u001B[0m\n\u001B[0;32m    602\u001B[0m           \u001B[0mapply_kwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"apply_state\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mapply_state\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    603\u001B[0m         return self._resource_apply_sparse_duplicate_indices(\n\u001B[1;32m--> 604\u001B[1;33m             grad.values, var, grad.indices, **apply_kwargs)\n\u001B[0m\u001B[0;32m    605\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    606\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[1;34m\"apply_state\"\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dense_apply_args\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001B[0m in \u001B[0;36m_resource_apply_sparse_duplicate_indices\u001B[1;34m(self, grad, handle, indices, **kwargs)\u001B[0m\n\u001B[0;32m   1124\u001B[0m         values=grad, indices=indices)\n\u001B[0;32m   1125\u001B[0m     return self._resource_apply_sparse(summed_grad, handle, unique_indices,\n\u001B[1;32m-> 1126\u001B[1;33m                                        **kwargs)\n\u001B[0m\u001B[0;32m   1127\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1128\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_resource_apply_sparse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mapply_state\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py\u001B[0m in \u001B[0;36m_resource_apply_sparse\u001B[1;34m(self, grad, var, indices, apply_state)\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mamsgrad\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 224\u001B[1;33m       \u001B[0mv_sqrt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmath_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mv_t\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    225\u001B[0m       var_update = state_ops.assign_sub(\n\u001B[0;32m    226\u001B[0m           \u001B[0mvar\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcoefficients\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'lr'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mm_t\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mv_sqrt\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mcoefficients\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'epsilon'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    202\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001B[0m in \u001B[0;36msqrt\u001B[1;34m(x, name)\u001B[0m\n\u001B[0;32m   4771\u001B[0m     \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0mof\u001B[0m \u001B[0msame\u001B[0m \u001B[0msize\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtype\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0msparsity\u001B[0m \u001B[1;32mas\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mx\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4772\u001B[0m   \"\"\"\n\u001B[1;32m-> 4773\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   4774\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   4775\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\01133357\\documents\\_praca_inzynierska\\ai-complete\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001B[0m in \u001B[0;36msqrt\u001B[1;34m(x, name)\u001B[0m\n\u001B[0;32m  10177\u001B[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0;32m  10178\u001B[0m         \u001B[0m_ctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_context_handle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Sqrt\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mop_callbacks\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m> 10179\u001B[1;33m         x)\n\u001B[0m\u001B[0;32m  10180\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m  10181\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from src.model import Model\n",
    "\n",
    "vocab_size = 20001\n",
    "embedding_dim = 64\n",
    "rnn_units = 512\n",
    "batch_size = 128\n",
    "win_size = 10\n",
    "model = Model(vocab_size, embedding_dim, rnn_units, batch_size, win_size, checkpoint_name='.\\\\checkpoints\\\\model.h5')\n",
    "model.train('.\\\\vocabulary', tokenized_files, 20)\n",
    "#start 15:16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.prepare_predictions('.\\\\vocabulary', '.\\\\checkpoints\\\\model.h5')\n",
    "# model.get_prediction('import numpy', 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}